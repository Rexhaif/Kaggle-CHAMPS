{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline, make_union\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import lightgbm as lgb\n",
    "import gc\n",
    "from time import time\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psutil\n",
    "import os\n",
    "def get_mem_usage():\n",
    "    pid = os.getpid()\n",
    "    py = psutil.Process(pid)\n",
    "    memoryUse = py.memory_info()[0]/2.**30\n",
    "    return memoryUse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import reduce_mem_usage, ClassifierTransformer, MoreStructureProperties, MakeMoreFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_atom_info(df_1, df_2, atom_idx):\n",
    "    df = pd.merge(df_1, df_2, how = 'left',\n",
    "                  left_on  = ['molecule_name', f'atom_index_{atom_idx}'],\n",
    "                  right_on = ['molecule_name',  'atom_index'])\n",
    "    \n",
    "    df = df.drop('atom_index', axis=1)\n",
    "    return df\n",
    "\n",
    "    \n",
    "def find_dist(df):\n",
    "    df_p_0 = df[['x_0', 'y_0', 'z_0']].values\n",
    "    df_p_1 = df[['x_1', 'y_1', 'z_1']].values\n",
    "    \n",
    "    df['dist'] = np.linalg.norm(df_p_0 - df_p_1, axis=1)\n",
    "    df['dist_inv2'] = 1/df['dist']**2\n",
    "    df['dist_x'] = (df['x_0'] - df['x_1']) ** 2\n",
    "    df['dist_y'] = (df['y_0'] - df['y_1']) ** 2\n",
    "    df['dist_z'] = (df['z_0'] - df['z_1']) ** 2\n",
    "    return df\n",
    "\n",
    "def find_closest_atom(df):    \n",
    "    df_temp = df.loc[:,[\"molecule_name\",\n",
    "                      \"atom_index_0\",\"atom_index_1\",\n",
    "                      \"dist\",\"x_0\",\"y_0\",\"z_0\",\"x_1\",\"y_1\",\"z_1\"]].copy()\n",
    "    df_temp_ = df_temp.copy()\n",
    "    df_temp_ = df_temp_.rename(columns={'atom_index_0': 'atom_index_1',\n",
    "                                       'atom_index_1': 'atom_index_0',\n",
    "                                       'x_0': 'x_1',\n",
    "                                       'y_0': 'y_1',\n",
    "                                       'z_0': 'z_1',\n",
    "                                       'x_1': 'x_0',\n",
    "                                       'y_1': 'y_0',\n",
    "                                       'z_1': 'z_0'})\n",
    "    df_temp_all = pd.concat((df_temp,df_temp_),axis=0)\n",
    "\n",
    "    df_temp_all[\"min_distance\"]=df_temp_all.groupby(['molecule_name', \n",
    "                                                     'atom_index_0'])['dist'].transform('min')\n",
    "    df_temp_all[\"max_distance\"]=df_temp_all.groupby(['molecule_name', \n",
    "                                                     'atom_index_0'])['dist'].transform('max')\n",
    "    \n",
    "    df_temp = df_temp_all[df_temp_all[\"min_distance\"]==df_temp_all[\"dist\"]].copy()\n",
    "    df_temp = df_temp.drop(['x_0','y_0','z_0','min_distance'], axis=1)\n",
    "    df_temp = df_temp.rename(columns={'atom_index_0': 'atom_index',\n",
    "                                         'atom_index_1': 'atom_index_closest',\n",
    "                                         'dist': 'distance_closest',\n",
    "                                         'x_1': 'x_closest',\n",
    "                                         'y_1': 'y_closest',\n",
    "                                         'z_1': 'z_closest'})\n",
    "    df_temp = df_temp.drop_duplicates(subset=['molecule_name', 'atom_index'])\n",
    "    \n",
    "    for atom_idx in [0,1]:\n",
    "        df = map_atom_info(df,df_temp, atom_idx)\n",
    "        df = df.rename(columns={'atom_index_closest': f'atom_index_closest_{atom_idx}',\n",
    "                                        'distance_closest': f'distance_closest_{atom_idx}',\n",
    "                                        'x_closest': f'x_closest_{atom_idx}',\n",
    "                                        'y_closest': f'y_closest_{atom_idx}',\n",
    "                                        'z_closest': f'z_closest_{atom_idx}'})\n",
    "        \n",
    "    df_temp= df_temp_all[df_temp_all[\"max_distance\"]==df_temp_all[\"dist\"]].copy()\n",
    "    df_temp = df_temp.drop(['x_0','y_0','z_0','max_distance'], axis=1)\n",
    "    df_temp= df_temp.rename(columns={'atom_index_0': 'atom_index',\n",
    "                                         'atom_index_1': 'atom_index_farthest',\n",
    "                                         'dist': 'distance_farthest',\n",
    "                                         'x_1': 'x_farthest',\n",
    "                                         'y_1': 'y_farthest',\n",
    "                                         'z_1': 'z_farthest'})\n",
    "    df_temp = df_temp.drop_duplicates(subset=['molecule_name', 'atom_index'])\n",
    "        \n",
    "    for atom_idx in [0,1]:\n",
    "        df = map_atom_info(df,df_temp, atom_idx)\n",
    "        df = df.rename(columns={'atom_index_farthest': f'atom_index_farthest_{atom_idx}',\n",
    "                                        'distance_farthest': f'distance_farthest_{atom_idx}',\n",
    "                                        'x_farthest': f'x_farthest_{atom_idx}',\n",
    "                                        'y_farthest': f'y_farthest_{atom_idx}',\n",
    "                                        'z_farthest': f'z_farthest_{atom_idx}'})\n",
    "    return df\n",
    "\n",
    "\n",
    "def add_cos_features(df):\n",
    "    \n",
    "    df[\"distance_center0\"] = np.sqrt((df['x_0']-df['c_x'])**2 \\\n",
    "                                   + (df['y_0']-df['c_y'])**2 \\\n",
    "                                   + (df['z_0']-df['c_z'])**2)\n",
    "    df[\"distance_center1\"] = np.sqrt((df['x_1']-df['c_x'])**2 \\\n",
    "                                   + (df['y_1']-df['c_y'])**2 \\\n",
    "                                   + (df['z_1']-df['c_z'])**2)\n",
    "    \n",
    "    df['distance_c0'] = np.sqrt((df['x_0']-df['x_closest_0'])**2 + \\\n",
    "                                (df['y_0']-df['y_closest_0'])**2 + \\\n",
    "                                (df['z_0']-df['z_closest_0'])**2)\n",
    "    df['distance_c1'] = np.sqrt((df['x_1']-df['x_closest_1'])**2 + \\\n",
    "                                (df['y_1']-df['y_closest_1'])**2 + \\\n",
    "                                (df['z_1']-df['z_closest_1'])**2)\n",
    "    \n",
    "    df[\"distance_f0\"] = np.sqrt((df['x_0']-df['x_farthest_0'])**2 + \\\n",
    "                                (df['y_0']-df['y_farthest_0'])**2 + \\\n",
    "                                (df['z_0']-df['z_farthest_0'])**2)\n",
    "    df[\"distance_f1\"] = np.sqrt((df['x_1']-df['x_farthest_1'])**2 + \\\n",
    "                                (df['y_1']-df['y_farthest_1'])**2 + \\\n",
    "                                (df['z_1']-df['z_farthest_1'])**2)\n",
    "    \n",
    "    vec_center0_x = (df['x_0']-df['c_x'])/(df[\"distance_center0\"]+1e-10)\n",
    "    vec_center0_y = (df['y_0']-df['c_y'])/(df[\"distance_center0\"]+1e-10)\n",
    "    vec_center0_z = (df['z_0']-df['c_z'])/(df[\"distance_center0\"]+1e-10)\n",
    "    \n",
    "    vec_center1_x = (df['x_1']-df['c_x'])/(df[\"distance_center1\"]+1e-10)\n",
    "    vec_center1_y = (df['y_1']-df['c_y'])/(df[\"distance_center1\"]+1e-10)\n",
    "    vec_center1_z = (df['z_1']-df['c_z'])/(df[\"distance_center1\"]+1e-10)\n",
    "    \n",
    "    vec_c0_x = (df['x_0']-df['x_closest_0'])/(df[\"distance_c0\"]+1e-10)\n",
    "    vec_c0_y = (df['y_0']-df['y_closest_0'])/(df[\"distance_c0\"]+1e-10)\n",
    "    vec_c0_z = (df['z_0']-df['z_closest_0'])/(df[\"distance_c0\"]+1e-10)\n",
    "    \n",
    "    vec_c1_x = (df['x_1']-df['x_closest_1'])/(df[\"distance_c1\"]+1e-10)\n",
    "    vec_c1_y = (df['y_1']-df['y_closest_1'])/(df[\"distance_c1\"]+1e-10)\n",
    "    vec_c1_z = (df['z_1']-df['z_closest_1'])/(df[\"distance_c1\"]+1e-10)\n",
    "    \n",
    "    vec_f0_x = (df['x_0']-df['x_farthest_0'])/(df[\"distance_f0\"]+1e-10)\n",
    "    vec_f0_y = (df['y_0']-df['y_farthest_0'])/(df[\"distance_f0\"]+1e-10)\n",
    "    vec_f0_z = (df['z_0']-df['z_farthest_0'])/(df[\"distance_f0\"]+1e-10)\n",
    "    \n",
    "    vec_f1_x = (df['x_1']-df['x_farthest_1'])/(df[\"distance_f1\"]+1e-10)\n",
    "    vec_f1_y = (df['y_1']-df['y_farthest_1'])/(df[\"distance_f1\"]+1e-10)\n",
    "    vec_f1_z = (df['z_1']-df['z_farthest_1'])/(df[\"distance_f1\"]+1e-10)\n",
    "    \n",
    "    vec_x = (df['x_1']-df['x_0'])/df['dist']\n",
    "    vec_y = (df['y_1']-df['y_0'])/df['dist']\n",
    "    vec_z = (df['z_1']-df['z_0'])/df['dist']\n",
    "    \n",
    "    df[\"cos_c0_c1\"] = vec_c0_x*vec_c1_x + vec_c0_y*vec_c1_y + vec_c0_z*vec_c1_z\n",
    "    df[\"cos_f0_f1\"] = vec_f0_x*vec_f1_x + vec_f0_y*vec_f1_y + vec_f0_z*vec_f1_z\n",
    "    \n",
    "    df[\"cos_c0_f0\"] = vec_c0_x*vec_f0_x + vec_c0_y*vec_f0_y + vec_c0_z*vec_f0_z\n",
    "    df[\"cos_c1_f1\"] = vec_c1_x*vec_f1_x + vec_c1_y*vec_f1_y + vec_c1_z*vec_f1_z\n",
    "    \n",
    "    df[\"cos_center0_center1\"] = vec_center0_x*vec_center1_x \\\n",
    "                              + vec_center0_y*vec_center1_y \\\n",
    "                              + vec_center0_z*vec_center1_z\n",
    "    \n",
    "    df[\"cos_c0\"] = vec_c0_x*vec_x + vec_c0_y*vec_y + vec_c0_z*vec_z\n",
    "    df[\"cos_c1\"] = vec_c1_x*vec_x + vec_c1_y*vec_y + vec_c1_z*vec_z\n",
    "    \n",
    "    df[\"cos_f0\"] = vec_f0_x*vec_x + vec_f0_y*vec_y + vec_f0_z*vec_z\n",
    "    df[\"cos_f1\"] = vec_f1_x*vec_x + vec_f1_y*vec_y + vec_f1_z*vec_z\n",
    "    \n",
    "    df[\"cos_center0\"] = vec_center0_x*vec_x + vec_center0_y*vec_y + vec_center0_z*vec_z\n",
    "    df[\"cos_center1\"] = vec_center1_x*vec_x + vec_center1_y*vec_y + vec_center1_z*vec_z\n",
    "\n",
    "    return df\n",
    "\n",
    "def dummies(df, list_cols):\n",
    "    for col in list_cols:\n",
    "        df_dummies = pd.get_dummies(df[col], drop_first=True, \n",
    "                                    prefix=(str(col)))\n",
    "        df = pd.concat([df, df_dummies], axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_qm9_features(df):\n",
    "    data_qm9 = pd.read_pickle('../data/raw/data.covs.pickle')\n",
    "    to_drop = ['type', \n",
    "               'linear', \n",
    "               'atom_index_0', \n",
    "               'atom_index_1', \n",
    "               'scalar_coupling_constant', \n",
    "               'U', 'G', 'H', \n",
    "               'mulliken_mean', 'r2', 'U0']\n",
    "    data_qm9 = data_qm9.drop(columns = to_drop, axis=1)\n",
    "    data_qm9 = reduce_mem_usage(data_qm9,verbose=False)\n",
    "    df = pd.merge(df, data_qm9, how='left', on=['molecule_name','id'])\n",
    "    del data_qm9\n",
    "    \n",
    "    df = dummies(df, ['type', 'atom_1'])\n",
    "    return df\n",
    "\n",
    "def get_features(df, struct):\n",
    "    for atom_idx in [0,1]:\n",
    "        df = map_atom_info(df, struct, atom_idx)\n",
    "        df = df.rename(columns={'atom': f'atom_{atom_idx}',\n",
    "                            'x': f'x_{atom_idx}',\n",
    "                            'y': f'y_{atom_idx}',\n",
    "                            'z': f'z_{atom_idx}'})\n",
    "        struct['c_x'] = struct.groupby('molecule_name')['x'].transform('mean')\n",
    "        struct['c_y'] = struct.groupby('molecule_name')['y'].transform('mean')\n",
    "        struct['c_z'] = struct.groupby('molecule_name')['z'].transform('mean')\n",
    "\n",
    "    df = find_dist(df)\n",
    "    df = find_closest_atom(df)\n",
    "    df = add_cos_features(df)\n",
    "    df = add_qm9_features(df)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feat_from_structures(df, st):\n",
    "    df = pd.merge(df,st,how='left',left_on=['molecule_name','atom_index_0'], right_on=['molecule_name','atom_index'])\n",
    "    df = pd.merge(df,st,how='left',left_on=['molecule_name','atom_index_1'], right_on=['molecule_name','atom_index'])\n",
    "    n_atoms = st.groupby(['molecule_name','atom'])['atom'].size().to_frame(name = 'count').reset_index()\n",
    "    n_atoms_df = n_atoms.pivot_table('count',['molecule_name'], 'atom')\n",
    "    n_atoms_df.fillna(0,inplace=True)\n",
    "    df = pd.merge(df,n_atoms_df,on=['molecule_name'],how='left')\n",
    "    del n_atoms\n",
    "    gc.collect()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comp_score (y_true, y_pred, jtype):\n",
    "    df = pd.DataFrame()\n",
    "    df['y_true'] , df['y_pred'], df['jtype'] = y_true , y_pred, jtype\n",
    "    score = 0 \n",
    "    for t in jtype.unique():\n",
    "        score_jtype = np.log(mean_absolute_error(df[df.jtype==t]['y_true'],df[df.jtype==t]['y_pred']))\n",
    "        score += score_jtype\n",
    "        print(f'{t} : {score_jtype}')\n",
    "    score /= len(jtype.unique())\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feat_from_structures(df, st):\n",
    "    df = pd.merge(df,st,how='left',left_on=['molecule_name','atom_index_0'], right_on=['molecule_name','atom_index'])\n",
    "    df = pd.merge(df,st,how='left',left_on=['molecule_name','atom_index_1'], right_on=['molecule_name','atom_index'])\n",
    "    n_atoms = st.groupby(['molecule_name','atom'])['atom'].size().to_frame(name = 'count').reset_index()\n",
    "    n_atoms_df = n_atoms.pivot_table('count',['molecule_name'], 'atom')\n",
    "    n_atoms_df.fillna(0,inplace=True)\n",
    "    df = pd.merge(df,n_atoms_df,on=['molecule_name'],how='left')\n",
    "    del n_atoms\n",
    "    gc.collect()\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "atomic_radius = {'H': 0.43, 'C': 0.82, 'N': 0.8, 'O': 0.78, 'F': 0.76}\n",
    "electronegativity = {'H': 2.2, 'C': 2.55, 'N': 3.04, 'O': 3.44, 'F': 3.98}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "161"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_model1 = make_pipeline(MoreStructureProperties(atomic_radius,electronegativity))\n",
    "pipeline_model2 = make_pipeline(MakeMoreFeatures())\n",
    "\n",
    "train = pd.read_csv('../data/raw/train.csv')\n",
    "test = pd.read_csv('../data/raw/test.csv')\n",
    "struct = pd.read_csv('../data/raw/structures.csv')\n",
    "structures_yukawa = pd.read_csv('../data/raw/structures_yukawa.csv')\n",
    "\n",
    "struct = pd.concat([struct, structures_yukawa], axis=1)\n",
    "del structures_yukawa\n",
    "\n",
    "struct = reduce_mem_usage(struct,verbose=False)\n",
    "gc.collect()\n",
    "\n",
    "train = get_features(train, struct.copy())\n",
    "test = get_features(test, struct.copy())\n",
    "y = train['scalar_coupling_constant']\n",
    "\n",
    "del struct\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/CHAMPS/experiments/utils.py:184: RuntimeWarning: Mean of empty slice\n",
      "  X['dist_mean_no_bond'] = np.nanmean(np.where(dist_matrix_no_bond==0,np.nan,dist_matrix), axis=1)\n",
      "/usr/local/lib/python3.7/dist-packages/numpy/lib/nanfunctions.py:1628: RuntimeWarning: Degrees of freedom <= 0 for slice.\n",
      "  keepdims=keepdims)\n",
      "/usr/local/lib/python3.7/dist-packages/numpy/lib/function_base.py:3405: RuntimeWarning: All-NaN slice encountered\n",
      "  r = func(a, **kwargs)\n",
      "/root/CHAMPS/experiments/utils.py:194: RuntimeWarning: All-NaN slice encountered\n",
      "  X['dist_no_bond_min'] = np.nanmin(np.where(dist_matrix_no_bond==0,np.nan,dist_matrix), axis=1)\n",
      "/root/CHAMPS/experiments/utils.py:195: RuntimeWarning: All-NaN slice encountered\n",
      "  X['dist_no_bond_max'] = np.nanmax(np.where(dist_matrix_no_bond==0,np.nan,dist_matrix), axis=1)\n"
     ]
    }
   ],
   "source": [
    "struct = pd.read_csv('../data/raw/structures.csv')\n",
    "struct = pipeline_model1.fit_transform(struct)\n",
    "\n",
    "train = feat_from_structures(train,struct)\n",
    "train = pipeline_model2.fit_transform(\n",
    "    train.drop(['scalar_coupling_constant'], axis=1),\n",
    "    train['scalar_coupling_constant']\n",
    ")\n",
    "\n",
    "test = feat_from_structures(test,struct)\n",
    "test = pipeline_model2.transform(test)\n",
    "\n",
    "train = reduce_mem_usage(train,verbose=False)\n",
    "test = reduce_mem_usage(test,verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.24.2'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "giba_columns = ['inv_dist0', 'inv_dist1', 'inv_distP', 'inv_dist0R', 'inv_dist1R', 'inv_distPR', 'inv_dist0E', 'inv_dist1E', 'inv_distPE', 'linkM0',\n",
    "         'linkM1', 'min_molecule_atom_0_dist_xyz', 'mean_molecule_atom_0_dist_xyz', 'max_molecule_atom_0_dist_xyz', 'sd_molecule_atom_0_dist_xyz', 'min_molecule_atom_1_dist_xyz',\n",
    "         'mean_molecule_atom_1_dist_xyz', 'max_molecule_atom_1_dist_xyz', 'sd_molecule_atom_1_dist_xyz', 'coulomb_C.x', 'coulomb_F.x', 'coulomb_H.x', 'coulomb_N.x',\n",
    "         'coulomb_O.x', 'yukawa_C.x', 'yukawa_F.x', 'yukawa_H.x', 'yukawa_N.x', 'yukawa_O.x', 'vander_C.x', 'vander_F.x', 'vander_H.x', 'vander_N.x', 'vander_O.x',\n",
    "         'coulomb_C.y', 'coulomb_F.y', 'coulomb_H.y', 'coulomb_N.y', 'coulomb_O.y', 'yukawa_C.y', 'yukawa_F.y', 'yukawa_H.y', 'yukawa_N.y', 'yukawa_O.y', 'vander_C.y',\n",
    "         'vander_F.y', 'vander_H.y', 'vander_N.y', 'vander_O.y', 'distC0', 'distH0', 'distN0', 'distC1', 'distH1', 'distN1', 'adH1', 'adH2', 'adH3', 'adH4', 'adC1',\n",
    "         'adC2', 'adC3', 'adC4', 'adN1', 'adN2', 'adN3', 'adN4', 'NC', 'NH', 'NN', 'NF', 'NO']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mem. usage decreased to 635.26 Mb (75.2% reduction)\n",
      "Mem. usage decreased to 339.30 Mb (75.3% reduction)\n"
     ]
    }
   ],
   "source": [
    "train_giba_t = pd.read_csv('../data/raw/train_giba.csv.gz',\n",
    "                        header=0,  usecols=giba_columns)\n",
    "test_giba_t = pd.read_csv('../data/raw/test_giba.csv.gz',\n",
    "                       header=0,  usecols=giba_columns)\n",
    "train_giba_t = reduce_mem_usage(train_giba_t)\n",
    "test_giba_t = reduce_mem_usage(test_giba_t)\n",
    "\n",
    "train = pd.concat((train,train_giba_t),axis=1)\n",
    "test = pd.concat((test,test_giba_t),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "69"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del train_giba_t\n",
    "del test_giba_t\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_features = ['type',   'x_x', 'y_x','z_x', 'atom_y', 'x_y', 'y_y',\n",
    "       'z_y', 'n_bonds_y', 'C', 'F', 'H', 'N', 'O', 'distance', 'dist_mean_x','dist_mean_y',\n",
    "        'x_dist_abs', 'y_dist_abs', 'z_dist_abs','inv_distance3',\n",
    "       'molecule_atom_1_dist_std_diff','molecule_dist_mean_x',\n",
    "       'molecule_dist_mean_y','molecule_dist_std_x','molecule_dist_std_y','molecule_atom_0_dist_mean',\n",
    "       'molecule_atom_1_dist_mean','dist_mean_bond_y',\n",
    "       'n_no_bonds_x','n_no_bonds_y', 'dist_std_x', 'dist_std_y','dist_min_x','dist_min_y','dist_max_x', 'dist_max_y',\n",
    "       'molecule_dist_range_x','molecule_dist_range_y', 'dimension_x', 'dimension_y','dimension_z','molecule_dist_mean_bond_x',\n",
    "       'molecule_dist_mean_bond_x','dist_mean_no_bond_x','dist_mean_no_bond_y',\n",
    "       'dist_std_bond_y','dist_bond_min_y','dist_bond_max_y',\n",
    "       'range_dist_bond_y','dist_std_no_bond_x','dist_std_no_bond_y', 'dist_no_bond_min_x','dist_no_bond_min_y','dist_no_bond_max_x',\n",
    "       'dist_no_bond_max_y', 'range_dist_no_bond_x','range_dist_no_bond_y','dist_median_bond_y','dist_median_x',\n",
    "       'dist_median_y','dist_median_no_bond_x','dist_median_no_bond_y','molecule_type_dist_min','molecule_type_dist_max',\n",
    "       'molecule_dist_mean_no_bond_x','molecule_dist_mean_no_bond_y', 'n_diff_y','molecule_atom_index_0_dist_min_div','molecule_atom_index_0_dist_std_div',\n",
    "        'molecule_atom_index_0_dist_mean','molecule_atom_index_0_dist_max','molecule_atom_index_1_dist_mean','molecule_atom_index_1_dist_max',\n",
    "       'molecule_atom_index_1_dist_min','molecule_atom_index_1_dist_min_div','molecule_atom_index_1_dist_std_diff','molecule_atom_index_0_dist_mean_diff',\n",
    "        'molecule_atom_index_1_dist_mean_div','molecule_atom_index_1_dist_min_diff', 'rc_A', 'rc_B', 'rc_C', 'mu', 'alpha', 'homo', 'lumo', 'gap', 'zpve', 'Cv',\n",
    "         'freqs_min', 'freqs_max', 'freqs_mean', 'mulliken_min', 'mulliken_max', 'mulliken_atom_0', 'mulliken_atom_1',\n",
    "         'dist_C_0_x', 'dist_C_1_x', 'dist_C_2_x', 'dist_C_3_x', 'dist_C_4_x', 'dist_F_0_x', 'dist_F_1_x', 'dist_F_2_x', 'dist_H_0_x',\n",
    "         'dist_H_1_x', 'dist_H_2_x', 'dist_H_3_x', 'dist_H_4_x', 'dist_N_0_x', 'dist_N_1_x', 'dist_N_2_x', 'dist_N_3_x', 'dist_N_4_x', 'dist_O_0_x', 'dist_O_1_x',\n",
    "         'dist_O_2_x', 'dist_O_3_x', 'dist_O_4_x', 'dist_C_0_y', 'dist_C_1_y', 'dist_C_2_y', 'dist_C_3_y', 'dist_C_4_y', 'dist_F_0_y', 'dist_F_1_y', 'dist_F_2_y',\n",
    "         'dist_F_3_y', 'dist_F_4_y', 'dist_H_0_y', 'dist_H_1_y', 'dist_H_2_y', 'dist_H_3_y', 'dist_H_4_y', 'dist_N_0_y', 'dist_N_1_y', 'dist_N_2_y', 'dist_N_3_y',\n",
    "         'dist_N_4_y', 'dist_O_0_y', 'dist_O_1_y', 'dist_O_2_y', 'dist_O_3_y', 'dist_O_4_y','distance_closest_0', 'distance_closest_1', 'distance_farthest_0',\n",
    "         'distance_farthest_1','cos_c0_c1', 'cos_f0_f1','cos_c0_f0', 'cos_c1_f1', 'cos_center0_center1', 'cos_c0', 'cos_c1', 'cos_f0', 'cos_f1',\n",
    "         'cos_center0', 'cos_center1'] + giba_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_features = ['atom_y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "X = train[all_features]\n",
    "\n",
    "n_folds = 10\n",
    "\n",
    "seed = 42\n",
    "\n",
    "params = {\n",
    "    'num_leaves': 50,\n",
    "    'min_child_samples': 79,\n",
    "    'min_data_in_leaf': 100,\n",
    "    'objective': 'regression',\n",
    "    'max_depth': 9,\n",
    "    'learning_rate': 0.2,\n",
    "    \"boosting_type\": \"gbdt\",\n",
    "    \"subsample_freq\": 1,\n",
    "    \"subsample\": 0.9,\n",
    "    \"bagging_seed\": 11,\n",
    "    \"metric\": 'mae',\n",
    "    \"verbosity\": -1,\n",
    "    'reg_alpha': 0.1,\n",
    "    'reg_lambda': 0.3,\n",
    "    'colsample_bytree': 1.0,\n",
    "    'num_iterations': 20000\n",
    "}\n",
    "\n",
    "folds = KFold(n_splits=n_folds, shuffle=True, random_state=seed)\n",
    "\n",
    "metaparams = {\n",
    "    'rf_n_estimators': 100,\n",
    "    'importance_num_iterations': 1000\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import fast_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = test[all_features]\n",
    "sub = pd.DataFrame()\n",
    "sub['id'] = test['id']\n",
    "sub['type'] = test['type']\n",
    "pred_sub = np.zeros(sub.shape[0])\n",
    "all_features.pop(0)\n",
    "unique_types = train['type'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOG_FILENAME = f\"LGB_log_{str(datetime.date.today())}-{str(datetime.datetime.now().hour)}.txt\"\n",
    "def logprint(*args):\n",
    "    logstring = f\"[{str(datetime.datetime.now())}] - \" + (\" \".join(list(args)))\n",
    "    with open(LOG_FILENAME, 'a+', encoding='utf-8') as file:\n",
    "        file.write(logstring + \"\\n\")\n",
    "        \n",
    "    print(logstring)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019-07-26 10:28:00.934841] - Beginning the training\n",
      "[2019-07-26 10:28:00.935019] - # folds: 5\n",
      "[2019-07-26 10:28:00.935159] - metaparams: {'rf_n_estimators': 10, 'importance_num_iterations': 1000}\n",
      "[2019-07-26 10:28:00.935752] - lgb params: {'num_leaves': 50, 'min_child_samples': 79, 'min_data_in_leaf': 100, 'objective': 'regression', 'max_depth': 9, 'learning_rate': 0.2, 'boosting_type': 'gbdt', 'subsample_freq': 1, 'subsample': 0.9, 'bagging_seed': 11, 'metric': 'mae', 'verbosity': -1, 'reg_alpha': 0.1, 'reg_lambda': 0.3, 'colsample_bytree': 1.0, 'num_iterations': 10000}\n",
      "[2019-07-26 10:28:01.122732] - --------------------------------------------------\n",
      "[2019-07-26 10:28:01.122920] - Training for split 1\n",
      "[2019-07-26 10:28:04.002383] -      Type: 1JHC\n",
      "[2019-07-26 10:28:04.739998] -      training and applying feature classifiers\n",
      "[2019-07-26 10:28:04.740602] -      memory footprint:11.8243 GB\n",
      "[2019-07-26 10:37:15.904456] -      training feature importance estimator\n",
      "[2019-07-26 10:37:15.905049] -      memory footprint:13.7739 GB\n",
      "[2019-07-26 10:38:34.910548] -     Top20 features: ['adC2', 'adC1', 'adC3', 'dist_no_bond_min_y', 'mulliken_atom_0', 'dist_median_bond_y', 'mulliken_atom_1', 'dist_no_bond_min_x', 'rf04', 'inv_dist0R', 'adN1', 'cos_c0_f0', 'dist_std_bond_y', 'adC4', 'dist_bond_max_y', 'dist_mean_bond_y', 'distC0', 'yukawa_O.y', 'inv_dist1R', 'vander_O.y']\n",
      "[2019-07-26 10:38:34.910812] -     Training final estiamtor for 1JHC\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/lightgbm/engine.py:118: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 200 rounds.\n",
      "[500]\ttrain's l1: 0.838954\tval's l1: 0.948793\n",
      "[1000]\ttrain's l1: 0.683237\tval's l1: 0.846056\n",
      "[1500]\ttrain's l1: 0.589249\tval's l1: 0.792758\n",
      "[2000]\ttrain's l1: 0.52253\tval's l1: 0.760159\n",
      "[2500]\ttrain's l1: 0.470108\tval's l1: 0.737563\n",
      "[3000]\ttrain's l1: 0.427097\tval's l1: 0.720795\n",
      "[3500]\ttrain's l1: 0.391529\tval's l1: 0.707892\n",
      "[4000]\ttrain's l1: 0.360967\tval's l1: 0.697235\n",
      "[4500]\ttrain's l1: 0.334118\tval's l1: 0.688702\n",
      "[5000]\ttrain's l1: 0.31038\tval's l1: 0.681913\n",
      "[5500]\ttrain's l1: 0.289086\tval's l1: 0.675626\n",
      "[6000]\ttrain's l1: 0.270068\tval's l1: 0.6708\n",
      "[6500]\ttrain's l1: 0.252888\tval's l1: 0.66644\n",
      "[7000]\ttrain's l1: 0.236976\tval's l1: 0.662532\n",
      "[7500]\ttrain's l1: 0.2228\tval's l1: 0.659584\n",
      "[8000]\ttrain's l1: 0.209562\tval's l1: 0.656577\n",
      "[8500]\ttrain's l1: 0.197455\tval's l1: 0.654191\n",
      "[9000]\ttrain's l1: 0.186178\tval's l1: 0.652033\n",
      "[9500]\ttrain's l1: 0.175806\tval's l1: 0.650009\n",
      "[10000]\ttrain's l1: 0.166192\tval's l1: 0.648338\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[10000]\ttrain's l1: 0.166192\tval's l1: 0.648338\n",
      "[2019-07-26 10:43:38.901265] -      Training finished, predicting something\n",
      "[2019-07-26 10:44:41.393928] -      memory footprint:14.2272 GB\n",
      "[2019-07-26 10:44:41.394084] -      reducing amount of garbadge\n",
      "[2019-07-26 10:44:41.514296] -      garbadge eliminated\n",
      "[2019-07-26 10:44:41.514763] -      memory footprint:11.8885 GB\n",
      "[2019-07-26 10:44:41.514818] -      Type: 2JHH\n",
      "[2019-07-26 10:44:42.077327] -      training and applying feature classifiers\n",
      "[2019-07-26 10:44:42.077767] -      memory footprint:11.8883 GB\n",
      "[2019-07-26 10:49:16.661421] -      training feature importance estimator\n",
      "[2019-07-26 10:49:16.661940] -      memory footprint:12.7930 GB\n",
      "[2019-07-26 10:50:02.534293] -     Top20 features: ['adC3', 'adC2', 'adN1', 'cos_c1_f1', 'rf_00', 'cos_f0_f1', 'mulliken_atom_0', 'mulliken_atom_1', 'cos_c0_c1', 'distance_farthest_1', 'cos_c0_f0', 'cos_f0', 'adC4', 'cos_f1', 'inv_distPE', 'rf_02', 'rf_03', 'dist_mean_bond_y', 'gap', 'adH1']\n",
      "[2019-07-26 10:50:02.534442] -     Training final estiamtor for 2JHH\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/lightgbm/engine.py:118: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 200 rounds.\n",
      "[500]\ttrain's l1: 0.156575\tval's l1: 0.194732\n",
      "[1000]\ttrain's l1: 0.120087\tval's l1: 0.174298\n",
      "[1500]\ttrain's l1: 0.0993682\tval's l1: 0.165269\n",
      "[2000]\ttrain's l1: 0.0848221\tval's l1: 0.160086\n",
      "[2500]\ttrain's l1: 0.0739003\tval's l1: 0.15678\n",
      "[3000]\ttrain's l1: 0.0650491\tval's l1: 0.154417\n",
      "[3500]\ttrain's l1: 0.0578219\tval's l1: 0.152801\n",
      "[4000]\ttrain's l1: 0.0517264\tval's l1: 0.151571\n",
      "[4500]\ttrain's l1: 0.0464849\tval's l1: 0.150601\n",
      "[5000]\ttrain's l1: 0.0420132\tval's l1: 0.149922\n",
      "[5500]\ttrain's l1: 0.0380624\tval's l1: 0.149293\n",
      "[6000]\ttrain's l1: 0.0345967\tval's l1: 0.148764\n",
      "[6500]\ttrain's l1: 0.03153\tval's l1: 0.148367\n",
      "[7000]\ttrain's l1: 0.0287826\tval's l1: 0.14804\n",
      "[7500]\ttrain's l1: 0.0263498\tval's l1: 0.147776\n",
      "[8000]\ttrain's l1: 0.0241221\tval's l1: 0.147536\n",
      "[8500]\ttrain's l1: 0.022137\tval's l1: 0.147355\n",
      "[9000]\ttrain's l1: 0.0203466\tval's l1: 0.147206\n",
      "[9500]\ttrain's l1: 0.0187314\tval's l1: 0.147077\n",
      "[10000]\ttrain's l1: 0.0172575\tval's l1: 0.146974\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[10000]\ttrain's l1: 0.0172575\tval's l1: 0.146974\n",
      "[2019-07-26 10:53:12.581016] -      Training finished, predicting something\n",
      "[2019-07-26 10:53:47.653940] -      memory footprint:12.9569 GB\n",
      "[2019-07-26 10:53:47.654091] -      reducing amount of garbadge\n",
      "[2019-07-26 10:53:47.731655] -      garbadge eliminated\n",
      "[2019-07-26 10:53:47.732165] -      memory footprint:11.9160 GB\n",
      "[2019-07-26 10:53:47.732224] -      Type: 1JHN\n",
      "[2019-07-26 10:53:48.128410] -      training and applying feature classifiers\n",
      "[2019-07-26 10:53:48.128839] -      memory footprint:11.9160 GB\n",
      "[2019-07-26 10:54:11.239071] -      training feature importance estimator\n",
      "[2019-07-26 10:54:11.239590] -      memory footprint:11.9160 GB\n",
      "[2019-07-26 10:54:22.361473] -     Top20 features: ['mulliken_atom_0', 'dist_mean_bond_y', 'mulliken_atom_1', 'inv_dist1R', 'dist_no_bond_min_x', 'adC2', 'inv_dist0R', 'adC3', 'dist_no_bond_min_y', 'rf_02', 'dist_std_bond_y', 'rf_00', 'adN1', 'inv_distPR', 'cos_c0_f0', 'linkM0', 'dist_median_bond_y', 'linkM1', 'adC4', 'adC1']\n",
      "[2019-07-26 10:54:22.361630] -     Training final estiamtor for 1JHN\n",
      "Training until validation scores don't improve for 200 rounds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/lightgbm/engine.py:118: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[500]\ttrain's l1: 0.145914\tval's l1: 0.366529\n",
      "[1000]\ttrain's l1: 0.0668429\tval's l1: 0.354313\n",
      "[1500]\ttrain's l1: 0.0331345\tval's l1: 0.351606\n",
      "[2000]\ttrain's l1: 0.017315\tval's l1: 0.350952\n",
      "[2500]\ttrain's l1: 0.00951313\tval's l1: 0.350644\n",
      "[3000]\ttrain's l1: 0.00560445\tval's l1: 0.350536\n",
      "[3500]\ttrain's l1: 0.00357489\tval's l1: 0.350485\n",
      "[4000]\ttrain's l1: 0.00246854\tval's l1: 0.350458\n",
      "[4500]\ttrain's l1: 0.00181697\tval's l1: 0.350433\n",
      "[5000]\ttrain's l1: 0.0014264\tval's l1: 0.350418\n",
      "Early stopping, best iteration is:\n",
      "[5163]\ttrain's l1: 0.00133785\tval's l1: 0.350411\n",
      "[2019-07-26 10:55:00.621068] -      Training finished, predicting something\n",
      "[2019-07-26 10:55:03.534779] -      memory footprint:11.9157 GB\n",
      "[2019-07-26 10:55:03.534922] -      reducing amount of garbadge\n",
      "[2019-07-26 10:55:03.576141] -      garbadge eliminated\n",
      "[2019-07-26 10:55:03.576601] -      memory footprint:11.9157 GB\n",
      "[2019-07-26 10:55:03.576656] -      Type: 2JHN\n",
      "[2019-07-26 10:55:04.050385] -      training and applying feature classifiers\n",
      "[2019-07-26 10:55:04.050870] -      memory footprint:11.9157 GB\n",
      "[2019-07-26 10:56:16.490068] -      training feature importance estimator\n",
      "[2019-07-26 10:56:16.490331] -      memory footprint:11.9157 GB\n",
      "[2019-07-26 10:56:34.785100] -     Top20 features: ['adC2', 'cos_c0', 'cos_f0', 'adC3', 'cos_c0_c1', 'cos_c0_f0', 'adC4', 'rf04', 'rf_10', 'dist_no_bond_min_x', 'mulliken_atom_1', 'cos_c1', 'rf_02', 'mulliken_atom_0', 'rf_03', 'dist_median_bond_y', 'rf05', 'dist_mean_bond_y', 'dist_min_y', 'molecule_atom_index_0_dist_min_div']\n",
      "[2019-07-26 10:56:34.785260] -     Training final estiamtor for 2JHN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/lightgbm/engine.py:118: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 200 rounds.\n",
      "[500]\ttrain's l1: 0.12274\tval's l1: 0.197773\n",
      "[1000]\ttrain's l1: 0.0795551\tval's l1: 0.183867\n",
      "[1500]\ttrain's l1: 0.0565798\tval's l1: 0.178441\n",
      "[2000]\ttrain's l1: 0.0418233\tval's l1: 0.175956\n",
      "[2500]\ttrain's l1: 0.0316945\tval's l1: 0.17444\n",
      "[3000]\ttrain's l1: 0.0244483\tval's l1: 0.173599\n",
      "[3500]\ttrain's l1: 0.0191513\tval's l1: 0.173032\n",
      "[4000]\ttrain's l1: 0.0151645\tval's l1: 0.17266\n",
      "[4500]\ttrain's l1: 0.012145\tval's l1: 0.1724\n",
      "[5000]\ttrain's l1: 0.00982536\tval's l1: 0.172237\n",
      "[5500]\ttrain's l1: 0.00803143\tval's l1: 0.172108\n",
      "[6000]\ttrain's l1: 0.0066486\tval's l1: 0.172016\n",
      "[6500]\ttrain's l1: 0.00555949\tval's l1: 0.171958\n",
      "[7000]\ttrain's l1: 0.00470096\tval's l1: 0.171904\n",
      "[7500]\ttrain's l1: 0.00401924\tval's l1: 0.171864\n",
      "[8000]\ttrain's l1: 0.00347403\tval's l1: 0.171833\n",
      "[8500]\ttrain's l1: 0.00303588\tval's l1: 0.171805\n",
      "[9000]\ttrain's l1: 0.0026797\tval's l1: 0.171783\n",
      "[9500]\ttrain's l1: 0.00238939\tval's l1: 0.171772\n",
      "[10000]\ttrain's l1: 0.00214458\tval's l1: 0.171761\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[10000]\ttrain's l1: 0.00214458\tval's l1: 0.171761\n",
      "[2019-07-26 10:58:08.715768] -      Training finished, predicting something\n",
      "[2019-07-26 10:58:21.507439] -      memory footprint:11.9182 GB\n",
      "[2019-07-26 10:58:21.507610] -      reducing amount of garbadge\n",
      "[2019-07-26 10:58:21.548368] -      garbadge eliminated\n",
      "[2019-07-26 10:58:21.548862] -      memory footprint:11.9182 GB\n",
      "[2019-07-26 10:58:21.548915] -      Type: 2JHC\n",
      "[2019-07-26 10:58:22.531284] -      training and applying feature classifiers\n",
      "[2019-07-26 10:58:22.531556] -      memory footprint:11.9182 GB\n",
      "[2019-07-26 11:15:13.123931] -      training feature importance estimator\n",
      "[2019-07-26 11:15:13.124473] -      memory footprint:15.2626 GB\n",
      "[2019-07-26 11:16:59.816590] -     Top20 features: ['adC2', 'rf04', 'cos_f0', 'dist_no_bond_min_x', 'cos_c0', 'adN1', 'adC3', 'cos_c0_f0', 'mulliken_atom_1', 'inv_dist0R', 'dist_no_bond_min_y', 'dist_bond_max_y', 'molecule_atom_index_0_dist_min_div', 'rf_00', 'vander_O.y', 'distance', 'gap', 'dist_mean_bond_y', 'cos_c0_c1', 'dist_median_bond_y']\n",
      "[2019-07-26 11:16:59.816760] -     Training final estiamtor for 2JHC\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/lightgbm/engine.py:118: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 200 rounds.\n",
      "[500]\ttrain's l1: 0.400513\tval's l1: 0.436492\n",
      "[1000]\ttrain's l1: 0.337286\tval's l1: 0.390822\n",
      "[1500]\ttrain's l1: 0.298693\tval's l1: 0.366837\n",
      "[2000]\ttrain's l1: 0.270634\tval's l1: 0.351047\n",
      "[2500]\ttrain's l1: 0.248681\tval's l1: 0.339283\n",
      "[3000]\ttrain's l1: 0.230719\tval's l1: 0.330831\n",
      "[3500]\ttrain's l1: 0.215497\tval's l1: 0.323967\n",
      "[4000]\ttrain's l1: 0.202195\tval's l1: 0.318291\n",
      "[4500]\ttrain's l1: 0.190574\tval's l1: 0.313824\n",
      "[5000]\ttrain's l1: 0.180243\tval's l1: 0.30999\n",
      "[5500]\ttrain's l1: 0.170958\tval's l1: 0.306723\n",
      "[6000]\ttrain's l1: 0.162415\tval's l1: 0.303846\n",
      "[6500]\ttrain's l1: 0.154655\tval's l1: 0.301373\n",
      "[7000]\ttrain's l1: 0.147506\tval's l1: 0.29928\n",
      "[7500]\ttrain's l1: 0.140949\tval's l1: 0.297336\n",
      "[8000]\ttrain's l1: 0.134841\tval's l1: 0.295635\n",
      "[8500]\ttrain's l1: 0.1291\tval's l1: 0.294035\n",
      "[9000]\ttrain's l1: 0.123695\tval's l1: 0.292688\n",
      "[9500]\ttrain's l1: 0.118677\tval's l1: 0.29141\n",
      "[10000]\ttrain's l1: 0.113957\tval's l1: 0.290191\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[10000]\ttrain's l1: 0.113957\tval's l1: 0.290191\n",
      "[2019-07-26 11:23:57.247020] -      Training finished, predicting something\n",
      "[2019-07-26 11:25:32.947924] -      memory footprint:15.8249 GB\n",
      "[2019-07-26 11:25:32.948083] -      reducing amount of garbadge\n",
      "[2019-07-26 11:25:33.107087] -      garbadge eliminated\n",
      "[2019-07-26 11:25:33.107674] -      memory footprint:12.0221 GB\n",
      "[2019-07-26 11:25:33.107729] -      Type: 3JHH\n",
      "[2019-07-26 11:25:33.827909] -      training and applying feature classifiers\n",
      "[2019-07-26 11:25:33.828183] -      memory footprint:12.0221 GB\n",
      "[2019-07-26 11:33:15.529675] -      training feature importance estimator\n",
      "[2019-07-26 11:33:15.530190] -      memory footprint:13.6458 GB\n",
      "[2019-07-26 11:34:16.867577] -     Top20 features: ['cos_c0_c1', 'dist_no_bond_min_y', 'dist_no_bond_min_x', 'cos_c1', 'rf04', 'cos_c0', 'adC3', 'distance_farthest_1', 'adC2', 'adC4', 'cos_f0', 'rf_03', 'adN1', 'cos_c1_f1', 'cos_f1', 'cos_c0_f0', 'dist_mean_bond_y', 'cos_f0_f1', 'rf_02', 'gap']\n",
      "[2019-07-26 11:34:16.867896] -     Training final estiamtor for 3JHH\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/lightgbm/engine.py:118: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 200 rounds.\n",
      "[500]\ttrain's l1: 0.206316\tval's l1: 0.236743\n",
      "[1000]\ttrain's l1: 0.165063\tval's l1: 0.210587\n",
      "[1500]\ttrain's l1: 0.140699\tval's l1: 0.197507\n",
      "[2000]\ttrain's l1: 0.123477\tval's l1: 0.189561\n",
      "[2500]\ttrain's l1: 0.110291\tval's l1: 0.184014\n",
      "[3000]\ttrain's l1: 0.0996382\tval's l1: 0.179972\n",
      "[3500]\ttrain's l1: 0.0907177\tval's l1: 0.177112\n",
      "[4000]\ttrain's l1: 0.0830437\tval's l1: 0.174705\n",
      "[4500]\ttrain's l1: 0.0764289\tval's l1: 0.172815\n",
      "[5000]\ttrain's l1: 0.0706876\tval's l1: 0.171189\n",
      "[5500]\ttrain's l1: 0.0655887\tval's l1: 0.169865\n",
      "[6000]\ttrain's l1: 0.0609811\tval's l1: 0.168786\n",
      "[6500]\ttrain's l1: 0.056825\tval's l1: 0.167875\n",
      "[7000]\ttrain's l1: 0.0530803\tval's l1: 0.167074\n",
      "[7500]\ttrain's l1: 0.0496656\tval's l1: 0.166357\n",
      "[8000]\ttrain's l1: 0.0465731\tval's l1: 0.1658\n",
      "[8500]\ttrain's l1: 0.0437131\tval's l1: 0.165291\n",
      "[9000]\ttrain's l1: 0.0410969\tval's l1: 0.164852\n",
      "[9500]\ttrain's l1: 0.0386796\tval's l1: 0.164486\n",
      "[10000]\ttrain's l1: 0.0364218\tval's l1: 0.164151\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[10000]\ttrain's l1: 0.0364218\tval's l1: 0.164151\n",
      "[2019-07-26 11:38:35.170336] -      Training finished, predicting something\n",
      "[2019-07-26 11:39:28.403731] -      memory footprint:13.8612 GB\n",
      "[2019-07-26 11:39:28.403870] -      reducing amount of garbadge\n",
      "[2019-07-26 11:39:28.505572] -      garbadge eliminated\n",
      "[2019-07-26 11:39:28.506056] -      memory footprint:12.0246 GB\n",
      "[2019-07-26 11:39:28.506112] -      Type: 3JHC\n",
      "[2019-07-26 11:39:29.718855] -      training and applying feature classifiers\n",
      "[2019-07-26 11:39:29.719401] -      memory footprint:12.0246 GB\n",
      "[2019-07-26 12:04:20.471348] -      training feature importance estimator\n",
      "[2019-07-26 12:04:20.471881] -      memory footprint:16.9318 GB\n",
      "[2019-07-26 12:06:47.756613] -     Top20 features: ['cos_c0', 'adC2', 'adC3', 'rf04', 'dist_no_bond_min_x', 'cos_f0', 'adN1', 'dist_no_bond_min_y', 'molecule_atom_index_0_dist_min_div', 'cos_c1', 'cos_c0_c1', 'cos_c0_f0', 'adC4', 'rf_10', 'gap', 'distance', 'rf_03', 'adC1', 'rf05', 'mulliken_atom_1']\n",
      "[2019-07-26 12:06:47.756766] -     Training final estiamtor for 3JHC\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/lightgbm/engine.py:118: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 200 rounds.\n",
      "[500]\ttrain's l1: 0.432041\tval's l1: 0.457657\n",
      "[1000]\ttrain's l1: 0.372714\tval's l1: 0.413222\n",
      "[1500]\ttrain's l1: 0.335518\tval's l1: 0.388399\n",
      "[2000]\ttrain's l1: 0.30764\tval's l1: 0.371332\n",
      "[2500]\ttrain's l1: 0.286282\tval's l1: 0.359325\n",
      "[3000]\ttrain's l1: 0.267332\tval's l1: 0.348888\n",
      "[3500]\ttrain's l1: 0.251447\tval's l1: 0.340865\n",
      "[4000]\ttrain's l1: 0.237788\tval's l1: 0.334342\n",
      "[4500]\ttrain's l1: 0.225603\tval's l1: 0.328812\n",
      "[5000]\ttrain's l1: 0.214822\tval's l1: 0.324288\n",
      "[5500]\ttrain's l1: 0.204941\tval's l1: 0.320267\n",
      "[6000]\ttrain's l1: 0.19616\tval's l1: 0.31687\n",
      "[6500]\ttrain's l1: 0.187856\tval's l1: 0.313707\n",
      "[7000]\ttrain's l1: 0.180272\tval's l1: 0.310976\n",
      "[7500]\ttrain's l1: 0.173236\tval's l1: 0.308619\n",
      "[8000]\ttrain's l1: 0.166648\tval's l1: 0.306487\n",
      "[8500]\ttrain's l1: 0.160503\tval's l1: 0.304658\n",
      "[9000]\ttrain's l1: 0.15484\tval's l1: 0.303084\n",
      "[9500]\ttrain's l1: 0.149417\tval's l1: 0.301586\n",
      "[10000]\ttrain's l1: 0.144262\tval's l1: 0.300147\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[10000]\ttrain's l1: 0.144262\tval's l1: 0.300147\n",
      "[2019-07-26 12:15:44.203655] -      Training finished, predicting something\n",
      "[2019-07-26 12:17:47.345664] -      memory footprint:17.6247 GB\n",
      "[2019-07-26 12:17:47.345807] -      reducing amount of garbadge\n",
      "[2019-07-26 12:17:47.543227] -      garbadge eliminated\n",
      "[2019-07-26 12:17:47.543828] -      memory footprint:12.4632 GB\n",
      "[2019-07-26 12:17:47.543881] -      Type: 3JHN\n",
      "[2019-07-26 12:17:48.053912] -      training and applying feature classifiers\n",
      "[2019-07-26 12:17:48.054206] -      memory footprint:12.4632 GB\n",
      "[2019-07-26 12:19:43.680626] -      training feature importance estimator\n",
      "[2019-07-26 12:19:43.681161] -      memory footprint:12.8634 GB\n",
      "[2019-07-26 12:20:06.719626] -     Top20 features: ['cos_c0', 'rf04', 'adC2', 'cos_c1', 'cos_c0_c1', 'adC3', 'dist_no_bond_min_y', 'dist_no_bond_min_x', 'cos_c0_f0', 'rf_10', 'molecule_atom_index_0_dist_min_div', 'adC4', 'mulliken_atom_1', 'dist_bond_max_y', 'cos_f0', 'adN1', 'distance', 'rf05', 'dist_median_bond_y', 'molecule_type_dist_min']\n",
      "[2019-07-26 12:20:06.719922] -     Training final estiamtor for 3JHN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/lightgbm/engine.py:118: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 200 rounds.\n",
      "[500]\ttrain's l1: 0.104655\tval's l1: 0.153751\n",
      "[1000]\ttrain's l1: 0.072074\tval's l1: 0.142253\n",
      "[1500]\ttrain's l1: 0.0535285\tval's l1: 0.137481\n",
      "[2000]\ttrain's l1: 0.0413689\tval's l1: 0.134986\n",
      "[2500]\ttrain's l1: 0.032686\tval's l1: 0.133453\n",
      "[3000]\ttrain's l1: 0.0262191\tval's l1: 0.132406\n",
      "[3500]\ttrain's l1: 0.0213275\tval's l1: 0.13176\n",
      "[4000]\ttrain's l1: 0.0174889\tval's l1: 0.131341\n",
      "[4500]\ttrain's l1: 0.0144814\tval's l1: 0.130986\n",
      "[5000]\ttrain's l1: 0.0120709\tval's l1: 0.130783\n",
      "[5500]\ttrain's l1: 0.0101516\tval's l1: 0.130613\n",
      "[6000]\ttrain's l1: 0.00858913\tval's l1: 0.130478\n",
      "[6500]\ttrain's l1: 0.00732621\tval's l1: 0.130368\n",
      "[7000]\ttrain's l1: 0.00629346\tval's l1: 0.130277\n",
      "[7500]\ttrain's l1: 0.00544252\tval's l1: 0.130201\n",
      "[8000]\ttrain's l1: 0.0047475\tval's l1: 0.130147\n",
      "[8500]\ttrain's l1: 0.00416321\tval's l1: 0.130106\n",
      "[9000]\ttrain's l1: 0.00367664\tval's l1: 0.130073\n",
      "[9500]\ttrain's l1: 0.00327035\tval's l1: 0.130052\n",
      "[10000]\ttrain's l1: 0.00293119\tval's l1: 0.130026\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[10000]\ttrain's l1: 0.00293119\tval's l1: 0.130026\n",
      "[2019-07-26 12:21:59.030935] -      Training finished, predicting something\n",
      "[2019-07-26 12:22:15.339128] -      memory footprint:12.7034 GB\n",
      "[2019-07-26 12:22:15.339341] -      reducing amount of garbadge\n",
      "[2019-07-26 12:22:15.397706] -      garbadge eliminated\n",
      "[2019-07-26 12:22:15.398307] -      memory footprint:12.3032 GB\n",
      "[2019-07-26 12:22:16.206952] - Fold result: -1.4311 group-mean log(mae)\n",
      "[2019-07-26 12:22:16.241230] - --------------------------------------------------\n",
      "[2019-07-26 12:22:16.241431] - Training for split 2\n",
      "[2019-07-26 12:22:19.249221] -      Type: 1JHC\n",
      "[2019-07-26 12:22:19.861311] -      training and applying feature classifiers\n",
      "[2019-07-26 12:22:19.861595] -      memory footprint:12.7144 GB\n",
      "[2019-07-26 12:31:26.687278] -      training feature importance estimator\n",
      "[2019-07-26 12:31:26.687780] -      memory footprint:14.4097 GB\n",
      "[2019-07-26 12:32:45.225256] -     Top20 features: ['adC1', 'adC2', 'dist_no_bond_min_y', 'adC3', 'mulliken_atom_0', 'mulliken_atom_1', 'dist_median_bond_y', 'dist_no_bond_min_x', 'rf04', 'adN1', 'inv_dist0R', 'cos_c0_f0', 'dist_std_bond_y', 'adC4', 'dist_bond_max_y', 'dist_mean_bond_y', 'yukawa_O.y', 'inv_dist1R', 'max_molecule_atom_0_dist_xyz', 'distC0']\n",
      "[2019-07-26 12:32:45.225406] -     Training final estiamtor for 1JHC\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/lightgbm/engine.py:118: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 200 rounds.\n",
      "[500]\ttrain's l1: 0.83931\tval's l1: 0.94792\n",
      "[1000]\ttrain's l1: 0.683988\tval's l1: 0.843894\n",
      "[1500]\ttrain's l1: 0.591378\tval's l1: 0.792865\n",
      "[2000]\ttrain's l1: 0.52459\tval's l1: 0.760773\n",
      "[2500]\ttrain's l1: 0.472706\tval's l1: 0.738541\n",
      "[3000]\ttrain's l1: 0.429606\tval's l1: 0.721568\n",
      "[3500]\ttrain's l1: 0.393348\tval's l1: 0.708233\n",
      "[4000]\ttrain's l1: 0.362375\tval's l1: 0.697691\n",
      "[4500]\ttrain's l1: 0.33552\tval's l1: 0.68952\n",
      "[5000]\ttrain's l1: 0.311592\tval's l1: 0.682437\n",
      "[5500]\ttrain's l1: 0.290347\tval's l1: 0.676761\n",
      "[6000]\ttrain's l1: 0.271229\tval's l1: 0.672009\n",
      "[6500]\ttrain's l1: 0.253599\tval's l1: 0.66781\n",
      "[7000]\ttrain's l1: 0.237805\tval's l1: 0.664163\n",
      "[7500]\ttrain's l1: 0.223346\tval's l1: 0.661248\n",
      "[8000]\ttrain's l1: 0.210008\tval's l1: 0.658588\n",
      "[8500]\ttrain's l1: 0.197935\tval's l1: 0.65595\n",
      "[9000]\ttrain's l1: 0.186604\tval's l1: 0.653759\n",
      "[9500]\ttrain's l1: 0.176103\tval's l1: 0.651823\n",
      "[10000]\ttrain's l1: 0.166401\tval's l1: 0.650107\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[10000]\ttrain's l1: 0.166401\tval's l1: 0.650107\n",
      "[2019-07-26 12:37:52.413088] -      Training finished, predicting something\n",
      "[2019-07-26 12:38:55.915882] -      memory footprint:14.6743 GB\n",
      "[2019-07-26 12:38:55.916030] -      reducing amount of garbadge\n",
      "[2019-07-26 12:38:56.017828] -      garbadge eliminated\n",
      "[2019-07-26 12:38:56.018445] -      memory footprint:12.6938 GB\n",
      "[2019-07-26 12:38:56.018502] -      Type: 2JHH\n",
      "[2019-07-26 12:38:56.556344] -      training and applying feature classifiers\n",
      "[2019-07-26 12:38:56.556895] -      memory footprint:12.6936 GB\n",
      "[2019-07-26 12:44:19.139929] -     Top20 features: ['adC3', 'adC2', 'adN1', 'cos_c1_f1', 'cos_c0_c1', 'cos_f0_f1', 'mulliken_atom_1', 'rf_00', 'distance_farthest_1', 'mulliken_atom_0', 'cos_c0_f0', 'adC4', 'cos_f0', 'cos_f1', 'inv_distPE', 'rf_02', 'rf_03', 'gap', 'dist_mean_bond_y', 'inv_distP']\n",
      "[2019-07-26 12:44:19.140085] -     Training final estiamtor for 2JHH\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/lightgbm/engine.py:118: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 200 rounds.\n",
      "[500]\ttrain's l1: 0.156444\tval's l1: 0.194377\n",
      "[1000]\ttrain's l1: 0.120333\tval's l1: 0.174506\n",
      "[1500]\ttrain's l1: 0.0996967\tval's l1: 0.16531\n",
      "[2000]\ttrain's l1: 0.0851934\tval's l1: 0.160005\n",
      "[2500]\ttrain's l1: 0.0742364\tval's l1: 0.1568\n",
      "[3000]\ttrain's l1: 0.0655237\tval's l1: 0.154542\n",
      "[3500]\ttrain's l1: 0.0583085\tval's l1: 0.152889\n",
      "[4000]\ttrain's l1: 0.0521304\tval's l1: 0.151619\n",
      "[4500]\ttrain's l1: 0.0469086\tval's l1: 0.150635\n",
      "[5000]\ttrain's l1: 0.0423306\tval's l1: 0.149793\n",
      "[5500]\ttrain's l1: 0.0384014\tval's l1: 0.149205\n",
      "[6000]\ttrain's l1: 0.0348871\tval's l1: 0.148746\n",
      "[6500]\ttrain's l1: 0.0318077\tval's l1: 0.148264\n",
      "[7000]\ttrain's l1: 0.0290961\tval's l1: 0.147907\n",
      "[7500]\ttrain's l1: 0.0266287\tval's l1: 0.147613\n",
      "[8000]\ttrain's l1: 0.0244144\tval's l1: 0.147379\n",
      "[8500]\ttrain's l1: 0.0224176\tval's l1: 0.147191\n",
      "[9000]\ttrain's l1: 0.0206212\tval's l1: 0.147004\n",
      "[9500]\ttrain's l1: 0.0189843\tval's l1: 0.146855\n",
      "[10000]\ttrain's l1: 0.0175115\tval's l1: 0.146723\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[10000]\ttrain's l1: 0.0175115\tval's l1: 0.146723\n",
      "[2019-07-26 12:47:35.350397] -      Training finished, predicting something\n",
      "[2019-07-26 12:48:12.310920] -      memory footprint:13.5318 GB\n",
      "[2019-07-26 12:48:12.311091] -      reducing amount of garbadge\n",
      "[2019-07-26 12:48:12.384111] -      garbadge eliminated\n",
      "[2019-07-26 12:48:12.384642] -      memory footprint:12.6276 GB\n",
      "[2019-07-26 12:48:12.384696] -      Type: 1JHN\n",
      "[2019-07-26 12:48:12.776324] -      training and applying feature classifiers\n",
      "[2019-07-26 12:48:12.776849] -      memory footprint:12.6273 GB\n",
      "[2019-07-26 12:48:35.550734] -      training feature importance estimator\n",
      "[2019-07-26 12:48:35.551277] -      memory footprint:12.6273 GB\n",
      "[2019-07-26 12:48:47.313449] -     Top20 features: ['mulliken_atom_0', 'dist_mean_bond_y', 'inv_dist1R', 'mulliken_atom_1', 'inv_dist0R', 'dist_no_bond_min_x', 'adC2', 'rf_02', 'adC3', 'dist_no_bond_min_y', 'dist_std_bond_y', 'rf_00', 'adN1', 'dist_median_bond_y', 'rf04', 'cos_c0_f0', 'inv_distPR', 'mulliken_max', 'adC1', 'linkM0']\n",
      "[2019-07-26 12:48:47.313599] -     Training final estiamtor for 1JHN\n",
      "Training until validation scores don't improve for 200 rounds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/lightgbm/engine.py:118: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[500]\ttrain's l1: 0.145504\tval's l1: 0.367084\n",
      "[1000]\ttrain's l1: 0.0666127\tval's l1: 0.354638\n",
      "[1500]\ttrain's l1: 0.0327977\tval's l1: 0.352218\n",
      "[2000]\ttrain's l1: 0.0168616\tval's l1: 0.351254\n",
      "[2500]\ttrain's l1: 0.00922866\tval's l1: 0.350969\n",
      "[3000]\ttrain's l1: 0.00538673\tval's l1: 0.350765\n",
      "[3500]\ttrain's l1: 0.00341806\tval's l1: 0.3507\n",
      "[4000]\ttrain's l1: 0.00235523\tval's l1: 0.350675\n",
      "[4500]\ttrain's l1: 0.00173915\tval's l1: 0.350651\n",
      "[5000]\ttrain's l1: 0.00137535\tval's l1: 0.350639\n",
      "Early stopping, best iteration is:\n",
      "[5076]\ttrain's l1: 0.00133513\tval's l1: 0.350638\n",
      "[2019-07-26 12:49:25.124329] -      Training finished, predicting something\n",
      "[2019-07-26 12:49:28.086905] -      memory footprint:12.6273 GB\n",
      "[2019-07-26 12:49:28.087129] -      reducing amount of garbadge\n",
      "[2019-07-26 12:49:28.128984] -      garbadge eliminated\n",
      "[2019-07-26 12:49:28.129473] -      memory footprint:12.6273 GB\n",
      "[2019-07-26 12:49:28.129528] -      Type: 2JHN\n",
      "[2019-07-26 12:49:28.593019] -      training and applying feature classifiers\n",
      "[2019-07-26 12:49:28.593575] -      memory footprint:12.6273 GB\n",
      "[2019-07-26 12:50:40.971721] -      training feature importance estimator\n",
      "[2019-07-26 12:50:40.972257] -      memory footprint:12.6273 GB\n",
      "[2019-07-26 12:50:59.548739] -     Top20 features: ['adC2', 'cos_c0', 'adC3', 'cos_c0_c1', 'cos_f0', 'adC4', 'cos_c0_f0', 'rf_10', 'dist_no_bond_min_x', 'cos_c1', 'rf04', 'mulliken_atom_1', 'rf_03', 'rf_02', 'mulliken_atom_0', 'dist_median_bond_y', 'rf05', 'dist_min_y', 'gap', 'molecule_atom_index_0_dist_min_div']\n",
      "[2019-07-26 12:50:59.549116] -     Training final estiamtor for 2JHN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/lightgbm/engine.py:118: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 200 rounds.\n",
      "[500]\ttrain's l1: 0.120901\tval's l1: 0.19853\n",
      "[1000]\ttrain's l1: 0.0779805\tval's l1: 0.184636\n",
      "[1500]\ttrain's l1: 0.0548096\tval's l1: 0.179526\n",
      "[2000]\ttrain's l1: 0.0401631\tval's l1: 0.176956\n",
      "[2500]\ttrain's l1: 0.0302102\tval's l1: 0.175697\n",
      "[3000]\ttrain's l1: 0.0231376\tval's l1: 0.175024\n",
      "[3500]\ttrain's l1: 0.0179604\tval's l1: 0.174536\n",
      "[4000]\ttrain's l1: 0.0141469\tval's l1: 0.174222\n",
      "[4500]\ttrain's l1: 0.0112476\tval's l1: 0.17405\n",
      "[5000]\ttrain's l1: 0.00904485\tval's l1: 0.173892\n",
      "[5500]\ttrain's l1: 0.00735494\tval's l1: 0.173798\n",
      "[6000]\ttrain's l1: 0.00606494\tval's l1: 0.173724\n",
      "[6500]\ttrain's l1: 0.00505025\tval's l1: 0.173655\n",
      "[7000]\ttrain's l1: 0.00426264\tval's l1: 0.173612\n",
      "[7500]\ttrain's l1: 0.00363831\tval's l1: 0.173572\n",
      "[8000]\ttrain's l1: 0.00314341\tval's l1: 0.173548\n",
      "[8500]\ttrain's l1: 0.00274115\tval's l1: 0.173527\n",
      "[9000]\ttrain's l1: 0.00241903\tval's l1: 0.173512\n",
      "[9500]\ttrain's l1: 0.00215361\tval's l1: 0.173504\n",
      "[10000]\ttrain's l1: 0.00193268\tval's l1: 0.173493\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[10000]\ttrain's l1: 0.00193268\tval's l1: 0.173493\n",
      "[2019-07-26 12:52:33.247327] -      Training finished, predicting something\n",
      "[2019-07-26 12:52:46.234466] -      memory footprint:12.6272 GB\n",
      "[2019-07-26 12:52:46.234665] -      reducing amount of garbadge\n",
      "[2019-07-26 12:52:46.275756] -      garbadge eliminated\n",
      "[2019-07-26 12:52:46.276249] -      memory footprint:12.6272 GB\n",
      "[2019-07-26 12:52:46.276302] -      Type: 2JHC\n",
      "[2019-07-26 12:52:47.143785] -      training and applying feature classifiers\n",
      "[2019-07-26 12:52:47.144305] -      memory footprint:12.6272 GB\n",
      "[2019-07-26 13:09:33.145602] -      training feature importance estimator\n",
      "[2019-07-26 13:09:33.145858] -      memory footprint:15.7633 GB\n",
      "[2019-07-26 13:11:20.109326] -     Top20 features: ['adC2', 'cos_f0', 'dist_no_bond_min_x', 'rf04', 'cos_c0', 'adC3', 'adN1', 'cos_c0_f0', 'inv_dist0R', 'dist_no_bond_min_y', 'mulliken_atom_1', 'molecule_atom_index_0_dist_min_div', 'dist_mean_bond_y', 'vander_O.y', 'dist_bond_max_y', 'rf_00', 'distance', 'dist_median_bond_y', 'gap', 'cos_c0_c1']\n",
      "[2019-07-26 13:11:20.109490] -     Training final estiamtor for 2JHC\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/lightgbm/engine.py:118: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 200 rounds.\n",
      "[500]\ttrain's l1: 0.399294\tval's l1: 0.43512\n",
      "[1000]\ttrain's l1: 0.336657\tval's l1: 0.391333\n",
      "[1500]\ttrain's l1: 0.29782\tval's l1: 0.367265\n",
      "[2000]\ttrain's l1: 0.270057\tval's l1: 0.351983\n",
      "[2500]\ttrain's l1: 0.248324\tval's l1: 0.341022\n",
      "[3000]\ttrain's l1: 0.230494\tval's l1: 0.332789\n",
      "[3500]\ttrain's l1: 0.215269\tval's l1: 0.325968\n",
      "[4000]\ttrain's l1: 0.202028\tval's l1: 0.320396\n",
      "[4500]\ttrain's l1: 0.190269\tval's l1: 0.315793\n",
      "[5000]\ttrain's l1: 0.179906\tval's l1: 0.312\n",
      "[5500]\ttrain's l1: 0.170629\tval's l1: 0.308707\n",
      "[6000]\ttrain's l1: 0.162105\tval's l1: 0.30588\n",
      "[6500]\ttrain's l1: 0.154212\tval's l1: 0.303217\n",
      "[7000]\ttrain's l1: 0.147066\tval's l1: 0.301037\n",
      "[7500]\ttrain's l1: 0.140468\tval's l1: 0.299094\n",
      "[8000]\ttrain's l1: 0.134333\tval's l1: 0.297349\n",
      "[8500]\ttrain's l1: 0.128593\tval's l1: 0.295753\n",
      "[9000]\ttrain's l1: 0.123295\tval's l1: 0.29444\n",
      "[9500]\ttrain's l1: 0.118323\tval's l1: 0.29317\n",
      "[10000]\ttrain's l1: 0.113621\tval's l1: 0.291982\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[10000]\ttrain's l1: 0.113621\tval's l1: 0.291982\n",
      "[2019-07-26 13:18:15.276361] -      Training finished, predicting something\n",
      "[2019-07-26 13:19:52.319740] -      memory footprint:16.2547 GB\n",
      "[2019-07-26 13:19:52.319886] -      reducing amount of garbadge\n",
      "[2019-07-26 13:19:52.474396] -      garbadge eliminated\n",
      "[2019-07-26 13:19:52.475015] -      memory footprint:12.5543 GB\n",
      "[2019-07-26 13:19:52.475072] -      Type: 3JHH\n",
      "[2019-07-26 13:19:53.079393] -      training and applying feature classifiers\n",
      "[2019-07-26 13:19:53.079945] -      memory footprint:12.5543 GB\n",
      "[2019-07-26 13:27:34.083891] -      training feature importance estimator\n",
      "[2019-07-26 13:27:34.084158] -      memory footprint:13.9673 GB\n",
      "[2019-07-26 13:28:36.029852] -     Top20 features: ['cos_c0_c1', 'dist_no_bond_min_y', 'dist_no_bond_min_x', 'cos_c1', 'rf04', 'cos_c0', 'adC3', 'adC2', 'cos_f0', 'adC4', 'cos_c1_f1', 'distance_farthest_1', 'cos_f1', 'rf_03', 'cos_c0_f0', 'adN1', 'dist_mean_bond_y', 'adH1', 'gap', 'rf_02']\n",
      "[2019-07-26 13:28:36.030154] -     Training final estiamtor for 3JHH\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/lightgbm/engine.py:118: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 200 rounds.\n",
      "[500]\ttrain's l1: 0.207074\tval's l1: 0.236465\n",
      "[1000]\ttrain's l1: 0.16546\tval's l1: 0.209818\n",
      "[1500]\ttrain's l1: 0.141095\tval's l1: 0.196727\n",
      "[2000]\ttrain's l1: 0.123578\tval's l1: 0.188394\n",
      "[2500]\ttrain's l1: 0.110296\tval's l1: 0.183048\n",
      "[3000]\ttrain's l1: 0.0995027\tval's l1: 0.179054\n",
      "[3500]\ttrain's l1: 0.0905046\tval's l1: 0.175999\n",
      "[4000]\ttrain's l1: 0.0829235\tval's l1: 0.17373\n",
      "[4500]\ttrain's l1: 0.0762342\tval's l1: 0.171821\n",
      "[5000]\ttrain's l1: 0.0704039\tval's l1: 0.170203\n",
      "[5500]\ttrain's l1: 0.0652129\tval's l1: 0.168914\n",
      "[6000]\ttrain's l1: 0.060601\tval's l1: 0.167868\n",
      "[6500]\ttrain's l1: 0.0564165\tval's l1: 0.167013\n",
      "[7000]\ttrain's l1: 0.0527013\tval's l1: 0.166322\n",
      "[7500]\ttrain's l1: 0.0492941\tval's l1: 0.165644\n",
      "[8000]\ttrain's l1: 0.046151\tval's l1: 0.165044\n",
      "[8500]\ttrain's l1: 0.0433124\tval's l1: 0.164552\n",
      "[9000]\ttrain's l1: 0.0406983\tval's l1: 0.164098\n",
      "[9500]\ttrain's l1: 0.0382713\tval's l1: 0.163714\n",
      "[10000]\ttrain's l1: 0.0360169\tval's l1: 0.163394\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[10000]\ttrain's l1: 0.0360169\tval's l1: 0.163394\n",
      "[2019-07-26 13:32:44.584338] -      Training finished, predicting something\n",
      "[2019-07-26 13:33:37.748252] -      memory footprint:14.0097 GB\n",
      "[2019-07-26 13:33:37.748628] -      reducing amount of garbadge\n",
      "[2019-07-26 13:33:37.841013] -      garbadge eliminated\n",
      "[2019-07-26 13:33:37.841546] -      memory footprint:12.3838 GB\n",
      "[2019-07-26 13:33:37.841600] -      Type: 3JHC\n",
      "[2019-07-26 13:33:39.022636] -      training and applying feature classifiers\n",
      "[2019-07-26 13:33:39.022913] -      memory footprint:12.3838 GB\n",
      "[2019-07-26 13:58:36.852025] -      training feature importance estimator\n",
      "[2019-07-26 13:58:36.852592] -      memory footprint:16.9342 GB\n",
      "[2019-07-26 14:01:02.793145] -     Top20 features: ['cos_c0', 'adC2', 'adC3', 'rf04', 'dist_no_bond_min_x', 'cos_f0', 'adN1', 'dist_no_bond_min_y', 'cos_c1', 'molecule_atom_index_0_dist_min_div', 'cos_c0_f0', 'adC4', 'cos_c0_c1', 'rf_10', 'gap', 'distance', 'rf_03', 'rf05', 'adC1', 'dist_bond_max_y']\n",
      "[2019-07-26 14:01:02.793315] -     Training final estiamtor for 3JHC\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/lightgbm/engine.py:118: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 200 rounds.\n",
      "[500]\ttrain's l1: 0.432272\tval's l1: 0.460499\n",
      "[1000]\ttrain's l1: 0.373496\tval's l1: 0.41672\n",
      "[1500]\ttrain's l1: 0.334987\tval's l1: 0.390457\n",
      "[2000]\ttrain's l1: 0.307342\tval's l1: 0.373255\n",
      "[2500]\ttrain's l1: 0.285125\tval's l1: 0.360384\n",
      "[3000]\ttrain's l1: 0.266926\tval's l1: 0.350555\n",
      "[3500]\ttrain's l1: 0.251176\tval's l1: 0.342559\n",
      "[4000]\ttrain's l1: 0.237394\tval's l1: 0.335867\n",
      "[4500]\ttrain's l1: 0.225364\tval's l1: 0.330329\n",
      "[5000]\ttrain's l1: 0.214492\tval's l1: 0.325672\n",
      "[5500]\ttrain's l1: 0.204771\tval's l1: 0.321772\n",
      "[6000]\ttrain's l1: 0.195865\tval's l1: 0.318306\n",
      "[6500]\ttrain's l1: 0.187692\tval's l1: 0.315228\n",
      "[7000]\ttrain's l1: 0.180152\tval's l1: 0.312446\n",
      "[7500]\ttrain's l1: 0.173146\tval's l1: 0.309975\n",
      "[8000]\ttrain's l1: 0.166673\tval's l1: 0.307807\n",
      "[8500]\ttrain's l1: 0.160493\tval's l1: 0.305714\n",
      "[9000]\ttrain's l1: 0.154756\tval's l1: 0.30401\n",
      "[9500]\ttrain's l1: 0.149377\tval's l1: 0.302408\n",
      "[10000]\ttrain's l1: 0.144266\tval's l1: 0.300964\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[10000]\ttrain's l1: 0.144266\tval's l1: 0.300964\n",
      "[2019-07-26 14:09:57.511230] -      Training finished, predicting something\n",
      "[2019-07-26 14:12:01.405524] -      memory footprint:17.6811 GB\n",
      "[2019-07-26 14:12:01.405691] -      reducing amount of garbadge\n",
      "[2019-07-26 14:12:01.603798] -      garbadge eliminated\n",
      "[2019-07-26 14:12:01.604321] -      memory footprint:12.3766 GB\n",
      "[2019-07-26 14:12:01.604377] -      Type: 3JHN\n",
      "[2019-07-26 14:12:02.094835] -      training and applying feature classifiers\n",
      "[2019-07-26 14:12:02.095384] -      memory footprint:12.3766 GB\n",
      "[2019-07-26 14:13:56.202063] -      training feature importance estimator\n",
      "[2019-07-26 14:13:56.202580] -      memory footprint:12.7765 GB\n",
      "[2019-07-26 14:14:19.779226] -     Top20 features: ['cos_c0', 'rf04', 'adC2', 'cos_c1', 'cos_c0_c1', 'cos_c0_f0', 'dist_no_bond_min_x', 'adC3', 'dist_no_bond_min_y', 'molecule_atom_index_0_dist_min_div', 'rf_10', 'adC4', 'dist_bond_max_y', 'cos_f0', 'mulliken_atom_1', 'adN1', 'dist_median_bond_y', 'distance', 'rf05', 'rf_00']\n",
      "[2019-07-26 14:14:19.779396] -     Training final estiamtor for 3JHN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/lightgbm/engine.py:118: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 200 rounds.\n",
      "[500]\ttrain's l1: 0.103687\tval's l1: 0.152857\n",
      "[1000]\ttrain's l1: 0.071365\tval's l1: 0.141645\n",
      "[1500]\ttrain's l1: 0.0531318\tval's l1: 0.136581\n",
      "[2000]\ttrain's l1: 0.0410229\tval's l1: 0.133948\n",
      "[2500]\ttrain's l1: 0.0323357\tval's l1: 0.132375\n",
      "[3000]\ttrain's l1: 0.0259545\tval's l1: 0.131379\n",
      "[3500]\ttrain's l1: 0.0210892\tval's l1: 0.13073\n",
      "[4000]\ttrain's l1: 0.0172974\tval's l1: 0.130222\n",
      "[4500]\ttrain's l1: 0.0143179\tval's l1: 0.129864\n",
      "[5000]\ttrain's l1: 0.0119464\tval's l1: 0.129641\n",
      "[5500]\ttrain's l1: 0.0100348\tval's l1: 0.129472\n",
      "[6000]\ttrain's l1: 0.00848091\tval's l1: 0.129328\n",
      "[6500]\ttrain's l1: 0.00723522\tval's l1: 0.129225\n",
      "[7000]\ttrain's l1: 0.00621249\tval's l1: 0.129152\n",
      "[7500]\ttrain's l1: 0.00536983\tval's l1: 0.129101\n",
      "[8000]\ttrain's l1: 0.00468048\tval's l1: 0.129053\n",
      "[8500]\ttrain's l1: 0.00410459\tval's l1: 0.129011\n",
      "[9000]\ttrain's l1: 0.00362528\tval's l1: 0.128971\n",
      "[9500]\ttrain's l1: 0.00322172\tval's l1: 0.128946\n",
      "[10000]\ttrain's l1: 0.0028878\tval's l1: 0.128928\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[10000]\ttrain's l1: 0.0028878\tval's l1: 0.128928\n",
      "[2019-07-26 14:16:09.711757] -      Training finished, predicting something\n",
      "[2019-07-26 14:16:26.432079] -      memory footprint:12.7765 GB\n",
      "[2019-07-26 14:16:26.432232] -      reducing amount of garbadge\n",
      "[2019-07-26 14:16:26.488315] -      garbadge eliminated\n",
      "[2019-07-26 14:16:26.488929] -      memory footprint:12.3766 GB\n",
      "[2019-07-26 14:16:27.301739] - Fold result: -1.4302 group-mean log(mae)\n",
      "[2019-07-26 14:16:27.336159] - --------------------------------------------------\n",
      "[2019-07-26 14:16:27.336333] - Training for split 3\n",
      "[2019-07-26 14:16:30.276993] -      Type: 1JHC\n",
      "[2019-07-26 14:16:31.023708] -      training and applying feature classifiers\n",
      "[2019-07-26 14:16:31.024288] -      memory footprint:12.0080 GB\n",
      "[2019-07-26 14:25:40.071335] -      training feature importance estimator\n",
      "[2019-07-26 14:25:40.071585] -      memory footprint:13.9572 GB\n",
      "[2019-07-26 14:26:59.759553] -     Top20 features: ['adC1', 'adC2', 'dist_no_bond_min_y', 'adC3', 'mulliken_atom_0', 'mulliken_atom_1', 'dist_no_bond_min_x', 'dist_median_bond_y', 'rf04', 'adN1', 'inv_dist0R', 'cos_c0_f0', 'adC4', 'dist_std_bond_y', 'dist_bond_max_y', 'dist_mean_bond_y', 'inv_dist1R', 'yukawa_O.y', 'rf_10', 'rf_00']\n",
      "[2019-07-26 14:26:59.759831] -     Training final estiamtor for 1JHC\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/lightgbm/engine.py:118: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 200 rounds.\n",
      "[500]\ttrain's l1: 0.840408\tval's l1: 0.946368\n",
      "[1000]\ttrain's l1: 0.683333\tval's l1: 0.841977\n",
      "[1500]\ttrain's l1: 0.588545\tval's l1: 0.788315\n",
      "[2000]\ttrain's l1: 0.520881\tval's l1: 0.754922\n",
      "[2500]\ttrain's l1: 0.468341\tval's l1: 0.731956\n",
      "[3000]\ttrain's l1: 0.425884\tval's l1: 0.715347\n",
      "[3500]\ttrain's l1: 0.389794\tval's l1: 0.702233\n",
      "[4000]\ttrain's l1: 0.358824\tval's l1: 0.691973\n",
      "[4500]\ttrain's l1: 0.331859\tval's l1: 0.683758\n",
      "[5000]\ttrain's l1: 0.308415\tval's l1: 0.677034\n",
      "[5500]\ttrain's l1: 0.286987\tval's l1: 0.671015\n",
      "[6000]\ttrain's l1: 0.267802\tval's l1: 0.666349\n",
      "[6500]\ttrain's l1: 0.250398\tval's l1: 0.662123\n",
      "[7000]\ttrain's l1: 0.234566\tval's l1: 0.658397\n",
      "[7500]\ttrain's l1: 0.220319\tval's l1: 0.655307\n",
      "[8000]\ttrain's l1: 0.207271\tval's l1: 0.652717\n",
      "[8500]\ttrain's l1: 0.195097\tval's l1: 0.650249\n",
      "[9000]\ttrain's l1: 0.183972\tval's l1: 0.648072\n",
      "[9500]\ttrain's l1: 0.173547\tval's l1: 0.646183\n",
      "[10000]\ttrain's l1: 0.163951\tval's l1: 0.644549\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[10000]\ttrain's l1: 0.163951\tval's l1: 0.644549\n",
      "[2019-07-26 14:32:05.092677] -      Training finished, predicting something\n",
      "[2019-07-26 14:33:09.568405] -      memory footprint:14.1852 GB\n",
      "[2019-07-26 14:33:09.568545] -      reducing amount of garbadge\n",
      "[2019-07-26 14:33:09.681223] -      garbadge eliminated\n",
      "[2019-07-26 14:33:09.681821] -      memory footprint:11.9517 GB\n",
      "[2019-07-26 14:33:09.681875] -      Type: 2JHH\n",
      "[2019-07-26 14:33:10.277001] -      training and applying feature classifiers\n",
      "[2019-07-26 14:33:10.277560] -      memory footprint:11.9515 GB\n",
      "[2019-07-26 14:37:50.854045] -      training feature importance estimator\n",
      "[2019-07-26 14:37:50.854295] -      memory footprint:12.9907 GB\n",
      "[2019-07-26 14:38:36.179884] -     Top20 features: ['adC3', 'adC2', 'adN1', 'cos_c1_f1', 'cos_c0_c1', 'cos_f0_f1', 'rf_00', 'mulliken_atom_1', 'distance_farthest_1', 'mulliken_atom_0', 'cos_c0_f0', 'adC4', 'cos_f0', 'cos_f1', 'inv_distPE', 'rf_02', 'gap', 'dist_mean_bond_y', 'rf_03', 'adH1']\n",
      "[2019-07-26 14:38:36.180164] -     Training final estiamtor for 2JHH\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/lightgbm/engine.py:118: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 200 rounds.\n",
      "[500]\ttrain's l1: 0.157037\tval's l1: 0.193778\n",
      "[1000]\ttrain's l1: 0.120446\tval's l1: 0.17331\n",
      "[1500]\ttrain's l1: 0.0998609\tval's l1: 0.164195\n",
      "[2000]\ttrain's l1: 0.0855014\tval's l1: 0.159224\n",
      "[2500]\ttrain's l1: 0.0746076\tval's l1: 0.155861\n",
      "[3000]\ttrain's l1: 0.0658133\tval's l1: 0.153564\n",
      "[3500]\ttrain's l1: 0.0585756\tval's l1: 0.151929\n",
      "[4000]\ttrain's l1: 0.0524677\tval's l1: 0.150694\n",
      "[4500]\ttrain's l1: 0.0472282\tval's l1: 0.149713\n",
      "[5000]\ttrain's l1: 0.042705\tval's l1: 0.148986\n",
      "[5500]\ttrain's l1: 0.0387123\tval's l1: 0.148367\n",
      "[6000]\ttrain's l1: 0.0352028\tval's l1: 0.14785\n",
      "[6500]\ttrain's l1: 0.0320768\tval's l1: 0.147436\n",
      "[7000]\ttrain's l1: 0.0292946\tval's l1: 0.147095\n",
      "[7500]\ttrain's l1: 0.0268172\tval's l1: 0.146732\n",
      "[8000]\ttrain's l1: 0.024596\tval's l1: 0.146481\n",
      "[8500]\ttrain's l1: 0.0225938\tval's l1: 0.146292\n",
      "[9000]\ttrain's l1: 0.0207946\tval's l1: 0.146131\n",
      "[9500]\ttrain's l1: 0.0191726\tval's l1: 0.146\n",
      "[10000]\ttrain's l1: 0.0176947\tval's l1: 0.145883\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[10000]\ttrain's l1: 0.0176947\tval's l1: 0.145883\n",
      "[2019-07-26 14:41:45.087409] -      Training finished, predicting something\n",
      "[2019-07-26 14:42:22.420683] -      memory footprint:13.1275 GB\n",
      "[2019-07-26 14:42:22.420821] -      reducing amount of garbadge\n",
      "[2019-07-26 14:42:22.504238] -      garbadge eliminated\n",
      "[2019-07-26 14:42:22.504609] -      memory footprint:11.9520 GB\n",
      "[2019-07-26 14:42:22.504656] -      Type: 1JHN\n",
      "[2019-07-26 14:42:22.892634] -      training and applying feature classifiers\n",
      "[2019-07-26 14:42:22.893082] -      memory footprint:11.9516 GB\n",
      "[2019-07-26 14:42:46.130146] -      training feature importance estimator\n",
      "[2019-07-26 14:42:46.130656] -      memory footprint:11.9516 GB\n",
      "[2019-07-26 14:42:57.772881] -     Top20 features: ['mulliken_atom_0', 'dist_mean_bond_y', 'inv_dist1R', 'dist_no_bond_min_x', 'inv_dist0R', 'mulliken_atom_1', 'adC2', 'rf_02', 'adC3', 'dist_no_bond_min_y', 'dist_std_bond_y', 'cos_c0_f0', 'dist_median_bond_y', 'rf_00', 'rf04', 'adC4', 'linkM1', 'adN1', 'rf_10', 'linkM0']\n",
      "[2019-07-26 14:42:57.773112] -     Training final estiamtor for 1JHN\n",
      "Training until validation scores don't improve for 200 rounds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/lightgbm/engine.py:118: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[500]\ttrain's l1: 0.166265\tval's l1: 0.380768\n",
      "[1000]\ttrain's l1: 0.0781175\tval's l1: 0.364309\n",
      "[1500]\ttrain's l1: 0.037908\tval's l1: 0.359861\n",
      "[2000]\ttrain's l1: 0.0194752\tval's l1: 0.358421\n",
      "[2500]\ttrain's l1: 0.0105396\tval's l1: 0.357949\n",
      "[3000]\ttrain's l1: 0.00609398\tval's l1: 0.357754\n",
      "[3500]\ttrain's l1: 0.00380225\tval's l1: 0.357656\n",
      "[4000]\ttrain's l1: 0.00257052\tval's l1: 0.357576\n",
      "[4500]\ttrain's l1: 0.00187102\tval's l1: 0.357559\n",
      "[5000]\ttrain's l1: 0.00145163\tval's l1: 0.357546\n",
      "[5500]\ttrain's l1: 0.00121123\tval's l1: 0.357533\n",
      "Early stopping, best iteration is:\n",
      "[5561]\ttrain's l1: 0.00119037\tval's l1: 0.357533\n",
      "[2019-07-26 14:43:42.198719] -      Training finished, predicting something\n",
      "[2019-07-26 14:43:45.375566] -      memory footprint:11.9516 GB\n",
      "[2019-07-26 14:43:45.375933] -      reducing amount of garbadge\n",
      "[2019-07-26 14:43:45.417115] -      garbadge eliminated\n",
      "[2019-07-26 14:43:45.417576] -      memory footprint:11.9516 GB\n",
      "[2019-07-26 14:43:45.417696] -      Type: 2JHN\n",
      "[2019-07-26 14:43:45.891183] -      training and applying feature classifiers\n",
      "[2019-07-26 14:43:45.891458] -      memory footprint:11.9516 GB\n",
      "[2019-07-26 14:44:59.029283] -      training feature importance estimator\n",
      "[2019-07-26 14:44:59.029526] -      memory footprint:12.2375 GB\n",
      "[2019-07-26 14:45:17.079713] -     Top20 features: ['adC2', 'cos_c0', 'adC3', 'cos_f0', 'cos_c0_c1', 'adC4', 'cos_c0_f0', 'rf04', 'rf_10', 'dist_no_bond_min_x', 'rf_03', 'mulliken_atom_1', 'rf_02', 'cos_c1', 'mulliken_atom_0', 'dist_median_bond_y', 'dist_min_y', 'rf05', 'molecule_atom_index_0_dist_min_div', 'inv_dist0R']\n",
      "[2019-07-26 14:45:17.079860] -     Training final estiamtor for 2JHN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/lightgbm/engine.py:118: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 200 rounds.\n",
      "[500]\ttrain's l1: 0.123618\tval's l1: 0.203552\n",
      "[1000]\ttrain's l1: 0.0802528\tval's l1: 0.188898\n",
      "[1500]\ttrain's l1: 0.0569389\tval's l1: 0.183396\n",
      "[2000]\ttrain's l1: 0.0418735\tval's l1: 0.18073\n",
      "[2500]\ttrain's l1: 0.0317256\tval's l1: 0.179097\n",
      "[3000]\ttrain's l1: 0.0243717\tval's l1: 0.178134\n",
      "[3500]\ttrain's l1: 0.0190313\tval's l1: 0.177591\n",
      "[4000]\ttrain's l1: 0.0150165\tval's l1: 0.177195\n",
      "[4500]\ttrain's l1: 0.0119894\tval's l1: 0.176937\n",
      "[5000]\ttrain's l1: 0.00967559\tval's l1: 0.176771\n",
      "[5500]\ttrain's l1: 0.00789309\tval's l1: 0.176659\n",
      "[6000]\ttrain's l1: 0.00650384\tval's l1: 0.176553\n",
      "[6500]\ttrain's l1: 0.00542775\tval's l1: 0.176473\n",
      "[7000]\ttrain's l1: 0.00458192\tval's l1: 0.176417\n",
      "[7500]\ttrain's l1: 0.00391458\tval's l1: 0.176377\n",
      "[8000]\ttrain's l1: 0.00337908\tval's l1: 0.176336\n",
      "[8500]\ttrain's l1: 0.00294809\tval's l1: 0.17631\n",
      "[9000]\ttrain's l1: 0.00259928\tval's l1: 0.176291\n",
      "[9500]\ttrain's l1: 0.00231323\tval's l1: 0.17628\n",
      "[10000]\ttrain's l1: 0.00207533\tval's l1: 0.176267\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[10000]\ttrain's l1: 0.00207533\tval's l1: 0.176267\n",
      "[2019-07-26 14:46:54.202579] -      Training finished, predicting something\n",
      "[2019-07-26 14:47:07.190342] -      memory footprint:12.2375 GB\n",
      "[2019-07-26 14:47:07.190546] -      reducing amount of garbadge\n",
      "[2019-07-26 14:47:07.246274] -      garbadge eliminated\n",
      "[2019-07-26 14:47:07.246726] -      memory footprint:11.9516 GB\n",
      "[2019-07-26 14:47:07.246780] -      Type: 2JHC\n",
      "[2019-07-26 14:47:08.216777] -      training and applying feature classifiers\n",
      "[2019-07-26 14:47:08.217325] -      memory footprint:11.9516 GB\n",
      "[2019-07-26 15:03:55.689612] -      training feature importance estimator\n",
      "[2019-07-26 15:03:55.690164] -      memory footprint:15.3336 GB\n",
      "[2019-07-26 15:05:43.021672] -     Top20 features: ['adC2', 'rf04', 'dist_no_bond_min_x', 'cos_f0', 'cos_c0', 'adC3', 'adN1', 'cos_c0_f0', 'dist_no_bond_min_y', 'inv_dist0R', 'mulliken_atom_1', 'dist_bond_max_y', 'molecule_atom_index_0_dist_min_div', 'rf_00', 'dist_mean_bond_y', 'vander_O.y', 'distance', 'gap', 'dist_median_bond_y', 'dist_min_y']\n",
      "[2019-07-26 15:05:43.021830] -     Training final estiamtor for 2JHC\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/lightgbm/engine.py:118: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 200 rounds.\n",
      "[500]\ttrain's l1: 0.401394\tval's l1: 0.438191\n",
      "[1000]\ttrain's l1: 0.337618\tval's l1: 0.393229\n",
      "[1500]\ttrain's l1: 0.298967\tval's l1: 0.369107\n",
      "[2000]\ttrain's l1: 0.270665\tval's l1: 0.353239\n",
      "[2500]\ttrain's l1: 0.248922\tval's l1: 0.342127\n",
      "[3000]\ttrain's l1: 0.230942\tval's l1: 0.333548\n",
      "[3500]\ttrain's l1: 0.215657\tval's l1: 0.326673\n",
      "[4000]\ttrain's l1: 0.202415\tval's l1: 0.321021\n",
      "[4500]\ttrain's l1: 0.190656\tval's l1: 0.31627\n",
      "[5000]\ttrain's l1: 0.180256\tval's l1: 0.312568\n",
      "[5500]\ttrain's l1: 0.170766\tval's l1: 0.309106\n",
      "[6000]\ttrain's l1: 0.162182\tval's l1: 0.306178\n",
      "[6500]\ttrain's l1: 0.154387\tval's l1: 0.303721\n",
      "[7000]\ttrain's l1: 0.147175\tval's l1: 0.301374\n",
      "[7500]\ttrain's l1: 0.140596\tval's l1: 0.299541\n",
      "[8000]\ttrain's l1: 0.134464\tval's l1: 0.29777\n",
      "[8500]\ttrain's l1: 0.128764\tval's l1: 0.296234\n",
      "[9000]\ttrain's l1: 0.12337\tval's l1: 0.29482\n",
      "[9500]\ttrain's l1: 0.118322\tval's l1: 0.293655\n",
      "[10000]\ttrain's l1: 0.113684\tval's l1: 0.292523\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[10000]\ttrain's l1: 0.113684\tval's l1: 0.292523\n",
      "[2019-07-26 15:12:46.624460] -      Training finished, predicting something\n",
      "[2019-07-26 15:14:23.497799] -      memory footprint:15.8700 GB\n",
      "[2019-07-26 15:14:23.497940] -      reducing amount of garbadge\n",
      "[2019-07-26 15:14:23.655767] -      garbadge eliminated\n",
      "[2019-07-26 15:14:23.656388] -      memory footprint:12.0463 GB\n",
      "[2019-07-26 15:14:23.656542] -      Type: 3JHH\n",
      "[2019-07-26 15:14:24.364841] -      training and applying feature classifiers\n",
      "[2019-07-26 15:14:24.365120] -      memory footprint:12.0463 GB\n",
      "[2019-07-26 15:22:03.785036] -      training feature importance estimator\n",
      "[2019-07-26 15:22:03.785555] -      memory footprint:13.6700 GB\n",
      "[2019-07-26 15:23:04.467274] -     Top20 features: ['cos_c0_c1', 'dist_no_bond_min_y', 'dist_no_bond_min_x', 'cos_c1', 'rf04', 'cos_c0', 'adC3', 'distance_farthest_1', 'adC2', 'adC4', 'cos_f0', 'cos_c1_f1', 'cos_f1', 'rf_03', 'dist_mean_bond_y', 'adN1', 'cos_c0_f0', 'cos_f0_f1', 'gap', 'rf_02']\n",
      "[2019-07-26 15:23:04.467653] -     Training final estiamtor for 3JHH\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/lightgbm/engine.py:118: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 200 rounds.\n",
      "[500]\ttrain's l1: 0.205653\tval's l1: 0.234643\n",
      "[1000]\ttrain's l1: 0.16461\tval's l1: 0.208983\n",
      "[1500]\ttrain's l1: 0.14019\tval's l1: 0.195583\n",
      "[2000]\ttrain's l1: 0.122808\tval's l1: 0.187476\n",
      "[2500]\ttrain's l1: 0.10946\tval's l1: 0.181868\n",
      "[3000]\ttrain's l1: 0.0988759\tval's l1: 0.177885\n",
      "[3500]\ttrain's l1: 0.0899875\tval's l1: 0.174875\n",
      "[4000]\ttrain's l1: 0.0823406\tval's l1: 0.172507\n",
      "[4500]\ttrain's l1: 0.0757822\tval's l1: 0.170766\n",
      "[5000]\ttrain's l1: 0.06997\tval's l1: 0.169283\n",
      "[5500]\ttrain's l1: 0.0647644\tval's l1: 0.16802\n",
      "[6000]\ttrain's l1: 0.0602256\tval's l1: 0.16697\n",
      "[6500]\ttrain's l1: 0.056068\tval's l1: 0.166016\n",
      "[7000]\ttrain's l1: 0.052355\tval's l1: 0.165351\n",
      "[7500]\ttrain's l1: 0.0489836\tval's l1: 0.164688\n",
      "[8000]\ttrain's l1: 0.0458999\tval's l1: 0.164106\n",
      "[8500]\ttrain's l1: 0.0430128\tval's l1: 0.163625\n",
      "[9000]\ttrain's l1: 0.0403938\tval's l1: 0.163202\n",
      "[9500]\ttrain's l1: 0.0379568\tval's l1: 0.162788\n",
      "[10000]\ttrain's l1: 0.0357407\tval's l1: 0.162451\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[10000]\ttrain's l1: 0.0357407\tval's l1: 0.162451\n",
      "[2019-07-26 15:27:11.863655] -      Training finished, predicting something\n",
      "[2019-07-26 15:28:06.059612] -      memory footprint:13.8832 GB\n",
      "[2019-07-26 15:28:06.059798] -      reducing amount of garbadge\n",
      "[2019-07-26 15:28:06.167691] -      garbadge eliminated\n",
      "[2019-07-26 15:28:06.168097] -      memory footprint:11.9699 GB\n",
      "[2019-07-26 15:28:06.168144] -      Type: 3JHC\n",
      "[2019-07-26 15:28:07.405827] -      training and applying feature classifiers\n",
      "[2019-07-26 15:28:07.406385] -      memory footprint:11.9699 GB\n",
      "[2019-07-26 15:53:11.272208] -      training feature importance estimator\n",
      "[2019-07-26 15:53:11.272465] -      memory footprint:16.9340 GB\n",
      "[2019-07-26 15:55:39.033178] -     Top20 features: ['cos_c0', 'adC2', 'adC3', 'rf04', 'dist_no_bond_min_x', 'cos_f0', 'adN1', 'cos_c1', 'dist_no_bond_min_y', 'molecule_atom_index_0_dist_min_div', 'cos_c0_c1', 'adC4', 'cos_c0_f0', 'gap', 'rf_10', 'rf_03', 'adC1', 'rf05', 'distance', 'dist_bond_max_y']\n",
      "[2019-07-26 15:55:39.033342] -     Training final estiamtor for 3JHC\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/lightgbm/engine.py:118: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 200 rounds.\n",
      "[500]\ttrain's l1: 0.431608\tval's l1: 0.458706\n",
      "[1000]\ttrain's l1: 0.373424\tval's l1: 0.415287\n",
      "[1500]\ttrain's l1: 0.33604\tval's l1: 0.390246\n",
      "[2000]\ttrain's l1: 0.307673\tval's l1: 0.372455\n",
      "[2500]\ttrain's l1: 0.285657\tval's l1: 0.359727\n",
      "[3000]\ttrain's l1: 0.267201\tval's l1: 0.349701\n",
      "[3500]\ttrain's l1: 0.251494\tval's l1: 0.341955\n",
      "[4000]\ttrain's l1: 0.237587\tval's l1: 0.335263\n",
      "[4500]\ttrain's l1: 0.225286\tval's l1: 0.329531\n",
      "[5000]\ttrain's l1: 0.214473\tval's l1: 0.324897\n",
      "[5500]\ttrain's l1: 0.204689\tval's l1: 0.320859\n",
      "[6000]\ttrain's l1: 0.19567\tval's l1: 0.317255\n",
      "[6500]\ttrain's l1: 0.18747\tval's l1: 0.314106\n",
      "[7000]\ttrain's l1: 0.179864\tval's l1: 0.311361\n",
      "[7500]\ttrain's l1: 0.172838\tval's l1: 0.308889\n",
      "[8000]\ttrain's l1: 0.166361\tval's l1: 0.306703\n",
      "[8500]\ttrain's l1: 0.160181\tval's l1: 0.304705\n",
      "[9000]\ttrain's l1: 0.154466\tval's l1: 0.303058\n",
      "[9500]\ttrain's l1: 0.148997\tval's l1: 0.30145\n",
      "[10000]\ttrain's l1: 0.143953\tval's l1: 0.300014\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[10000]\ttrain's l1: 0.143953\tval's l1: 0.300014\n",
      "[2019-07-26 16:04:27.189273] -      Training finished, predicting something\n",
      "[2019-07-26 16:06:31.233127] -      memory footprint:17.6328 GB\n",
      "[2019-07-26 16:06:31.233263] -      reducing amount of garbadge\n",
      "[2019-07-26 16:06:31.430800] -      garbadge eliminated\n",
      "[2019-07-26 16:06:31.431393] -      memory footprint:12.4722 GB\n",
      "[2019-07-26 16:06:31.431451] -      Type: 3JHN\n",
      "[2019-07-26 16:06:31.929495] -      training and applying feature classifiers\n",
      "[2019-07-26 16:06:31.930071] -      memory footprint:12.4722 GB\n",
      "[2019-07-26 16:08:26.149164] -      training feature importance estimator\n",
      "[2019-07-26 16:08:26.149415] -      memory footprint:12.8723 GB\n",
      "[2019-07-26 16:08:49.220596] -     Top20 features: ['cos_c0', 'adC2', 'rf04', 'cos_c1', 'cos_c0_c1', 'dist_no_bond_min_y', 'dist_no_bond_min_x', 'adC3', 'cos_c0_f0', 'molecule_atom_index_0_dist_min_div', 'rf_10', 'adC4', 'cos_f0', 'mulliken_atom_1', 'dist_bond_max_y', 'adN1', 'rf05', 'distance', 'dist_min_y', 'dist_median_bond_y']\n",
      "[2019-07-26 16:08:49.220769] -     Training final estiamtor for 3JHN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/lightgbm/engine.py:118: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 200 rounds.\n",
      "[500]\ttrain's l1: 0.104699\tval's l1: 0.153571\n",
      "[1000]\ttrain's l1: 0.0721189\tval's l1: 0.141566\n",
      "[1500]\ttrain's l1: 0.0537671\tval's l1: 0.13642\n",
      "[2000]\ttrain's l1: 0.0415282\tval's l1: 0.133741\n",
      "[2500]\ttrain's l1: 0.0328007\tval's l1: 0.132194\n",
      "[3000]\ttrain's l1: 0.0263598\tval's l1: 0.131183\n",
      "[3500]\ttrain's l1: 0.0214451\tval's l1: 0.130477\n",
      "[4000]\ttrain's l1: 0.0176377\tval's l1: 0.130006\n",
      "[4500]\ttrain's l1: 0.0146068\tval's l1: 0.129641\n",
      "[5000]\ttrain's l1: 0.0121934\tval's l1: 0.129391\n",
      "[5500]\ttrain's l1: 0.0102485\tval's l1: 0.129238\n",
      "[6000]\ttrain's l1: 0.00869923\tval's l1: 0.129091\n",
      "[6500]\ttrain's l1: 0.00742218\tval's l1: 0.129003\n",
      "[7000]\ttrain's l1: 0.00638249\tval's l1: 0.128913\n",
      "[7500]\ttrain's l1: 0.00552747\tval's l1: 0.128844\n",
      "[8000]\ttrain's l1: 0.0048133\tval's l1: 0.128785\n",
      "[8500]\ttrain's l1: 0.00422997\tval's l1: 0.128742\n",
      "[9000]\ttrain's l1: 0.00373929\tval's l1: 0.128713\n",
      "[9500]\ttrain's l1: 0.00332493\tval's l1: 0.128684\n",
      "[10000]\ttrain's l1: 0.00298084\tval's l1: 0.12866\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[10000]\ttrain's l1: 0.00298084\tval's l1: 0.12866\n",
      "[2019-07-26 16:10:37.952774] -      Training finished, predicting something\n",
      "[2019-07-26 16:10:54.590678] -      memory footprint:12.8064 GB\n",
      "[2019-07-26 16:10:54.590951] -      reducing amount of garbadge\n",
      "[2019-07-26 16:10:54.649194] -      garbadge eliminated\n",
      "[2019-07-26 16:10:54.649765] -      memory footprint:12.3340 GB\n",
      "[2019-07-26 16:10:55.458785] - Fold result: -1.4287 group-mean log(mae)\n",
      "[2019-07-26 16:10:55.492926] - --------------------------------------------------\n",
      "[2019-07-26 16:10:55.493117] - Training for split 4\n",
      "[2019-07-26 16:10:58.491979] -      Type: 1JHC\n",
      "[2019-07-26 16:10:59.103597] -      training and applying feature classifiers\n",
      "[2019-07-26 16:10:59.103879] -      memory footprint:12.7449 GB\n",
      "[2019-07-26 16:20:04.799282] -      training feature importance estimator\n",
      "[2019-07-26 16:20:04.799877] -      memory footprint:14.4406 GB\n",
      "[2019-07-26 16:21:24.756512] -     Top20 features: ['adC2', 'adC1', 'adC3', 'dist_no_bond_min_y', 'mulliken_atom_0', 'mulliken_atom_1', 'dist_median_bond_y', 'rf04', 'adN1', 'inv_dist0R', 'dist_no_bond_min_x', 'cos_c0_f0', 'dist_std_bond_y', 'adC4', 'dist_bond_max_y', 'dist_mean_bond_y', 'inv_dist1R', 'yukawa_O.y', 'vander_O.y', 'range_dist_bond_y']\n",
      "[2019-07-26 16:21:24.756679] -     Training final estiamtor for 1JHC\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/lightgbm/engine.py:118: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 200 rounds.\n",
      "[500]\ttrain's l1: 0.842829\tval's l1: 0.9498\n",
      "[1000]\ttrain's l1: 0.687674\tval's l1: 0.847821\n",
      "[1500]\ttrain's l1: 0.592729\tval's l1: 0.79372\n",
      "[2000]\ttrain's l1: 0.525496\tval's l1: 0.760676\n",
      "[2500]\ttrain's l1: 0.472195\tval's l1: 0.736717\n",
      "[3000]\ttrain's l1: 0.428975\tval's l1: 0.719496\n",
      "[3500]\ttrain's l1: 0.392552\tval's l1: 0.705852\n",
      "[4000]\ttrain's l1: 0.361666\tval's l1: 0.695233\n",
      "[4500]\ttrain's l1: 0.334754\tval's l1: 0.687187\n",
      "[5000]\ttrain's l1: 0.310843\tval's l1: 0.680217\n",
      "[5500]\ttrain's l1: 0.289399\tval's l1: 0.674315\n",
      "[6000]\ttrain's l1: 0.27041\tval's l1: 0.669372\n",
      "[6500]\ttrain's l1: 0.253168\tval's l1: 0.665282\n",
      "[7000]\ttrain's l1: 0.23743\tval's l1: 0.661664\n",
      "[7500]\ttrain's l1: 0.223025\tval's l1: 0.658368\n",
      "[8000]\ttrain's l1: 0.209813\tval's l1: 0.655691\n",
      "[8500]\ttrain's l1: 0.197659\tval's l1: 0.653173\n",
      "[9000]\ttrain's l1: 0.186425\tval's l1: 0.651101\n",
      "[9500]\ttrain's l1: 0.176105\tval's l1: 0.649114\n",
      "[10000]\ttrain's l1: 0.166459\tval's l1: 0.647394\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[10000]\ttrain's l1: 0.166459\tval's l1: 0.647394\n",
      "[2019-07-26 16:26:30.918367] -      Training finished, predicting something\n",
      "[2019-07-26 16:27:34.446443] -      memory footprint:14.7046 GB\n",
      "[2019-07-26 16:27:34.446585] -      reducing amount of garbadge\n",
      "[2019-07-26 16:27:34.549413] -      garbadge eliminated\n",
      "[2019-07-26 16:27:34.549919] -      memory footprint:12.7243 GB\n",
      "[2019-07-26 16:27:34.549974] -      Type: 2JHH\n",
      "[2019-07-26 16:27:35.078079] -      training and applying feature classifiers\n",
      "[2019-07-26 16:27:35.078358] -      memory footprint:12.7241 GB\n",
      "[2019-07-26 16:32:08.628577] -      training feature importance estimator\n",
      "[2019-07-26 16:32:08.629166] -      memory footprint:13.6278 GB\n",
      "[2019-07-26 16:32:54.286483] -     Top20 features: ['adC3', 'adC2', 'adN1', 'cos_c1_f1', 'cos_f0_f1', 'cos_c0_c1', 'mulliken_atom_0', 'rf_00', 'mulliken_atom_1', 'distance_farthest_1', 'cos_c0_f0', 'cos_f0', 'adC4', 'cos_f1', 'inv_distPE', 'rf_02', 'gap', 'rf_03', 'adH1', 'dist_mean_bond_y']\n",
      "[2019-07-26 16:32:54.286637] -     Training final estiamtor for 2JHH\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/lightgbm/engine.py:118: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 200 rounds.\n",
      "[500]\ttrain's l1: 0.156593\tval's l1: 0.193914\n",
      "[1000]\ttrain's l1: 0.12014\tval's l1: 0.173612\n",
      "[1500]\ttrain's l1: 0.0993891\tval's l1: 0.16415\n",
      "[2000]\ttrain's l1: 0.0851481\tval's l1: 0.159019\n",
      "[2500]\ttrain's l1: 0.0743058\tval's l1: 0.155745\n",
      "[3000]\ttrain's l1: 0.0654928\tval's l1: 0.153396\n",
      "[3500]\ttrain's l1: 0.0583054\tval's l1: 0.151769\n",
      "[4000]\ttrain's l1: 0.0521075\tval's l1: 0.150466\n",
      "[4500]\ttrain's l1: 0.0469266\tval's l1: 0.149493\n",
      "[5000]\ttrain's l1: 0.0423661\tval's l1: 0.1487\n",
      "[5500]\ttrain's l1: 0.0383898\tval's l1: 0.148033\n",
      "[6000]\ttrain's l1: 0.0348732\tval's l1: 0.147502\n",
      "[6500]\ttrain's l1: 0.031761\tval's l1: 0.147099\n",
      "[7000]\ttrain's l1: 0.0290174\tval's l1: 0.14673\n",
      "[7500]\ttrain's l1: 0.026541\tval's l1: 0.146429\n",
      "[8000]\ttrain's l1: 0.0243417\tval's l1: 0.146225\n",
      "[8500]\ttrain's l1: 0.0223384\tval's l1: 0.14604\n",
      "[9000]\ttrain's l1: 0.0205573\tval's l1: 0.145872\n",
      "[9500]\ttrain's l1: 0.0189292\tval's l1: 0.145718\n",
      "[10000]\ttrain's l1: 0.0174543\tval's l1: 0.145583\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[10000]\ttrain's l1: 0.0174543\tval's l1: 0.145583\n",
      "[2019-07-26 16:36:03.366915] -      Training finished, predicting something\n",
      "[2019-07-26 16:36:40.647504] -      memory footprint:13.6294 GB\n",
      "[2019-07-26 16:36:40.647896] -      reducing amount of garbadge\n",
      "[2019-07-26 16:36:40.723034] -      garbadge eliminated\n",
      "[2019-07-26 16:36:40.723569] -      memory footprint:12.7257 GB\n",
      "[2019-07-26 16:36:40.723623] -      Type: 1JHN\n",
      "[2019-07-26 16:36:41.116933] -      training and applying feature classifiers\n",
      "[2019-07-26 16:36:41.117451] -      memory footprint:12.7254 GB\n",
      "[2019-07-26 16:37:04.288523] -      training feature importance estimator\n",
      "[2019-07-26 16:37:04.289123] -      memory footprint:12.7254 GB\n",
      "[2019-07-26 16:37:15.610424] -     Top20 features: ['mulliken_atom_0', 'dist_mean_bond_y', 'inv_dist1R', 'mulliken_atom_1', 'dist_no_bond_min_x', 'adC2', 'inv_dist0R', 'rf_02', 'dist_no_bond_min_y', 'adC3', 'cos_c0_f0', 'rf_00', 'dist_std_bond_y', 'linkM0', 'dist_median_bond_y', 'adN1', 'inv_distPR', 'adC4', 'homo', 'rf04']\n",
      "[2019-07-26 16:37:15.610590] -     Training final estiamtor for 1JHN\n",
      "Training until validation scores don't improve for 200 rounds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/lightgbm/engine.py:118: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[500]\ttrain's l1: 0.156006\tval's l1: 0.381676\n",
      "[1000]\ttrain's l1: 0.0706732\tval's l1: 0.36644\n",
      "[1500]\ttrain's l1: 0.0351125\tval's l1: 0.363218\n",
      "[2000]\ttrain's l1: 0.0183405\tval's l1: 0.362253\n",
      "[2500]\ttrain's l1: 0.0100437\tval's l1: 0.361877\n",
      "[3000]\ttrain's l1: 0.00588077\tval's l1: 0.361669\n",
      "[3500]\ttrain's l1: 0.00373947\tval's l1: 0.361561\n",
      "[4000]\ttrain's l1: 0.00256607\tval's l1: 0.361509\n",
      "[4500]\ttrain's l1: 0.00189445\tval's l1: 0.361476\n",
      "Early stopping, best iteration is:\n",
      "[4492]\ttrain's l1: 0.00190259\tval's l1: 0.361474\n",
      "[2019-07-26 16:37:50.100728] -      Training finished, predicting something\n",
      "[2019-07-26 16:37:52.684953] -      memory footprint:12.7254 GB\n",
      "[2019-07-26 16:37:52.685117] -      reducing amount of garbadge\n",
      "[2019-07-26 16:37:52.726186] -      garbadge eliminated\n",
      "[2019-07-26 16:37:52.726647] -      memory footprint:12.7254 GB\n",
      "[2019-07-26 16:37:52.726704] -      Type: 2JHN\n",
      "[2019-07-26 16:37:53.197212] -      training and applying feature classifiers\n",
      "[2019-07-26 16:37:53.197764] -      memory footprint:12.7254 GB\n",
      "[2019-07-26 16:39:04.780729] -      training feature importance estimator\n",
      "[2019-07-26 16:39:04.781256] -      memory footprint:12.7254 GB\n",
      "[2019-07-26 16:39:22.917460] -     Top20 features: ['adC2', 'cos_c0', 'adC3', 'cos_c0_c1', 'cos_f0', 'cos_c0_f0', 'adC4', 'rf_10', 'rf04', 'cos_c1', 'rf_02', 'mulliken_atom_1', 'dist_no_bond_min_x', 'rf_03', 'dist_median_bond_y', 'rf05', 'mulliken_atom_0', 'dist_min_y', 'dist_mean_bond_y', 'molecule_atom_index_0_dist_mean_diff']\n",
      "[2019-07-26 16:39:22.917606] -     Training final estiamtor for 2JHN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/lightgbm/engine.py:118: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 200 rounds.\n",
      "[500]\ttrain's l1: 0.124347\tval's l1: 0.206747\n",
      "[1000]\ttrain's l1: 0.0802683\tval's l1: 0.192741\n",
      "[1500]\ttrain's l1: 0.0564339\tval's l1: 0.187686\n",
      "[2000]\ttrain's l1: 0.0414845\tval's l1: 0.185235\n",
      "[2500]\ttrain's l1: 0.0311858\tval's l1: 0.18386\n",
      "[3000]\ttrain's l1: 0.0238572\tval's l1: 0.183071\n",
      "[3500]\ttrain's l1: 0.0185156\tval's l1: 0.182569\n",
      "[4000]\ttrain's l1: 0.0145786\tval's l1: 0.182216\n",
      "[4500]\ttrain's l1: 0.0115565\tval's l1: 0.181951\n",
      "[5000]\ttrain's l1: 0.00929246\tval's l1: 0.181769\n",
      "[5500]\ttrain's l1: 0.00753474\tval's l1: 0.181642\n",
      "[6000]\ttrain's l1: 0.00619855\tval's l1: 0.181558\n",
      "[6500]\ttrain's l1: 0.00516216\tval's l1: 0.181494\n",
      "[7000]\ttrain's l1: 0.00434629\tval's l1: 0.18144\n",
      "[7500]\ttrain's l1: 0.00370038\tval's l1: 0.181406\n",
      "[8000]\ttrain's l1: 0.00319108\tval's l1: 0.181376\n",
      "[8500]\ttrain's l1: 0.00277844\tval's l1: 0.181353\n",
      "[9000]\ttrain's l1: 0.00244672\tval's l1: 0.181336\n",
      "[9500]\ttrain's l1: 0.0021754\tval's l1: 0.181325\n",
      "[10000]\ttrain's l1: 0.00195049\tval's l1: 0.181314\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[10000]\ttrain's l1: 0.00195049\tval's l1: 0.181314\n",
      "[2019-07-26 16:40:58.887317] -      Training finished, predicting something\n",
      "[2019-07-26 16:41:11.907976] -      memory footprint:12.7254 GB\n",
      "[2019-07-26 16:41:11.908153] -      reducing amount of garbadge\n",
      "[2019-07-26 16:41:11.948885] -      garbadge eliminated\n",
      "[2019-07-26 16:41:11.949416] -      memory footprint:12.7254 GB\n",
      "[2019-07-26 16:41:11.949470] -      Type: 2JHC\n",
      "[2019-07-26 16:41:12.815924] -      training and applying feature classifiers\n",
      "[2019-07-26 16:41:12.816197] -      memory footprint:12.7254 GB\n",
      "[2019-07-26 16:58:00.632051] -      training feature importance estimator\n",
      "[2019-07-26 16:58:00.632490] -      memory footprint:15.8615 GB\n",
      "[2019-07-26 16:59:46.981517] -     Top20 features: ['adC2', 'rf04', 'dist_no_bond_min_x', 'cos_f0', 'cos_c0', 'adC3', 'adN1', 'cos_c0_f0', 'mulliken_atom_1', 'inv_dist0R', 'dist_no_bond_min_y', 'molecule_atom_index_0_dist_min_div', 'vander_O.y', 'gap', 'dist_mean_bond_y', 'dist_bond_max_y', 'dist_median_bond_y', 'rf_00', 'cos_c0_c1', 'distance']\n",
      "[2019-07-26 16:59:46.981673] -     Training final estiamtor for 2JHC\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/lightgbm/engine.py:118: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 200 rounds.\n",
      "[500]\ttrain's l1: 0.400751\tval's l1: 0.43786\n",
      "[1000]\ttrain's l1: 0.338171\tval's l1: 0.39309\n",
      "[1500]\ttrain's l1: 0.299732\tval's l1: 0.369182\n",
      "[2000]\ttrain's l1: 0.271743\tval's l1: 0.35349\n",
      "[2500]\ttrain's l1: 0.250055\tval's l1: 0.342534\n",
      "[3000]\ttrain's l1: 0.23183\tval's l1: 0.333919\n",
      "[3500]\ttrain's l1: 0.216442\tval's l1: 0.327025\n",
      "[4000]\ttrain's l1: 0.203165\tval's l1: 0.321636\n",
      "[4500]\ttrain's l1: 0.191462\tval's l1: 0.317079\n",
      "[5000]\ttrain's l1: 0.181078\tval's l1: 0.313364\n",
      "[5500]\ttrain's l1: 0.171586\tval's l1: 0.309991\n",
      "[6000]\ttrain's l1: 0.16304\tval's l1: 0.307104\n",
      "[6500]\ttrain's l1: 0.155242\tval's l1: 0.304625\n",
      "[7000]\ttrain's l1: 0.148083\tval's l1: 0.302367\n",
      "[7500]\ttrain's l1: 0.141443\tval's l1: 0.300427\n",
      "[8000]\ttrain's l1: 0.13526\tval's l1: 0.29861\n",
      "[8500]\ttrain's l1: 0.129542\tval's l1: 0.297138\n",
      "[9000]\ttrain's l1: 0.124186\tval's l1: 0.295758\n",
      "[9500]\ttrain's l1: 0.119117\tval's l1: 0.294478\n",
      "[10000]\ttrain's l1: 0.114416\tval's l1: 0.293316\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[10000]\ttrain's l1: 0.114416\tval's l1: 0.293316\n",
      "[2019-07-26 17:06:40.613400] -      Training finished, predicting something\n",
      "[2019-07-26 17:08:18.162769] -      memory footprint:16.2726 GB\n",
      "[2019-07-26 17:08:18.162910] -      reducing amount of garbadge\n",
      "[2019-07-26 17:08:18.313136] -      garbadge eliminated\n",
      "[2019-07-26 17:08:18.313749] -      memory footprint:12.7254 GB\n",
      "[2019-07-26 17:08:18.313903] -      Type: 3JHH\n",
      "[2019-07-26 17:08:18.916115] -      training and applying feature classifiers\n",
      "[2019-07-26 17:08:18.916389] -      memory footprint:12.7254 GB\n",
      "[2019-07-26 17:15:57.649950] -      training feature importance estimator\n",
      "[2019-07-26 17:15:57.650487] -      memory footprint:14.1389 GB\n",
      "[2019-07-26 17:16:59.468328] -     Top20 features: ['cos_c0_c1', 'dist_no_bond_min_y', 'cos_c1', 'dist_no_bond_min_x', 'rf04', 'cos_c0', 'adC3', 'adC2', 'adC4', 'distance_farthest_1', 'rf_03', 'cos_f0', 'cos_c1_f1', 'cos_f1', 'adN1', 'cos_c0_f0', 'dist_mean_bond_y', 'rf_02', 'adH1', 'gap']\n",
      "[2019-07-26 17:16:59.468599] -     Training final estiamtor for 3JHH\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/lightgbm/engine.py:118: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 200 rounds.\n",
      "[500]\ttrain's l1: 0.206512\tval's l1: 0.23793\n",
      "[1000]\ttrain's l1: 0.164814\tval's l1: 0.211052\n",
      "[1500]\ttrain's l1: 0.140339\tval's l1: 0.197619\n",
      "[2000]\ttrain's l1: 0.122901\tval's l1: 0.189242\n",
      "[2500]\ttrain's l1: 0.109739\tval's l1: 0.183638\n",
      "[3000]\ttrain's l1: 0.0989947\tval's l1: 0.179705\n",
      "[3500]\ttrain's l1: 0.0900969\tval's l1: 0.176693\n",
      "[4000]\ttrain's l1: 0.0825075\tval's l1: 0.17432\n",
      "[4500]\ttrain's l1: 0.0758521\tval's l1: 0.17241\n",
      "[5000]\ttrain's l1: 0.0700442\tval's l1: 0.170818\n",
      "[5500]\ttrain's l1: 0.0648568\tval's l1: 0.169574\n",
      "[6000]\ttrain's l1: 0.060252\tval's l1: 0.168527\n",
      "[6500]\ttrain's l1: 0.0561242\tval's l1: 0.167587\n",
      "[7000]\ttrain's l1: 0.0523799\tval's l1: 0.166817\n",
      "[7500]\ttrain's l1: 0.0489794\tval's l1: 0.166158\n",
      "[8000]\ttrain's l1: 0.0458351\tval's l1: 0.165584\n",
      "[8500]\ttrain's l1: 0.043009\tval's l1: 0.165079\n",
      "[9000]\ttrain's l1: 0.0404082\tval's l1: 0.164648\n",
      "[9500]\ttrain's l1: 0.037985\tval's l1: 0.164233\n",
      "[10000]\ttrain's l1: 0.0357632\tval's l1: 0.163873\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[10000]\ttrain's l1: 0.0357632\tval's l1: 0.163873\n",
      "[2019-07-26 17:21:08.299094] -      Training finished, predicting something\n",
      "[2019-07-26 17:22:03.160487] -      memory footprint:14.3516 GB\n",
      "[2019-07-26 17:22:03.160704] -      reducing amount of garbadge\n",
      "[2019-07-26 17:22:03.254728] -      garbadge eliminated\n",
      "[2019-07-26 17:22:03.255236] -      memory footprint:12.7254 GB\n",
      "[2019-07-26 17:22:03.255291] -      Type: 3JHC\n",
      "[2019-07-26 17:22:04.442661] -      training and applying feature classifiers\n",
      "[2019-07-26 17:22:04.443231] -      memory footprint:12.7254 GB\n"
     ]
    }
   ],
   "source": [
    "scores = []\n",
    "\n",
    "i = 1\n",
    "\n",
    "logprint(\"Beginning the training\")\n",
    "logprint(f\"# folds: {n_folds}\")\n",
    "logprint(f\"metaparams: {metaparams}\")\n",
    "logprint(f\"lgb params: {params}\")\n",
    "for i_train, i_val in folds.split(X):\n",
    "    logprint(\"-\"*50)\n",
    "\n",
    "    logprint(f\"Training for split {i}\")\n",
    "    i += 1\n",
    "    \n",
    "    X_train, X_val, y_train, y_val = X.iloc[i_train], X.iloc[i_val], y[i_train], y[i_val]\n",
    "    pred = np.zeros(X_val.shape[0])\n",
    "\n",
    "\n",
    "    d_map_val = dict(zip(list(y_val.index.values),list(np.arange(y_val.shape[0]))))\n",
    "\n",
    "    rf_cols = ['rf_00','rf_01','rf_02','rf_03','rf04','rf05']\n",
    "    rf_cols1 = ['rf_10','rf_11','rf12']\n",
    "    for t in unique_types:\n",
    "        logprint(f\"     Type: {t}\")\n",
    "        \n",
    "        evals_result = {}\n",
    "        \n",
    "        idx_train = X_train[X_train.type==t].index.values\n",
    "        idx_val = X_val[X_val.type==t].index.values\n",
    "        idx_sub = sub[sub.type==t].index.values\n",
    "        \n",
    "        clf = ClassifierTransformer(RandomForestClassifier(n_estimators=metaparams['rf_n_estimators'], n_jobs=28),n_classes=5,cv=5)\n",
    "        clf1 = ClassifierTransformer(RandomForestClassifier(n_estimators=metaparams['rf_n_estimators'], n_jobs=28),n_classes=2,cv=5)\n",
    "       \n",
    "        logprint(\"     training and applying feature classifiers\")\n",
    "        logprint(f\"     memory footprint:{get_mem_usage():.4f} GB\")\n",
    "        X_extra = np.hstack([clf.fit_transform(X_train[X_train.type==t].drop(['type'],axis=1), y_train[idx_train]),clf1.fit_transform(X_train[X_train.type==t].drop(['type'],axis=1), y_train[idx_train])])\n",
    "        X_extra_val = np.hstack([clf.transform(X_val[X_val.type==t].drop(['type'],axis=1)),clf1.transform(X_val[X_val.type==t].drop(['type'],axis=1))])\n",
    "        X_extra_test = np.hstack([clf.transform(X_test[X_test.type==t].drop(['type'],axis=1)),clf1.transform(X_test[X_test.type==t].drop(['type'],axis=1))])\n",
    "        \n",
    "        X_p = pd.DataFrame(data=np.hstack([X_train[X_train.type==t].drop(['type'],axis=1).values,X_extra]), columns=list(X_train.drop(['type'],axis=1).columns)+rf_cols+rf_cols1)\n",
    "        X_p_val = pd.DataFrame(data=np.hstack([X_val[X_val.type==t].drop(['type'],axis=1).values,X_extra_val]), columns=list(X_val.drop(['type'],axis=1).columns)+rf_cols+rf_cols1)\n",
    "        X_p_test = pd.DataFrame(data=np.hstack([X_test[X_test.type==t].drop(['type'],axis=1).values,X_extra_test]), columns=list(X_test.drop(['type'],axis=1).columns)+rf_cols+rf_cols1)\n",
    "\n",
    "        logprint(\"     training feature importance estimator\")\n",
    "        logprint(f\"     memory footprint:{get_mem_usage():.4f} GB\")\n",
    "\n",
    "        gbm = lgb.LGBMRegressor(n_estimators=metaparams['importance_num_iterations'])\n",
    "        gbm.fit(X_p, y_train[idx_train])\n",
    "        gbm.booster_.feature_importance()\n",
    "\n",
    "        fea_imp_ = pd.DataFrame({'cols':X_p.columns, 'fea_imp':gbm.feature_importances_})\n",
    "        \n",
    "        if t=='1JHC':\n",
    "            nb_feat=100\n",
    "        else:\n",
    "            nb_feat=90\n",
    "            \n",
    "        remain_features = list(fea_imp_.loc[fea_imp_.fea_imp > 0].sort_values(by=['fea_imp'], ascending = False)['cols'].values[:nb_feat])\n",
    "        logprint(f\"    Top20 features: {remain_features[:20]}\")\n",
    "\n",
    "        logprint(f'    Training final estiamtor for {t}')\n",
    "        lgb_train = lgb.Dataset(X_p[remain_features],label=y_train[idx_train])\n",
    "        lgb_val = lgb.Dataset(X_p_val[remain_features],label=y_val[idx_val])\n",
    "        model = lgb.train(params=params, train_set=lgb_train,  valid_sets=[lgb_train,lgb_val], valid_names=['train','val'],\n",
    "                      fobj=None, feval=None, init_model=None, feature_name='auto', categorical_feature='auto',evals_result=evals_result,\n",
    "                      early_stopping_rounds=200,  verbose_eval=500)\n",
    "        logprint(\"     Training finished, predicting something\")\n",
    "        val_map = [d_map_val[k] for k in list(idx_val)]\n",
    "        pred[val_map] = model.predict(X_p_val[remain_features])\n",
    "        pred_sub[idx_sub] += (model.predict(X_p_test[remain_features]) / n_folds)\n",
    "        logprint(f\"     memory footprint:{get_mem_usage():.4f} GB\")\n",
    "\n",
    "        logprint(\"     reducing amount of garbadge\")\n",
    "        del X_p\n",
    "        del X_p_val\n",
    "        del X_p_test\n",
    "        del X_extra\n",
    "        del X_extra_val\n",
    "        del X_extra_test\n",
    "        del clf\n",
    "        del clf1\n",
    "        del gbm\n",
    "        gc.collect()\n",
    "        logprint(\"     garbadge eliminated\")\n",
    "        logprint(f\"     memory footprint:{get_mem_usage():.4f} GB\")\n",
    "\n",
    "\n",
    "    jtype = X_val['type']\n",
    "    scores.append(fast_metric(y_val, pred, jtype, verbose=False))\n",
    "    logprint(f\"Fold result: {scores[-1]:.4f} group-mean log(mae)\")\n",
    "\n",
    "CV_score = np.mean(scores)\n",
    "CV_std = np.std(scores)\n",
    "\n",
    "logprint(f\"Cross-Validation score over {n_folds} folds: {CV_score:.4f} +- {CV_std:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub['scalar_coupling_constant'] = pred_sub\n",
    "sub[['id','scalar_coupling_constant']].to_csv(f'../submits/LGB_CV_[{n_folds}]_[{CV_score:.4f}+-{CV_std:.4f}].csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
