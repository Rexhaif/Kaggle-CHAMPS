{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "w# Keras Multiple Output Solution\n",
    "Thanks to https://www.pyimagesearch.com/2018/06/04/keras-multiple-outputs-and-multiple-losses/ and https://www.kaggle.com/kmat2019/neural-network-modeling-with-multiple-outputs for the idea on approaching this problem as a multiple output problem.  Though it doesn't seem to be the favored approach for this competition, I feel that there ought to be a good neural network approach.  This kernel tries a multi-layer, dense neural network implemented in Keras.  The advantage of this approach is that it does not seem to be overfitting, which may pay off against the full dataset.\n",
    "\n",
    "Ways to improve:\n",
    "*  I'm not a domain expert in the molecular chem field... I strongly suspect that stronger feature engineering would cause this approach to score higher.  \n",
    "*  Network architecture:  I'm putting a simpler variant forward here with some options commented out.  There are tweaks that could be made to this architecture that will improve the score.  Forcing the model to overfit to gain a better score on the leaderboard does not usually pay off in the end...\n",
    "*  More epochs.  The more epochs that can be run without overfitting, the better score could be achieved.  My observation is that even after long training epochs, the model seems to still be learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.layers import Dense, Input, Activation\n",
    "from keras.layers import BatchNormalization, Add, Dropout\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Model, load_model\n",
    "from keras import callbacks\n",
    "from keras import backend as K\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "warnings.filterwarnings(action=\"ignore\", category=DeprecationWarning)\n",
    "warnings.filterwarnings(action=\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First grab the data.\n",
    "I don't like to clutter up my solution notebooks with my EDA work.  That's usually a separate notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('../data/train.csv')\n",
    "df_test = pd.read_csv('../data/test.csv')\n",
    "\n",
    "df_giba_train = pd.read_csv(\"../data/train_giba.csv.gz\")\n",
    "df_giba_test = pd.read_csv(\"../data/test_giba.csv.gz\")\n",
    "\n",
    "giba_columns_train = list(set(df_giba_train.columns).difference(set(df_train.columns)))\n",
    "giba_columns_test = list(set(df_giba_test.columns).difference(set(df_test.columns)))\n",
    "\n",
    "df_train = pd.concat((df_train, df_giba_train[giba_columns_train]), axis=1)\n",
    "df_test = pd.concat((df_test, df_giba_test[giba_columns_test]), axis=1)\n",
    "\n",
    "\n",
    "df_struct = pd.read_csv('../data/structures.csv')\n",
    "\n",
    "# df_train_sub_potential=pd.read_csv('/content/champs/potential_energy.csv')\n",
    "# df_train_sub_moment=pd.read_csv('../input/dipole_moments.csv')\n",
    "df_train_sub_charge = pd.read_csv('../data/mulliken_charges.csv')\n",
    "df_train_sub_tensor = pd.read_csv('../data/magnetic_shielding_tensors.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reduce the Memory Usage\n",
    "Without this call, this kernel definitely can't be run on smaller cloud instances... I always test solutions on CoLaboratory to see if low-resource nodes can process them.  In this case, CoLab can't unless you reduce down.  The results seem similar to when the same network is trained on the full dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_mem_usage(df, verbose=True):\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)\n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    if verbose:\n",
    "        print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(\n",
    "            end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Map data into a master dataframe\n",
    "Here's the code to do mappings.  The drop_duplicates is important, else your test dataset will grow and your predictions will not be output correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAM usage: 11.968887329101562 GB\n",
      "Mapping... (4658147, 102) (2358657, 6) 0\n",
      "Mapping... (4658147, 106) (1533537, 3) 0\n",
      "Mapping... (4658147, 107) (1533537, 11) 0\n",
      "Mapping... (2505542, 102) (2358657, 6) 0\n",
      "RAM usage: 14.655155181884766 GB\n",
      "(4658147, 116) (2505542, 106)\n",
      "Mapping... (4658147, 116) (2358657, 10) 1\n"
     ]
    }
   ],
   "source": [
    "''' \n",
    "Map atom info from the structures.csv into the train/test files\n",
    "'''\n",
    "import psutil\n",
    "import os\n",
    "\n",
    "\n",
    "def map_atom_info(df_1, df_2, atom_idx):\n",
    "    print('Mapping...', df_1.shape, df_2.shape, atom_idx)\n",
    "\n",
    "    df = pd.merge(df_1, df_2.drop_duplicates(subset=['molecule_name', 'atom_index']), how='left',\n",
    "                  left_on=['molecule_name', f'atom_index_{atom_idx}'],\n",
    "                  right_on=['molecule_name',  'atom_index'])\n",
    "\n",
    "    df = df.drop('atom_index', axis=1)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def show_ram_usage():\n",
    "    py = psutil.Process(os.getpid())\n",
    "    print('RAM usage: {} GB'.format(py.memory_info()[0]/2. ** 30))\n",
    "\n",
    "\n",
    "show_ram_usage()\n",
    "\n",
    "for atom_idx in [0, 1]:\n",
    "    df_train = map_atom_info(df_train, df_struct, atom_idx)\n",
    "    df_train = map_atom_info(df_train, df_train_sub_charge, atom_idx)\n",
    "    df_train = map_atom_info(df_train, df_train_sub_tensor, atom_idx)\n",
    "    df_train = df_train.rename(columns={'atom': f'atom_{atom_idx}',\n",
    "                                        'x': f'x_{atom_idx}',\n",
    "                                        'y': f'y_{atom_idx}',\n",
    "                                        'z': f'z_{atom_idx}',\n",
    "                                        'mulliken_charge': f'charge_{atom_idx}',\n",
    "                                        'XX': f'XX_{atom_idx}',\n",
    "                                        'YX': f'YX_{atom_idx}',\n",
    "                                        'ZX': f'ZX_{atom_idx}',\n",
    "                                        'XY': f'XY_{atom_idx}',\n",
    "                                        'YY': f'YY_{atom_idx}',\n",
    "                                        'ZY': f'ZY_{atom_idx}',\n",
    "                                        'XZ': f'XZ_{atom_idx}',\n",
    "                                        'YZ': f'YZ_{atom_idx}',\n",
    "                                        'ZZ': f'ZZ_{atom_idx}', })\n",
    "    df_test = map_atom_info(df_test, df_struct, atom_idx)\n",
    "    df_test = df_test.rename(columns={'atom': f'atom_{atom_idx}',\n",
    "                                      'x': f'x_{atom_idx}',\n",
    "                                      'y': f'y_{atom_idx}',\n",
    "                                      'z': f'z_{atom_idx}'})\n",
    "    # add some features\n",
    "\n",
    "    df_struct['c_x'] = df_struct.groupby('molecule_name')[\n",
    "        'x'].transform('mean')\n",
    "    df_struct['c_y'] = df_struct.groupby('molecule_name')[\n",
    "        'y'].transform('mean')\n",
    "    df_struct['c_z'] = df_struct.groupby('molecule_name')[\n",
    "        'z'].transform('mean')\n",
    "    df_struct['atom_n'] = df_struct.groupby(\n",
    "        'molecule_name')['atom_index'].transform('max')\n",
    "\n",
    "    show_ram_usage()\n",
    "    print(df_train.shape, df_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start developing more complex features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAM usage: 15.412620544433594 GB\n",
      "(4658147, 140) (2505542, 118)\n",
      "Mapping... (2505542, 118) (756113, 8) 0\n",
      "Mapping... (2505542, 124) (756113, 8) 1\n",
      "Mapping... (2505542, 130) (756113, 8) 0\n",
      "Mapping... (2505542, 136) (756113, 8) 1\n",
      "Mapping... (4658147, 140) (1405128, 8) 0\n",
      "Mapping... (4658147, 146) (1405128, 8) 1\n",
      "Mapping... (4658147, 152) (1405130, 8) 0\n",
      "Mapping... (4658147, 158) (1405130, 8) 1\n",
      "(4658147, 164) (2505542, 142)\n",
      "RAM usage: 21.83550262451172 GB\n"
     ]
    }
   ],
   "source": [
    "def make_features(df):\n",
    "    df['dx'] = df['x_1']-df['x_0']\n",
    "    df['dy'] = df['y_1']-df['y_0']\n",
    "    df['dz'] = df['z_1']-df['z_0']\n",
    "    df['distance'] = (df['dx']**2+df['dy']**2+df['dz']**2)**(1/2)\n",
    "    return df\n",
    "\n",
    "\n",
    "df_train = make_features(df_train)\n",
    "df_test = make_features(df_test)\n",
    "#df_train = reduce_mem_usage(df_train)\n",
    "#df_test = reduce_mem_usage(df_test)\n",
    "test_prediction = np.zeros(len(df_test))\n",
    "show_ram_usage()\n",
    "print(df_train.shape, df_test.shape)\n",
    "\n",
    "\n",
    "def get_dist(df):\n",
    "    df_temp = df.loc[:, [\"molecule_name\", \"atom_index_0\", \"atom_index_1\",\n",
    "                         \"distance\", \"x_0\", \"y_0\", \"z_0\", \"x_1\", \"y_1\", \"z_1\"]].copy()\n",
    "    df_temp_ = df_temp.copy()\n",
    "    df_temp_ = df_temp_.rename(columns={'atom_index_0': 'atom_index_1',\n",
    "                                        'atom_index_1': 'atom_index_0',\n",
    "                                        'x_0': 'x_1',\n",
    "                                        'y_0': 'y_1',\n",
    "                                        'z_0': 'z_1',\n",
    "                                        'x_1': 'x_0',\n",
    "                                        'y_1': 'y_0',\n",
    "                                        'z_1': 'z_0'})\n",
    "    df_temp_all = pd.concat((df_temp, df_temp_), axis=0)\n",
    "\n",
    "    df_temp_all[\"min_distance\"] = df_temp_all.groupby(\n",
    "        ['molecule_name', 'atom_index_0'])['distance'].transform('min')\n",
    "    df_temp_all[\"max_distance\"] = df_temp_all.groupby(\n",
    "        ['molecule_name', 'atom_index_0'])['distance'].transform('max')\n",
    "\n",
    "    df_temp = df_temp_all[df_temp_all[\"min_distance\"]\n",
    "                          == df_temp_all[\"distance\"]].copy()\n",
    "    df_temp = df_temp.drop(['x_0', 'y_0', 'z_0', 'min_distance'], axis=1)\n",
    "    df_temp = df_temp.rename(columns={'atom_index_0': 'atom_index',\n",
    "                                      'atom_index_1': 'atom_index_closest',\n",
    "                                      'distance': 'distance_closest',\n",
    "                                      'x_1': 'x_closest',\n",
    "                                      'y_1': 'y_closest',\n",
    "                                      'z_1': 'z_closest'})\n",
    "\n",
    "    for atom_idx in [0, 1]:\n",
    "        df = map_atom_info(df, df_temp, atom_idx)\n",
    "        df = df.rename(columns={'atom_index_closest': f'atom_index_closest_{atom_idx}',\n",
    "                                'distance_closest': f'distance_closest_{atom_idx}',\n",
    "                                'x_closest': f'x_closest_{atom_idx}',\n",
    "                                'y_closest': f'y_closest_{atom_idx}',\n",
    "                                'z_closest': f'z_closest_{atom_idx}'})\n",
    "\n",
    "    df_temp = df_temp_all[df_temp_all[\"max_distance\"]\n",
    "                          == df_temp_all[\"distance\"]].copy()\n",
    "    df_temp = df_temp.drop(['x_0', 'y_0', 'z_0', 'max_distance'], axis=1)\n",
    "    df_temp = df_temp.rename(columns={'atom_index_0': 'atom_index',\n",
    "                                      'atom_index_1': 'atom_index_farthest',\n",
    "                                      'distance': 'distance_farthest',\n",
    "                                      'x_1': 'x_farthest',\n",
    "                                      'y_1': 'y_farthest',\n",
    "                                      'z_1': 'z_farthest'})\n",
    "\n",
    "    for atom_idx in [0, 1]:\n",
    "        df = map_atom_info(df, df_temp, atom_idx)\n",
    "        df = df.rename(columns={'atom_index_farthest': f'atom_index_farthest_{atom_idx}',\n",
    "                                'distance_farthest': f'distance_farthest_{atom_idx}',\n",
    "                                'x_farthest': f'x_farthest_{atom_idx}',\n",
    "                                'y_farthest': f'y_farthest_{atom_idx}',\n",
    "                                'z_farthest': f'z_farthest_{atom_idx}'})\n",
    "    return df\n",
    "\n",
    "\n",
    "df_test = (get_dist(df_test))\n",
    "df_train = (get_dist(df_train))\n",
    "\n",
    "print(df_train.shape, df_test.shape)\n",
    "show_ram_usage()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Here is where the Cosine Distance features are Created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4658147, 177) (2505542, 157)\n",
      "RAM usage: 21.950580596923828 GB\n"
     ]
    }
   ],
   "source": [
    "def add_features(df):\n",
    "    df[\"distance_center0\"] = (\n",
    "        (df['x_0']-df['c_x'])**2+(df['y_0']-df['c_y'])**2+(df['z_0']-df['c_z'])**2)**(1/2)\n",
    "    df[\"distance_center1\"] = (\n",
    "        (df['x_1']-df['c_x'])**2+(df['y_1']-df['c_y'])**2+(df['z_1']-df['c_z'])**2)**(1/2)\n",
    "\n",
    "    df[\"distance_c0\"] = ((df['x_0']-df['x_closest_0'])**2+(df['y_0'] -\n",
    "                                                           df['y_closest_0'])**2+(df['z_0']-df['z_closest_0'])**2)**(1/2)\n",
    "    df[\"distance_c1\"] = ((df['x_1']-df['x_closest_1'])**2+(df['y_1'] -\n",
    "                                                           df['y_closest_1'])**2+(df['z_1']-df['z_closest_1'])**2)**(1/2)\n",
    "\n",
    "    df[\"distance_f0\"] = ((df['x_0']-df['x_farthest_0'])**2+(df['y_0'] -\n",
    "                                                            df['y_farthest_0'])**2+(df['z_0']-df['z_farthest_0'])**2)**(1/2)\n",
    "    df[\"distance_f1\"] = ((df['x_1']-df['x_farthest_1'])**2+(df['y_1'] -\n",
    "                                                            df['y_farthest_1'])**2+(df['z_1']-df['z_farthest_1'])**2)**(1/2)\n",
    "\n",
    "    df[\"vec_center0_x\"] = (df['x_0']-df['c_x'])/(df[\"distance_center0\"]+1e-10)\n",
    "    df[\"vec_center0_y\"] = (df['y_0']-df['c_y'])/(df[\"distance_center0\"]+1e-10)\n",
    "    df[\"vec_center0_z\"] = (df['z_0']-df['c_z'])/(df[\"distance_center0\"]+1e-10)\n",
    "\n",
    "    df[\"vec_center1_x\"] = (df['x_1']-df['c_x'])/(df[\"distance_center1\"]+1e-10)\n",
    "    df[\"vec_center1_y\"] = (df['y_1']-df['c_y'])/(df[\"distance_center1\"]+1e-10)\n",
    "    df[\"vec_center1_z\"] = (df['z_1']-df['c_z'])/(df[\"distance_center1\"]+1e-10)\n",
    "\n",
    "    df[\"vec_c0_x\"] = (df['x_0']-df['x_closest_0'])/(df[\"distance_c0\"]+1e-10)\n",
    "    df[\"vec_c0_y\"] = (df['y_0']-df['y_closest_0'])/(df[\"distance_c0\"]+1e-10)\n",
    "    df[\"vec_c0_z\"] = (df['z_0']-df['z_closest_0'])/(df[\"distance_c0\"]+1e-10)\n",
    "\n",
    "    df[\"vec_c1_x\"] = (df['x_1']-df['x_closest_1'])/(df[\"distance_c1\"]+1e-10)\n",
    "    df[\"vec_c1_y\"] = (df['y_1']-df['y_closest_1'])/(df[\"distance_c1\"]+1e-10)\n",
    "    df[\"vec_c1_z\"] = (df['z_1']-df['z_closest_1'])/(df[\"distance_c1\"]+1e-10)\n",
    "\n",
    "    df[\"vec_f0_x\"] = (df['x_0']-df['x_farthest_0'])/(df[\"distance_f0\"]+1e-10)\n",
    "    df[\"vec_f0_y\"] = (df['y_0']-df['y_farthest_0'])/(df[\"distance_f0\"]+1e-10)\n",
    "    df[\"vec_f0_z\"] = (df['z_0']-df['z_farthest_0'])/(df[\"distance_f0\"]+1e-10)\n",
    "\n",
    "    df[\"vec_f1_x\"] = (df['x_1']-df['x_farthest_1'])/(df[\"distance_f1\"]+1e-10)\n",
    "    df[\"vec_f1_y\"] = (df['y_1']-df['y_farthest_1'])/(df[\"distance_f1\"]+1e-10)\n",
    "    df[\"vec_f1_z\"] = (df['z_1']-df['z_farthest_1'])/(df[\"distance_f1\"]+1e-10)\n",
    "\n",
    "    df[\"vec_x\"] = (df['x_1']-df['x_0'])/df[\"distance\"]\n",
    "    df[\"vec_y\"] = (df['y_1']-df['y_0'])/df[\"distance\"]\n",
    "    df[\"vec_z\"] = (df['z_1']-df['z_0'])/df[\"distance\"]\n",
    "\n",
    "    df[\"cos_c0_c1\"] = df[\"vec_c0_x\"]*df[\"vec_c1_x\"] + \\\n",
    "        df[\"vec_c0_y\"]*df[\"vec_c1_y\"]+df[\"vec_c0_z\"]*df[\"vec_c1_z\"]\n",
    "    df[\"cos_f0_f1\"] = df[\"vec_f0_x\"]*df[\"vec_f1_x\"] + \\\n",
    "        df[\"vec_f0_y\"]*df[\"vec_f1_y\"]+df[\"vec_f0_z\"]*df[\"vec_f1_z\"]\n",
    "\n",
    "    df[\"cos_center0_center1\"] = df[\"vec_center0_x\"]*df[\"vec_center1_x\"] + \\\n",
    "        df[\"vec_center0_y\"]*df[\"vec_center1_y\"] + \\\n",
    "        df[\"vec_center0_z\"]*df[\"vec_center1_z\"]\n",
    "\n",
    "    df[\"cos_c0\"] = df[\"vec_c0_x\"]*df[\"vec_x\"] + \\\n",
    "        df[\"vec_c0_y\"]*df[\"vec_y\"]+df[\"vec_c0_z\"]*df[\"vec_z\"]\n",
    "    df[\"cos_c1\"] = df[\"vec_c1_x\"]*df[\"vec_x\"] + \\\n",
    "        df[\"vec_c1_y\"]*df[\"vec_y\"]+df[\"vec_c1_z\"]*df[\"vec_z\"]\n",
    "\n",
    "    df[\"cos_f0\"] = df[\"vec_f0_x\"]*df[\"vec_x\"] + \\\n",
    "        df[\"vec_f0_y\"]*df[\"vec_y\"]+df[\"vec_f0_z\"]*df[\"vec_z\"]\n",
    "    df[\"cos_f1\"] = df[\"vec_f1_x\"]*df[\"vec_x\"] + \\\n",
    "        df[\"vec_f1_y\"]*df[\"vec_y\"]+df[\"vec_f1_z\"]*df[\"vec_z\"]\n",
    "\n",
    "    df[\"cos_center0\"] = df[\"vec_center0_x\"]*df[\"vec_x\"] + \\\n",
    "        df[\"vec_center0_y\"]*df[\"vec_y\"]+df[\"vec_center0_z\"]*df[\"vec_z\"]\n",
    "    df[\"cos_center1\"] = df[\"vec_center1_x\"]*df[\"vec_x\"] + \\\n",
    "        df[\"vec_center1_y\"]*df[\"vec_y\"]+df[\"vec_center1_z\"]*df[\"vec_z\"]\n",
    "\n",
    "    df = df.drop(['vec_c0_x', 'vec_c0_y', 'vec_c0_z', 'vec_c1_x', 'vec_c1_y', 'vec_c1_z',\n",
    "                  'vec_f0_x', 'vec_f0_y', 'vec_f0_z', 'vec_f1_x', 'vec_f1_y', 'vec_f1_z',\n",
    "                  'vec_center0_x', 'vec_center0_y', 'vec_center0_z', 'vec_center1_x',\n",
    "                  'vec_center1_y', 'vec_center1_z',\n",
    "                  'vec_x', 'vec_y', 'vec_z'], axis=1)\n",
    "    return df\n",
    "\n",
    "\n",
    "df_train = add_features(df_train)\n",
    "df_test = add_features(df_test)\n",
    "print(df_train.shape, df_test.shape)\n",
    "show_ram_usage()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network definition\n",
    "\n",
    "This neural network is many layers.  In the middle we define our outputs for our two Mullikan charges as well as our Dipole Moment.  The final output is the one we care the most about, the Scalar Coupling Constant.\n",
    "\n",
    "I think that BatchNormalization at each layer seems superior than small amounts of dropouts.  The network seems to not overfit, even in large numbers of training epochs.  If you do wind up seeing some overfitting, then adding the dropout to a couple of layers ought to help a lot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_nn_model(input_shape):\n",
    "    inp = Input(shape=(input_shape,))\n",
    "    x = Dense(256)(inp)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU(alpha=0.05)(x)\n",
    "    x = Dropout(0.4)(x)\n",
    "    \n",
    "    x = Dense(1024)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU(alpha=0.05)(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    \n",
    "    x = Dense(1024)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU(alpha=0.05)(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    \n",
    "    x = Dense(512)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU(alpha=0.05)(x)\n",
    "    x = Dropout(0.4)(x)\n",
    "    \n",
    "    x = Dense(512)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU(alpha=0.05)(x)\n",
    "    #x = Dropout(0.4)(x)\n",
    "    \n",
    "    x = Dense(256)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU(alpha=0.05)(x)\n",
    "    x = Dropout(0.4)(x)\n",
    "    \n",
    "    out1 = Dense(2, activation=\"linear\")(x)  # mulliken charge 2\n",
    "    out2 = Dense(6, activation=\"linear\")(x)  # tensor 6(xx,yy,zz)\n",
    "    out3 = Dense(12, activation=\"linear\")(x)  # tensor 12(others)\n",
    "    \n",
    "    x = Dense(128)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU(alpha=0.05)(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    \n",
    "    x = Dense(128)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU(alpha=0.05)(x)\n",
    "    \n",
    "    x = Dense(64)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU(alpha=0.05)(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    \n",
    "    out = Dense(1, activation=\"linear\")(x)  # scalar_coupling_constant\n",
    "    model = Model(inputs=inp, outputs=[out, out1, out2, out3])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Function\n",
    "I rely a lot on loss plots to detect when learning has stopped as well as when overfitting begins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(history, label):\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('Loss for %s' % label)\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    _ = plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_features = [\n",
    "        \"x_0\", \"y_0\", \"z_0\",\n",
    "        \"x_1\", \"y_1\", \"z_1\",\n",
    "        \"c_x\", \"c_y\", \"c_z\",\n",
    "        'x_closest_0', 'y_closest_0', 'z_closest_0',\n",
    "        'x_closest_1', 'y_closest_1', 'z_closest_1', \n",
    "        \"distance\", \"distance_center0\",\"distance_center1\", \"distance_c0\",\n",
    "        \"distance_c1\", \"distance_f0\", \"distance_f1\", \n",
    "        \"cos_c0_c1\", \"cos_f0_f1\", \"cos_center0_center1\", \"cos_c0\", \"cos_c1\",\n",
    "        \"cos_f0\", \"cos_f1\", \"cos_center0\", \"cos_center1\",\n",
    "        \"atom_n\"\n",
    "    ] + giba_columns_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_features_fixed = ['x_0',\n",
    " 'y_0',\n",
    " 'z_0',\n",
    " 'x_1',\n",
    " 'y_1',\n",
    " 'z_1',\n",
    " 'c_x',\n",
    " 'c_y',\n",
    " 'c_z',\n",
    " 'x_closest_0',\n",
    " 'y_closest_0',\n",
    " 'z_closest_0',\n",
    " 'x_closest_1',\n",
    " 'y_closest_1',\n",
    " 'z_closest_1',\n",
    " 'distance',\n",
    " 'distance_center0',\n",
    " 'distance_center1',\n",
    " 'distance_c0',\n",
    " 'distance_c1',\n",
    " 'distance_f0',\n",
    " 'distance_f1',\n",
    " 'cos_c0_c1',\n",
    " 'cos_f0_f1',\n",
    " 'cos_center0_center1',\n",
    " 'cos_c0',\n",
    " 'cos_c1',\n",
    " 'cos_f0',\n",
    " 'cos_f1',\n",
    " 'cos_center0',\n",
    " 'cos_center1',\n",
    " 'atom_n',\n",
    " 'adC1',\n",
    " 'structure_z_1',\n",
    " 'linkM0',\n",
    " 'coulomb_C.x',\n",
    " 'inv_dist1',\n",
    " 'structure_x_0',\n",
    " 'inv_dist1R',\n",
    " 'dist_xyz',\n",
    " 'structure_x_1',\n",
    " 'distN1',\n",
    " 'N1',\n",
    " 'E0',\n",
    " 'coulomb_N.y',\n",
    " 'structure_y_1',\n",
    " 'E1',\n",
    " 'adC3',\n",
    " 'distC0',\n",
    " 'yukawa_C.x',\n",
    " 'coulomb_O.y',\n",
    " 'vander_N.y',\n",
    " 'adH4',\n",
    " 'structure_y_0',\n",
    " 'coulomb_O.x',\n",
    " 'yukawa_H.x',\n",
    " 'link1',\n",
    " 'min_molecule_atom_0_dist_xyz',\n",
    " 'distH1',\n",
    " 'sd_molecule_atom_0_dist_xyz',\n",
    " 'yukawa_O.x',\n",
    " 'inv_distPE',\n",
    " 'inv_dist0E',\n",
    " 'link0',\n",
    " 'adN2',\n",
    " 'yukawa_H.y',\n",
    " 'NN',\n",
    " 'mean_molecule_atom_1_dist_xyz',\n",
    " 'yukawa_N.y',\n",
    " 'vander_C.y',\n",
    " 'adN3',\n",
    " 'mean_molecule_atom_0_dist_xyz',\n",
    " 'yukawa_O.y',\n",
    " 'inv_dist1E',\n",
    " 'max_molecule_atom_1_dist_xyz',\n",
    " 'linkM1',\n",
    " 'yukawa_F.x',\n",
    " 'NH',\n",
    " 'coulomb_N.x',\n",
    " 'inv_dist0',\n",
    " 'NO',\n",
    " 'vander_F.y',\n",
    " 'adN1',\n",
    " 'structure_z_0',\n",
    " 'adC4',\n",
    " 'vander_H.x',\n",
    " 'R0',\n",
    " 'atom_index_1.1',\n",
    " 'NF',\n",
    " 'vander_C.x',\n",
    " 'NC',\n",
    " 'ID',\n",
    " 'yukawa_F.y',\n",
    " 'distC1',\n",
    " 'adN4',\n",
    " 'pos',\n",
    " 'linkN',\n",
    " 'adH2',\n",
    " 'R1',\n",
    " 'adH3',\n",
    " 'typei',\n",
    " 'coulomb_H.x',\n",
    " 'vander_O.y',\n",
    " 'yukawa_C.y',\n",
    " 'adC2',\n",
    " 'vander_H.y',\n",
    " 'coulomb_H.y',\n",
    " 'adH1',\n",
    " 'coulomb_F.y',\n",
    " 'max_molecule_atom_0_dist_xyz',\n",
    " 'inv_distP',\n",
    " 'vander_F.x',\n",
    " 'coulomb_C.y',\n",
    " 'N2',\n",
    " 'distH0',\n",
    " 'yukawa_N.x',\n",
    " 'distN0',\n",
    " 'coulomb_F.x',\n",
    " 'vander_N.x',\n",
    " 'min_molecule_atom_1_dist_xyz',\n",
    " 'inv_distPR',\n",
    " 'inv_dist0R',\n",
    " 'vander_O.x',\n",
    " 'sd_molecule_atom_1_dist_xyz']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.loc[:, input_features].to_hdf(\"../data/train_features.hdf\", \"df\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.loc[:, 'scalar_coupling_constant'].to_hdf(\"../data/train_label.hdf\", 'df')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.loc[:, [\"charge_0\", \"charge_1\"]].to_hdf(\"../data/train_target_1.hdf\", 'df')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.loc[:, [\"XX_0\", \"YY_0\", \"ZZ_0\", \"XX_1\", \"YY_1\", \"ZZ_1\"]].to_hdf(\"../data/train_target_2.hdf\", 'df')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.loc[:, [\"YX_0\", \"ZX_0\", \"XY_0\", \"ZY_0\",\n",
    "                                      \"XZ_0\", \"YZ_0\", \"YX_1\", \"ZX_1\", \"XY_1\", \"ZY_1\", \"XZ_1\", \"YZ_1\"]].to_hdf(\"../data/train_target_3.hdf\", 'df')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.loc[:, input_features].to_hdf(\"../data/test_features.hdf\", 'df')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Routine\n",
    "\n",
    "A bunch of stuff happens here.  Pay attention to the callbacks.  I train a different model for each molecule type, which allows for future retraining.  If you have kept your network the same (except for dropout, etc.), and want to retrain for a few more epochs without having to go back to the beginning, then set the retrain flag to False and it will grab the trained models as starting points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training 1JHC out of ['1JHC' '2JHH' '1JHN' '2JHN' '2JHC' '3JHH' '3JHC' '3JHN'] \n",
      "\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Restoring model weights from the end of the best epoch\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-51-3529f45c033f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    105\u001b[0m                            validation_data=(\n\u001b[1;32m    106\u001b[0m                                cv_input, [cv_target, cv_target_1, cv_target_2, cv_target_3]),\n\u001b[0;32m--> 107\u001b[0;31m                            callbacks=[es, rlr, sv_mod], epochs=epoch_n, batch_size=batch_size, verbose=verbose)\n\u001b[0m\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0mcv_predict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcv_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    215\u001b[0m                         \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_outs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m                             \u001b[0mepoch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m         \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36mon_epoch_end\u001b[0;34m(self, epoch, logs)\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogs\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m             \u001b[0mcallback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36mon_epoch_end\u001b[0;34m(self, epoch, logs)\u001b[0m\n\u001b[1;32m    555\u001b[0m                         print('Restoring model weights from the end of '\n\u001b[1;32m    556\u001b[0m                               'the best epoch')\n\u001b[0;32m--> 557\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    558\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_train_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/network.py\u001b[0m in \u001b[0;36mset_weights\u001b[0;34m(self, weights)\u001b[0m\n\u001b[1;32m    502\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    503\u001b[0m             \u001b[0mnum_param\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 504\u001b[0;31m             \u001b[0mlayer_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mnum_param\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    505\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0msw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    506\u001b[0m                 \u001b[0mtuples\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "\n",
    "mol_types = df_train[\"type\"].unique()\n",
    "cv_score = []\n",
    "cv_score_total = 0\n",
    "epoch_n = 300\n",
    "verbose = 0\n",
    "batch_size = 4096\n",
    "\n",
    "# Set to True if we want to train from scratch.  False will reuse saved models as a starting point.\n",
    "retrain = True\n",
    "\n",
    "\n",
    "# Set up GPU preferences\n",
    "config = tf.ConfigProto(device_count={'GPU': 1, 'CPU': 2})\n",
    "config.gpu_options.allow_growth = True\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.6\n",
    "sess = tf.Session(config=config)\n",
    "K.set_session(sess)\n",
    "\n",
    "start_time = datetime.now()\n",
    "\n",
    "# Loop through each molecule type\n",
    "for mol_type in mol_types:\n",
    "    model_name_rd = (\n",
    "        '../models/molecule_model_giba_%s.hdf5' % mol_type)\n",
    "    model_name_wrt = ('../models/molecule_model_giba_%s.hdf5' % mol_type)\n",
    "    print('Training %s' % mol_type, 'out of', mol_types, '\\n')\n",
    "\n",
    "    df_train_ = df_train[df_train[\"type\"] == mol_type]\n",
    "    df_test_ = df_test[df_test[\"type\"] == mol_type]\n",
    "\n",
    "    # Here's our best features.  We think.\n",
    "    input_features = [\n",
    "        \"x_0\", \"y_0\", \"z_0\",\n",
    "        \"x_1\", \"y_1\", \"z_1\",\n",
    "        \"c_x\", \"c_y\", \"c_z\",\n",
    "        'x_closest_0', 'y_closest_0', 'z_closest_0',\n",
    "        'x_closest_1', 'y_closest_1', 'z_closest_1', \n",
    "        \"distance\", \"distance_center0\",\"distance_center1\", \"distance_c0\",\n",
    "        \"distance_c1\", \"distance_f0\", \"distance_f1\", \n",
    "        \"cos_c0_c1\", \"cos_f0_f1\", \"cos_center0_center1\", \"cos_c0\", \"cos_c1\",\n",
    "        \"cos_f0\", \"cos_f1\", \"cos_center0\", \"cos_center1\",\n",
    "        \"atom_n\"\n",
    "    ] + giba_columns_train\n",
    "\n",
    "    # Standard Scaler from sklearn does seem to work better here than other Scalers\n",
    "    input_data = StandardScaler().fit_transform(\n",
    "        pd.concat([df_train_.loc[:, input_features].dropna(axis=1, how='all'), df_test_.loc[:, input_features].dropna(axis=1, how='all')]))\n",
    "\n",
    "    target_data = df_train_.loc[:, \"scalar_coupling_constant\"].values\n",
    "    target_data_1 = df_train_.loc[:, [\"charge_0\", \"charge_1\"]]\n",
    "    target_data_2 = df_train_.loc[:, [\n",
    "        \"XX_0\", \"YY_0\", \"ZZ_0\", \"XX_1\", \"YY_1\", \"ZZ_1\"]]\n",
    "    target_data_3 = df_train_.loc[:, [\"YX_0\", \"ZX_0\", \"XY_0\", \"ZY_0\",\n",
    "                                      \"XZ_0\", \"YZ_0\", \"YX_1\", \"ZX_1\", \"XY_1\", \"ZY_1\", \"XZ_1\", \"YZ_1\"]]\n",
    "\n",
    "    # following parameters should be adjusted to control the loss function\n",
    "    # if all parameters are zero, attractors do not work. (-> simple neural network)\n",
    "    m1 = 1\n",
    "    m2 = 4\n",
    "    m3 = 1\n",
    "    target_data_1 = m1*(StandardScaler().fit_transform(target_data_1))\n",
    "    target_data_2 = m2*(StandardScaler().fit_transform(target_data_2))\n",
    "    target_data_3 = m3*(StandardScaler().fit_transform(target_data_3))\n",
    "\n",
    "    # Simple split to provide us a validation set to do our CV checks with\n",
    "    train_index, cv_index = train_test_split(\n",
    "        np.arange(len(df_train_)), random_state=111, test_size=0.1)\n",
    "\n",
    "    # Split all our input and targets by train and cv indexes\n",
    "    train_input = input_data[train_index]\n",
    "    cv_input = input_data[cv_index]\n",
    "    train_target = target_data[train_index]\n",
    "    cv_target = target_data[cv_index]\n",
    "    train_target_1 = target_data_1[train_index]\n",
    "    cv_target_1 = target_data_1[cv_index]\n",
    "    train_target_2 = target_data_2[train_index]\n",
    "    cv_target_2 = target_data_2[cv_index]\n",
    "    train_target_3 = target_data_3[train_index]\n",
    "    cv_target_3 = target_data_3[cv_index]\n",
    "    test_input = input_data[len(df_train_):, :]\n",
    "\n",
    "    # Build the Neural Net\n",
    "    nn_model = create_nn_model(train_input.shape[1])\n",
    "\n",
    "    # If retrain==False, then we load a previous saved model as a starting point.\n",
    "    if not retrain:\n",
    "        nn_model = load_model(model_name_rd)\n",
    "\n",
    "    nn_model.compile(loss='mae', optimizer=Adam(lr=0.001))  # , metrics=[auc])\n",
    "\n",
    "    # Callback for Early Stopping... May want to raise the min_delta for small numbers of epochs\n",
    "    es = callbacks.EarlyStopping(monitor='val_loss', min_delta=0.0001,\n",
    "                                 patience=8, verbose=1, mode='auto', restore_best_weights=True)\n",
    "    # Callback for Reducing the Learning Rate... when the monitor levels out for 'patience' epochs, then the LR is reduced\n",
    "    rlr = callbacks.ReduceLROnPlateau(\n",
    "        monitor='val_loss', factor=0.1, patience=7, min_lr=1e-6, mode='auto', verbose=1)\n",
    "    # Save the best value of the model for future use\n",
    "    sv_mod = callbacks.ModelCheckpoint(\n",
    "        model_name_wrt, monitor='val_loss', save_best_only=True, period=1)\n",
    "\n",
    "    history = nn_model.fit(train_input, [train_target, train_target_1, train_target_2, train_target_3],\n",
    "                           validation_data=(\n",
    "                               cv_input, [cv_target, cv_target_1, cv_target_2, cv_target_3]),\n",
    "                           callbacks=[es, rlr, sv_mod], epochs=epoch_n, batch_size=batch_size, verbose=verbose)\n",
    "\n",
    "    cv_predict = nn_model.predict(cv_input)\n",
    "    plot_history(history, mol_type)\n",
    "\n",
    "    accuracy = np.mean(np.abs(cv_target-cv_predict[0][:, 0]))\n",
    "    cv_score.append(np.log(accuracy))\n",
    "    cv_score_total += np.log(accuracy)\n",
    "\n",
    "    # Predict on the test data set using our trained model\n",
    "    test_predict = nn_model.predict(test_input)\n",
    "\n",
    "    # for each molecule type we'll grab the predicted values\n",
    "    test_prediction[df_test[\"type\"] == mol_type] = test_predict[0][:, 0]\n",
    "    K.clear_session()\n",
    "\n",
    "cv_score_total /= len(mol_types)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare results for Submission\n",
    "\n",
    "The total CV score matches Kaggle's score pretty closely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2505542 2505542\n",
      "Total training time:  0:47:28.974955\n",
      "1JHC : cv score is  0.26284854816454384\n",
      "2JHH : cv score is  -1.4871404813769378\n",
      "1JHN : cv score is  -0.32283828204587417\n",
      "2JHN : cv score is  -1.6303546359911507\n",
      "2JHC : cv score is  -1.0032503524487386\n",
      "3JHH : cv score is  -1.5911100306572448\n",
      "3JHC : cv score is  -1.0280413210526658\n",
      "3JHN : cv score is  -1.9336959373865361\n",
      "total cv score is -1.0916978115993254\n"
     ]
    }
   ],
   "source": [
    "def submit(predictions):\n",
    "    submit = pd.read_csv('../data/sample_submission.csv')\n",
    "    print(len(submit), len(predictions))\n",
    "    submit[\"scalar_coupling_constant\"] = predictions\n",
    "    submit.to_csv(\"../submits/workingsubmission-test.csv.zip\", index=False)\n",
    "\n",
    "\n",
    "submit(test_prediction)\n",
    "\n",
    "print('Total training time: ', datetime.now() - start_time)\n",
    "\n",
    "i = 0\n",
    "for mol_type in mol_types:\n",
    "    print(mol_type, \": cv score is \", cv_score[i])\n",
    "    i += 1\n",
    "print(\"total cv score is\", cv_score_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.4\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "print(keras.__version__)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
