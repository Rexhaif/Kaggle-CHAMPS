{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "If you feel 16GB memory is too small for >200 features bond-wise, you are not alone. In this kernel, we updated Andrew's workflow by generating features for each type to save memory. No `reduce_mem_usage` by Andrew is used (except when importing QM9), so every computation is retained its `np.float64` default accuracy. \n",
    "\n",
    "Within the loop of the training for each type, first the features are generated for a minimal passed train/test dataframe, then [Giba's features](https://www.kaggle.com/scaomath/lgb-giba-features-qm9-custom-objective-in-python) are loaded using the format of a kernel I made earlier. Finally the [Yukawa potentials](https://www.kaggle.com/scaomath/parallelization-of-coulomb-yukawa-interaction) are added as well using `structures` dataframe by type to save a tons of memory.\n",
    "\n",
    "Just wrapping your feature generation into a function you are good to go.\n",
    "\n",
    "\n",
    "### References:\n",
    "- [Brute force feature engineering](https://www.kaggle.com/artgor/brute-force-feature-engineering)\n",
    "- [Keras Neural Net for CHAMPS](https://www.kaggle.com/todnewman/keras-neural-net-for-champs)\n",
    "- [Giba R + data.table + Simple Features](https://www.kaggle.com/titericz/giba-r-data-table-simple-features-1-17-lb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_kg_hide-input": false,
    "_kg_hide-output": false,
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm_notebook\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "pd.options.display.precision = 15\n",
    "\n",
    "from catboost import CatBoostRegressor\n",
    "\n",
    "import lightgbm as lgb\n",
    "import time\n",
    "import datetime\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import StratifiedKFold, KFold, RepeatedKFold\n",
    "from sklearn import metrics\n",
    "from sklearn import linear_model\n",
    "import gc\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.impute import SimpleImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_kg_hide-input": true,
    "_kg_hide-output": false,
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "## Andrew's utils\n",
    "def reduce_mem_usage(df, verbose=True):\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024**2    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)    \n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
    "    return df\n",
    "\n",
    "\n",
    "def group_mean_log_mae(y_true, y_pred, types, floor=1e-9):\n",
    "    \"\"\"\n",
    "    Fast metric computation for this competition: https://www.kaggle.com/c/champs-scalar-coupling\n",
    "    Code is from this kernel: https://www.kaggle.com/uberkinder/efficient-metric\n",
    "    \"\"\"\n",
    "    maes = (y_true-y_pred).abs().groupby(types).mean()\n",
    "    return np.log(maes.map(lambda x: max(x, floor))).mean()\n",
    "    \n",
    "\n",
    "def train_model_regression(X, X_test, y, params, folds, model_type='lgb', eval_metric='mae', columns=None, plot_feature_importance=False, model=None,\n",
    "                               verbose=10000, early_stopping_rounds=200, n_estimators=50000):\n",
    "    \"\"\"\n",
    "    A function to train a variety of regression models.\n",
    "    Returns dictionary with oof predictions, test predictions, scores and, if necessary, feature importances.\n",
    "    \n",
    "    :params: X - training data, can be pd.DataFrame or np.ndarray (after normalizing)\n",
    "    :params: X_test - test data, can be pd.DataFrame or np.ndarray (after normalizing)\n",
    "    :params: y - target\n",
    "    :params: folds - folds to split data\n",
    "    :params: model_type - type of model to use\n",
    "    :params: eval_metric - metric to use\n",
    "    :params: columns - columns to use. If None - use all columns\n",
    "    :params: plot_feature_importance - whether to plot feature importance of LGB\n",
    "    :params: model - sklearn model, works only for \"sklearn\" model type\n",
    "    \n",
    "    \"\"\"\n",
    "    columns = X.columns if columns is None else columns\n",
    "    X_test = X_test[columns]\n",
    "    \n",
    "    # to set up scoring parameters\n",
    "    metrics_dict = {'mae': {'lgb_metric_name': 'mae',\n",
    "                        'catboost_metric_name': 'MAE',\n",
    "                        'sklearn_scoring_function': metrics.mean_absolute_error},\n",
    "                    'group_mae': {'lgb_metric_name': 'mae',\n",
    "                        'catboost_metric_name': 'MAE',\n",
    "                        'scoring_function': group_mean_log_mae},\n",
    "                    'mse': {'lgb_metric_name': 'mse',\n",
    "                        'catboost_metric_name': 'MSE',\n",
    "                        'sklearn_scoring_function': metrics.mean_squared_error}\n",
    "                    }\n",
    "\n",
    "    \n",
    "    result_dict = {}\n",
    "    \n",
    "    # out-of-fold predictions on train data\n",
    "    oof = np.zeros(len(X))\n",
    "    \n",
    "    # averaged predictions on train data\n",
    "    prediction = np.zeros(len(X_test))\n",
    "    \n",
    "    # list of scores on folds\n",
    "    scores = []\n",
    "    feature_importance = pd.DataFrame()\n",
    "    \n",
    "    # split and train on folds\n",
    "    for fold_n, (train_index, valid_index) in enumerate(folds.split(X)):\n",
    "        print(f'Fold {fold_n + 1} started at {time.ctime()}')\n",
    "        if type(X) == np.ndarray:\n",
    "            X_train, X_valid = X[columns][train_index], X[columns][valid_index]\n",
    "            y_train, y_valid = y[train_index], y[valid_index]\n",
    "        else:\n",
    "            X_train, X_valid = X[columns].iloc[train_index], X[columns].iloc[valid_index]\n",
    "            y_train, y_valid = y.iloc[train_index], y.iloc[valid_index]\n",
    "            \n",
    "        if model_type == 'lgb':\n",
    "            model = lgb.LGBMRegressor(**params, n_estimators = n_estimators, n_jobs = -1)\n",
    "            model.fit(X_train, y_train, \n",
    "                    eval_set=[(X_train, y_train), (X_valid, y_valid)], eval_metric=metrics_dict[eval_metric]['lgb_metric_name'],\n",
    "                    verbose=verbose, early_stopping_rounds=early_stopping_rounds)\n",
    "            \n",
    "            y_pred_valid = model.predict(X_valid)\n",
    "            y_pred = model.predict(X_test, num_iteration=model.best_iteration_)\n",
    "            \n",
    "        if model_type == 'xgb':\n",
    "            train_data = xgb.DMatrix(data=X_train, label=y_train, feature_names=X.columns)\n",
    "            valid_data = xgb.DMatrix(data=X_valid, label=y_valid, feature_names=X.columns)\n",
    "\n",
    "            watchlist = [(train_data, 'train'), (valid_data, 'valid_data')]\n",
    "            model = xgb.train(dtrain=train_data, num_boost_round=20000, evals=watchlist, early_stopping_rounds=200, verbose_eval=verbose, params=params)\n",
    "            y_pred_valid = model.predict(xgb.DMatrix(X_valid, feature_names=X.columns), ntree_limit=model.best_ntree_limit)\n",
    "            y_pred = model.predict(xgb.DMatrix(X_test, feature_names=X.columns), ntree_limit=model.best_ntree_limit)\n",
    "        \n",
    "        if model_type == 'sklearn-linreg':\n",
    "            model = LinearRegression(**params)\n",
    "            \n",
    "            imputer = SimpleImputer(missing_values='NaN', strategy='median')\n",
    "            X_train = imputer.fit_transform(X_train)\n",
    "            X_valid = imputer.transform(X_valid)\n",
    "            X_test = imputer.transform(X_test)\n",
    "            \n",
    "            model.fit(X_train, y_train)\n",
    "            \n",
    "            y_pred_valid = model.predict(X_valid).reshape(-1,)\n",
    "            score = metrics_dict[eval_metric]['sklearn_scoring_function'](y_valid, y_pred_valid)\n",
    "            print(f'Fold {fold_n}. {eval_metric}: {score:.4f}.')\n",
    "            print('')\n",
    "            \n",
    "            y_pred = model.predict(X_test).reshape(-1,)\n",
    "        \n",
    "        if model_type == 'cat':\n",
    "            model = CatBoostRegressor(iterations=40000, task_type='GPU',\n",
    "                                      eval_metric=metrics_dict[eval_metric]['catboost_metric_name'], **params,\n",
    "                                      loss_function=metrics_dict[eval_metric]['catboost_metric_name'])\n",
    "            model.fit(X_train, y_train, eval_set=(X_valid, y_valid), cat_features=[], use_best_model=True, verbose=False, early_stopping_rounds=2000)\n",
    "\n",
    "            y_pred_valid = model.predict(X_valid)\n",
    "            y_pred = model.predict(X_test)\n",
    "            score = metrics_dict[eval_metric]['sklearn_scoring_function'](y_valid, y_pred_valid)\n",
    "            print(f'Fold {fold_n}. {eval_metric}: {score:.4f}.')\n",
    "            print('')\n",
    "        \n",
    "        oof[valid_index] = y_pred_valid.reshape(-1,)\n",
    "        if eval_metric != 'group_mae':\n",
    "            scores.append(metrics_dict[eval_metric]['sklearn_scoring_function'](y_valid, y_pred_valid))\n",
    "        else:\n",
    "            scores.append(metrics_dict[eval_metric]['scoring_function'](y_valid, y_pred_valid, X_valid['type']))\n",
    "\n",
    "        prediction += y_pred    \n",
    "        \n",
    "        if model_type == 'lgb' and plot_feature_importance:\n",
    "            # feature importance\n",
    "            fold_importance = pd.DataFrame()\n",
    "            fold_importance[\"feature\"] = columns\n",
    "            fold_importance[\"importance\"] = model.feature_importances_\n",
    "            fold_importance[\"fold\"] = fold_n + 1\n",
    "            feature_importance = pd.concat([feature_importance, fold_importance], axis=0)\n",
    "\n",
    "    prediction /= folds.n_splits\n",
    "    \n",
    "    print('CV mean score: {0:.4f}, std: {1:.4f}.'.format(np.mean(scores), np.std(scores)))\n",
    "    \n",
    "    result_dict['oof'] = oof\n",
    "    result_dict['prediction'] = prediction\n",
    "    result_dict['scores'] = scores\n",
    "    \n",
    "    if model_type == 'lgb':\n",
    "        if plot_feature_importance:\n",
    "            feature_importance[\"importance\"] /= folds.n_splits\n",
    "            cols = feature_importance[[\"feature\", \"importance\"]].groupby(\"feature\").mean().sort_values(\n",
    "                by=\"importance\", ascending=False)[:50].index\n",
    "\n",
    "            best_features = feature_importance.loc[feature_importance.feature.isin(cols)]\n",
    "\n",
    "            plt.figure(figsize=(16, 12));\n",
    "            sns.barplot(x=\"importance\", y=\"feature\", data=best_features.sort_values(by=\"importance\", ascending=False));\n",
    "            plt.title('LGB Features (avg over folds)');\n",
    "            \n",
    "            result_dict['feature_importance'] = feature_importance\n",
    "        \n",
    "    return result_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading and overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_folder = '../data/raw' \n",
    "train = pd.read_csv(f'{file_folder}/train.csv')\n",
    "test = pd.read_csv(f'{file_folder}/test.csv')\n",
    "sub = pd.read_csv(f'{file_folder}/sample_submission.csv')\n",
    "structures = pd.read_csv(f'{file_folder}/structures.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../data/raw'\n",
    "structures_yukawa = pd.read_csv(f'{path}/structures_yukawa.csv')\n",
    "structures = pd.concat([structures, structures_yukawa], axis=1)\n",
    "del structures_yukawa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../data/raw'\n",
    "\n",
    "structures_yukawa = pd.read_csv(f'{path}/structures_yukawa.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_columns = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dist_C_0</th>\n",
       "      <th>dist_C_1</th>\n",
       "      <th>dist_C_2</th>\n",
       "      <th>dist_C_3</th>\n",
       "      <th>dist_C_4</th>\n",
       "      <th>dist_F_0</th>\n",
       "      <th>dist_F_1</th>\n",
       "      <th>dist_F_2</th>\n",
       "      <th>dist_F_3</th>\n",
       "      <th>dist_F_4</th>\n",
       "      <th>dist_H_0</th>\n",
       "      <th>dist_H_1</th>\n",
       "      <th>dist_H_2</th>\n",
       "      <th>dist_H_3</th>\n",
       "      <th>dist_H_4</th>\n",
       "      <th>dist_N_0</th>\n",
       "      <th>dist_N_1</th>\n",
       "      <th>dist_N_2</th>\n",
       "      <th>dist_N_3</th>\n",
       "      <th>dist_N_4</th>\n",
       "      <th>dist_O_0</th>\n",
       "      <th>dist_O_1</th>\n",
       "      <th>dist_O_2</th>\n",
       "      <th>dist_O_3</th>\n",
       "      <th>dist_O_4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2.358632000000000e+06</td>\n",
       "      <td>2.358053000000000e+06</td>\n",
       "      <td>2.351206000000000e+06</td>\n",
       "      <td>2.308218000000000e+06</td>\n",
       "      <td>2.136831000000000e+06</td>\n",
       "      <td>24464.000000000000000</td>\n",
       "      <td>8346.000000000000000</td>\n",
       "      <td>5184.000000000000000</td>\n",
       "      <td>11.000000000000000</td>\n",
       "      <td>11.000000000000000</td>\n",
       "      <td>2.358457000000000e+06</td>\n",
       "      <td>2.355452000000000e+06</td>\n",
       "      <td>2.344934000000000e+06</td>\n",
       "      <td>2.318028000000000e+06</td>\n",
       "      <td>2.259819000000000e+06</td>\n",
       "      <td>1.322060000000000e+06</td>\n",
       "      <td>516508.000000000000000</td>\n",
       "      <td>168668.000000000000000</td>\n",
       "      <td>43629.000000000000000</td>\n",
       "      <td>8487.000000000000000</td>\n",
       "      <td>1.940877000000000e+06</td>\n",
       "      <td>913141.000000000000000</td>\n",
       "      <td>190422.000000000000000</td>\n",
       "      <td>12573.000000000000000</td>\n",
       "      <td>69.000000000000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>8.280172934363168e+00</td>\n",
       "      <td>5.340089955065765e+00</td>\n",
       "      <td>4.073269655172166e+00</td>\n",
       "      <td>3.161279359924292e+00</td>\n",
       "      <td>2.488129297545051e+00</td>\n",
       "      <td>4.495547943393690</td>\n",
       "      <td>7.648822427725515</td>\n",
       "      <td>3.511858784963872</td>\n",
       "      <td>6.705966468188260</td>\n",
       "      <td>4.394424232223272</td>\n",
       "      <td>1.519901772429125e+00</td>\n",
       "      <td>1.000781265592448e+00</td>\n",
       "      <td>6.537944176990844e-01</td>\n",
       "      <td>4.363332055080515e-01</td>\n",
       "      <td>3.461756357036815e-01</td>\n",
       "      <td>3.029555443980486e+00</td>\n",
       "      <td>3.974042955272756</td>\n",
       "      <td>3.072208662003881</td>\n",
       "      <td>2.895839456500164</td>\n",
       "      <td>2.412170160828161</td>\n",
       "      <td>2.996098748534189e+00</td>\n",
       "      <td>3.861635303946878</td>\n",
       "      <td>2.968262862917400</td>\n",
       "      <td>2.939332912483140</td>\n",
       "      <td>4.212455632870468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>6.472437760766262e+00</td>\n",
       "      <td>5.418931705534025e+00</td>\n",
       "      <td>4.378796112343876e+00</td>\n",
       "      <td>3.612515223263715e+00</td>\n",
       "      <td>3.093334596212980e+00</td>\n",
       "      <td>5.682357718078486</td>\n",
       "      <td>7.901115153522256</td>\n",
       "      <td>6.123648644443697</td>\n",
       "      <td>5.371721896048220</td>\n",
       "      <td>2.678478046172352</td>\n",
       "      <td>1.614726093829267e+00</td>\n",
       "      <td>1.216707473859449e+00</td>\n",
       "      <td>8.668271439036094e-01</td>\n",
       "      <td>4.982346310815925e-01</td>\n",
       "      <td>3.897080960729700e-01</td>\n",
       "      <td>4.725024005833252e+00</td>\n",
       "      <td>5.508270146788696</td>\n",
       "      <td>4.054558738905269</td>\n",
       "      <td>3.823746442647552</td>\n",
       "      <td>3.318101097250907</td>\n",
       "      <td>4.827770289798093e+00</td>\n",
       "      <td>5.038446629687462</td>\n",
       "      <td>3.823217614648587</td>\n",
       "      <td>3.701284571309532</td>\n",
       "      <td>4.379161958258686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>2.961911403971992e-02</td>\n",
       "      <td>3.254959884122560e-02</td>\n",
       "      <td>1.887788306449050e-02</td>\n",
       "      <td>1.790474958838670e-02</td>\n",
       "      <td>2.089492547671974e-02</td>\n",
       "      <td>0.021445092366132</td>\n",
       "      <td>0.026058800019966</td>\n",
       "      <td>0.023985133933970</td>\n",
       "      <td>0.686609596518800</td>\n",
       "      <td>0.766540091272864</td>\n",
       "      <td>1.141404580697836e-02</td>\n",
       "      <td>1.141364412757553e-02</td>\n",
       "      <td>1.141236627741181e-02</td>\n",
       "      <td>1.219137424428674e-02</td>\n",
       "      <td>1.181923933452479e-02</td>\n",
       "      <td>2.303527657640051e-02</td>\n",
       "      <td>0.020756898015294</td>\n",
       "      <td>0.022698682071845</td>\n",
       "      <td>0.038521160827556</td>\n",
       "      <td>0.029278181010677</td>\n",
       "      <td>1.375727595969839e-02</td>\n",
       "      <td>0.017551418705169</td>\n",
       "      <td>0.020358850012194</td>\n",
       "      <td>0.025111439619053</td>\n",
       "      <td>0.301086769754756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>3.586795443094233e+00</td>\n",
       "      <td>1.201177712559297e+00</td>\n",
       "      <td>9.217038549000895e-01</td>\n",
       "      <td>6.782684604522693e-01</td>\n",
       "      <td>5.223378662522787e-01</td>\n",
       "      <td>0.576080558821762</td>\n",
       "      <td>1.015576222879062</td>\n",
       "      <td>0.308495726577606</td>\n",
       "      <td>3.157578682370143</td>\n",
       "      <td>2.798644458975836</td>\n",
       "      <td>2.796718803121323e-01</td>\n",
       "      <td>1.639971469817920e-01</td>\n",
       "      <td>1.244225069045667e-01</td>\n",
       "      <td>8.815842420741427e-02</td>\n",
       "      <td>6.504466212494500e-02</td>\n",
       "      <td>3.318684849991966e-01</td>\n",
       "      <td>0.367344192264577</td>\n",
       "      <td>0.378064694434752</td>\n",
       "      <td>0.369682431331787</td>\n",
       "      <td>0.318782619314686</td>\n",
       "      <td>3.316127741481465e-01</td>\n",
       "      <td>0.351781848142190</td>\n",
       "      <td>0.413621201195521</td>\n",
       "      <td>0.603155756862551</td>\n",
       "      <td>0.611554847196156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>4.081952860702785e+00</td>\n",
       "      <td>1.874516554802728e+00</td>\n",
       "      <td>1.616593933026131e+00</td>\n",
       "      <td>1.425978741367701e+00</td>\n",
       "      <td>1.031631629419797e+00</td>\n",
       "      <td>2.368887671937529</td>\n",
       "      <td>3.625318399424649</td>\n",
       "      <td>1.219648896147668</td>\n",
       "      <td>7.332173303246091</td>\n",
       "      <td>3.540141577183312</td>\n",
       "      <td>3.608731516393332e-01</td>\n",
       "      <td>3.465464567525139e-01</td>\n",
       "      <td>2.082685800174132e-01</td>\n",
       "      <td>1.819326577390514e-01</td>\n",
       "      <td>1.425819642982107e-01</td>\n",
       "      <td>1.377754953960830e+00</td>\n",
       "      <td>1.724194863267630</td>\n",
       "      <td>1.376354778268855</td>\n",
       "      <td>1.320685049239473</td>\n",
       "      <td>0.943737500418678</td>\n",
       "      <td>1.359644096525404e+00</td>\n",
       "      <td>1.896373172755041</td>\n",
       "      <td>1.203177394866193</td>\n",
       "      <td>1.191475235048948</td>\n",
       "      <td>3.933263893878170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.299698147678747e+01</td>\n",
       "      <td>9.099164412208367e+00</td>\n",
       "      <td>5.921775370046984e+00</td>\n",
       "      <td>4.790016107304813e+00</td>\n",
       "      <td>3.784055845532048e+00</td>\n",
       "      <td>5.289297003615636</td>\n",
       "      <td>13.883536359885113</td>\n",
       "      <td>3.060738724830649</td>\n",
       "      <td>8.226181346328568</td>\n",
       "      <td>6.947516627958130</td>\n",
       "      <td>3.300018117749618e+00</td>\n",
       "      <td>1.222988659946064e+00</td>\n",
       "      <td>9.681534602444097e-01</td>\n",
       "      <td>7.433759408176641e-01</td>\n",
       "      <td>5.581691505421759e-01</td>\n",
       "      <td>3.269496131302390e+00</td>\n",
       "      <td>4.102221066477412</td>\n",
       "      <td>4.294924387581315</td>\n",
       "      <td>4.140915279688331</td>\n",
       "      <td>3.429009033857146</td>\n",
       "      <td>3.086847085496565e+00</td>\n",
       "      <td>3.919452409680491</td>\n",
       "      <td>4.130919136312462</td>\n",
       "      <td>4.508902533440904</td>\n",
       "      <td>6.162698571469604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>3.198266338393780e+01</td>\n",
       "      <td>3.262905683348890e+01</td>\n",
       "      <td>3.302386675480917e+01</td>\n",
       "      <td>3.159093091257384e+01</td>\n",
       "      <td>3.100225734439231e+01</td>\n",
       "      <td>27.781299783360573</td>\n",
       "      <td>29.185297151512064</td>\n",
       "      <td>27.740428524302136</td>\n",
       "      <td>19.545679679004945</td>\n",
       "      <td>8.498100337121762</td>\n",
       "      <td>3.101459825747287e+01</td>\n",
       "      <td>2.874382209610579e+01</td>\n",
       "      <td>2.940360628711539e+01</td>\n",
       "      <td>2.953075591857797e+01</td>\n",
       "      <td>2.726191329329848e+01</td>\n",
       "      <td>3.149087939993671e+01</td>\n",
       "      <td>30.820672974424905</td>\n",
       "      <td>29.534995703307032</td>\n",
       "      <td>28.240976833868462</td>\n",
       "      <td>27.656250967540650</td>\n",
       "      <td>3.120219388028912e+01</td>\n",
       "      <td>31.685978682307926</td>\n",
       "      <td>30.910329011112552</td>\n",
       "      <td>29.178828940837558</td>\n",
       "      <td>19.440918898792376</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    dist_C_0               dist_C_1               dist_C_2  \\\n",
       "count  2.358632000000000e+06  2.358053000000000e+06  2.351206000000000e+06   \n",
       "mean   8.280172934363168e+00  5.340089955065765e+00  4.073269655172166e+00   \n",
       "std    6.472437760766262e+00  5.418931705534025e+00  4.378796112343876e+00   \n",
       "min    2.961911403971992e-02  3.254959884122560e-02  1.887788306449050e-02   \n",
       "25%    3.586795443094233e+00  1.201177712559297e+00  9.217038549000895e-01   \n",
       "50%    4.081952860702785e+00  1.874516554802728e+00  1.616593933026131e+00   \n",
       "75%    1.299698147678747e+01  9.099164412208367e+00  5.921775370046984e+00   \n",
       "max    3.198266338393780e+01  3.262905683348890e+01  3.302386675480917e+01   \n",
       "\n",
       "                    dist_C_3               dist_C_4               dist_F_0  \\\n",
       "count  2.308218000000000e+06  2.136831000000000e+06  24464.000000000000000   \n",
       "mean   3.161279359924292e+00  2.488129297545051e+00      4.495547943393690   \n",
       "std    3.612515223263715e+00  3.093334596212980e+00      5.682357718078486   \n",
       "min    1.790474958838670e-02  2.089492547671974e-02      0.021445092366132   \n",
       "25%    6.782684604522693e-01  5.223378662522787e-01      0.576080558821762   \n",
       "50%    1.425978741367701e+00  1.031631629419797e+00      2.368887671937529   \n",
       "75%    4.790016107304813e+00  3.784055845532048e+00      5.289297003615636   \n",
       "max    3.159093091257384e+01  3.100225734439231e+01     27.781299783360573   \n",
       "\n",
       "                   dist_F_1              dist_F_2            dist_F_3  \\\n",
       "count  8346.000000000000000  5184.000000000000000  11.000000000000000   \n",
       "mean      7.648822427725515     3.511858784963872   6.705966468188260   \n",
       "std       7.901115153522256     6.123648644443697   5.371721896048220   \n",
       "min       0.026058800019966     0.023985133933970   0.686609596518800   \n",
       "25%       1.015576222879062     0.308495726577606   3.157578682370143   \n",
       "50%       3.625318399424649     1.219648896147668   7.332173303246091   \n",
       "75%      13.883536359885113     3.060738724830649   8.226181346328568   \n",
       "max      29.185297151512064    27.740428524302136  19.545679679004945   \n",
       "\n",
       "                 dist_F_4               dist_H_0               dist_H_1  \\\n",
       "count  11.000000000000000  2.358457000000000e+06  2.355452000000000e+06   \n",
       "mean    4.394424232223272  1.519901772429125e+00  1.000781265592448e+00   \n",
       "std     2.678478046172352  1.614726093829267e+00  1.216707473859449e+00   \n",
       "min     0.766540091272864  1.141404580697836e-02  1.141364412757553e-02   \n",
       "25%     2.798644458975836  2.796718803121323e-01  1.639971469817920e-01   \n",
       "50%     3.540141577183312  3.608731516393332e-01  3.465464567525139e-01   \n",
       "75%     6.947516627958130  3.300018117749618e+00  1.222988659946064e+00   \n",
       "max     8.498100337121762  3.101459825747287e+01  2.874382209610579e+01   \n",
       "\n",
       "                    dist_H_2               dist_H_3               dist_H_4  \\\n",
       "count  2.344934000000000e+06  2.318028000000000e+06  2.259819000000000e+06   \n",
       "mean   6.537944176990844e-01  4.363332055080515e-01  3.461756357036815e-01   \n",
       "std    8.668271439036094e-01  4.982346310815925e-01  3.897080960729700e-01   \n",
       "min    1.141236627741181e-02  1.219137424428674e-02  1.181923933452479e-02   \n",
       "25%    1.244225069045667e-01  8.815842420741427e-02  6.504466212494500e-02   \n",
       "50%    2.082685800174132e-01  1.819326577390514e-01  1.425819642982107e-01   \n",
       "75%    9.681534602444097e-01  7.433759408176641e-01  5.581691505421759e-01   \n",
       "max    2.940360628711539e+01  2.953075591857797e+01  2.726191329329848e+01   \n",
       "\n",
       "                    dist_N_0                dist_N_1                dist_N_2  \\\n",
       "count  1.322060000000000e+06  516508.000000000000000  168668.000000000000000   \n",
       "mean   3.029555443980486e+00       3.974042955272756       3.072208662003881   \n",
       "std    4.725024005833252e+00       5.508270146788696       4.054558738905269   \n",
       "min    2.303527657640051e-02       0.020756898015294       0.022698682071845   \n",
       "25%    3.318684849991966e-01       0.367344192264577       0.378064694434752   \n",
       "50%    1.377754953960830e+00       1.724194863267630       1.376354778268855   \n",
       "75%    3.269496131302390e+00       4.102221066477412       4.294924387581315   \n",
       "max    3.149087939993671e+01      30.820672974424905      29.534995703307032   \n",
       "\n",
       "                    dist_N_3              dist_N_4               dist_O_0  \\\n",
       "count  43629.000000000000000  8487.000000000000000  1.940877000000000e+06   \n",
       "mean       2.895839456500164     2.412170160828161  2.996098748534189e+00   \n",
       "std        3.823746442647552     3.318101097250907  4.827770289798093e+00   \n",
       "min        0.038521160827556     0.029278181010677  1.375727595969839e-02   \n",
       "25%        0.369682431331787     0.318782619314686  3.316127741481465e-01   \n",
       "50%        1.320685049239473     0.943737500418678  1.359644096525404e+00   \n",
       "75%        4.140915279688331     3.429009033857146  3.086847085496565e+00   \n",
       "max       28.240976833868462    27.656250967540650  3.120219388028912e+01   \n",
       "\n",
       "                     dist_O_1                dist_O_2               dist_O_3  \\\n",
       "count  913141.000000000000000  190422.000000000000000  12573.000000000000000   \n",
       "mean        3.861635303946878       2.968262862917400      2.939332912483140   \n",
       "std         5.038446629687462       3.823217614648587      3.701284571309532   \n",
       "min         0.017551418705169       0.020358850012194      0.025111439619053   \n",
       "25%         0.351781848142190       0.413621201195521      0.603155756862551   \n",
       "50%         1.896373172755041       1.203177394866193      1.191475235048948   \n",
       "75%         3.919452409680491       4.130919136312462      4.508902533440904   \n",
       "max        31.685978682307926      30.910329011112552     29.178828940837558   \n",
       "\n",
       "                 dist_O_4  \n",
       "count  69.000000000000000  \n",
       "mean    4.212455632870468  \n",
       "std     4.379161958258686  \n",
       "min     0.301086769754756  \n",
       "25%     0.611554847196156  \n",
       "50%     3.933263893878170  \n",
       "75%     6.162698571469604  \n",
       "max    19.440918898792376  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "structures_yukawa.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = train['scalar_coupling_constant']\n",
    "train = train.drop(columns = ['scalar_coupling_constant'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature generation funcs\n",
    "\n",
    "The features here are:\n",
    "\n",
    "- First the `type` is encoded by a label encoder.\n",
    "- The merging template and selected features from [Andrew's brute force feature engineering](https://www.kaggle.com/artgor/brute-force-feature-engineering)\n",
    "- Cosine features originally from [Effective feature](https://www.kaggle.com/kmat2019/effective-feature) and expanded in [Keras Neural Net for CHAMPS](https://www.kaggle.com/todnewman/keras-neural-net-for-champs), I simplified the generation procedure by removing unnecessary `pandas` operations since vanilla `numpy` arrays operation is faster.\n",
    "- QM9 dataset from [Quantum Machine 9 - QM9](https://www.kaggle.com/zaharch/quantum-machine-9-qm9).\n",
    "- Parallelization computed [Yukawa potentials](https://www.kaggle.com/scaomath/parallelization-of-coulomb-yukawa-interaction).\n",
    "- Giba's features from [Giba R + data.table + Simple Features](https://www.kaggle.com/titericz/giba-r-data-table-simple-features-1-17-lb), which I now export the features to a dataset: [Giba molecular features](https://www.kaggle.com/scaomath/giba-molecular-features)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "def map_atom_info(df_1, df_2, atom_idx):\n",
    "    df = pd.merge(df_1, df_2, how = 'left',\n",
    "                  left_on  = ['molecule_name', f'atom_index_{atom_idx}'],\n",
    "                  right_on = ['molecule_name',  'atom_index'])\n",
    "    \n",
    "    df = df.drop('atom_index', axis=1)\n",
    "    return df\n",
    "\n",
    "    \n",
    "def find_dist(df):\n",
    "    df_p_0 = df[['x_0', 'y_0', 'z_0']].values\n",
    "    df_p_1 = df[['x_1', 'y_1', 'z_1']].values\n",
    "    \n",
    "    df['dist'] = np.linalg.norm(df_p_0 - df_p_1, axis=1)\n",
    "    df['dist_inv2'] = 1/df['dist']**2\n",
    "    df['dist_x'] = (df['x_0'] - df['x_1']) ** 2\n",
    "    df['dist_y'] = (df['y_0'] - df['y_1']) ** 2\n",
    "    df['dist_z'] = (df['z_0'] - df['z_1']) ** 2\n",
    "    return df\n",
    "\n",
    "def find_closest_atom(df):    \n",
    "    df_temp = df.loc[:,[\"molecule_name\",\n",
    "                      \"atom_index_0\",\"atom_index_1\",\n",
    "                      \"dist\",\"x_0\",\"y_0\",\"z_0\",\"x_1\",\"y_1\",\"z_1\"]].copy()\n",
    "    df_temp_ = df_temp.copy()\n",
    "    df_temp_ = df_temp_.rename(columns={'atom_index_0': 'atom_index_1',\n",
    "                                       'atom_index_1': 'atom_index_0',\n",
    "                                       'x_0': 'x_1',\n",
    "                                       'y_0': 'y_1',\n",
    "                                       'z_0': 'z_1',\n",
    "                                       'x_1': 'x_0',\n",
    "                                       'y_1': 'y_0',\n",
    "                                       'z_1': 'z_0'})\n",
    "    df_temp_all = pd.concat((df_temp,df_temp_),axis=0)\n",
    "\n",
    "    df_temp_all[\"min_distance\"]=df_temp_all.groupby(['molecule_name', \n",
    "                                                     'atom_index_0'])['dist'].transform('min')\n",
    "    df_temp_all[\"max_distance\"]=df_temp_all.groupby(['molecule_name', \n",
    "                                                     'atom_index_0'])['dist'].transform('max')\n",
    "    \n",
    "    df_temp = df_temp_all[df_temp_all[\"min_distance\"]==df_temp_all[\"dist\"]].copy()\n",
    "    df_temp = df_temp.drop(['x_0','y_0','z_0','min_distance'], axis=1)\n",
    "    df_temp = df_temp.rename(columns={'atom_index_0': 'atom_index',\n",
    "                                         'atom_index_1': 'atom_index_closest',\n",
    "                                         'dist': 'distance_closest',\n",
    "                                         'x_1': 'x_closest',\n",
    "                                         'y_1': 'y_closest',\n",
    "                                         'z_1': 'z_closest'})\n",
    "    df_temp = df_temp.drop_duplicates(subset=['molecule_name', 'atom_index'])\n",
    "    \n",
    "    for atom_idx in [0,1]:\n",
    "        df = map_atom_info(df,df_temp, atom_idx)\n",
    "        df = df.rename(columns={'atom_index_closest': f'atom_index_closest_{atom_idx}',\n",
    "                                        'distance_closest': f'distance_closest_{atom_idx}',\n",
    "                                        'x_closest': f'x_closest_{atom_idx}',\n",
    "                                        'y_closest': f'y_closest_{atom_idx}',\n",
    "                                        'z_closest': f'z_closest_{atom_idx}'})\n",
    "        \n",
    "    df_temp= df_temp_all[df_temp_all[\"max_distance\"]==df_temp_all[\"dist\"]].copy()\n",
    "    df_temp = df_temp.drop(['x_0','y_0','z_0','max_distance'], axis=1)\n",
    "    df_temp= df_temp.rename(columns={'atom_index_0': 'atom_index',\n",
    "                                         'atom_index_1': 'atom_index_farthest',\n",
    "                                         'dist': 'distance_farthest',\n",
    "                                         'x_1': 'x_farthest',\n",
    "                                         'y_1': 'y_farthest',\n",
    "                                         'z_1': 'z_farthest'})\n",
    "    df_temp = df_temp.drop_duplicates(subset=['molecule_name', 'atom_index'])\n",
    "        \n",
    "    for atom_idx in [0,1]:\n",
    "        df = map_atom_info(df,df_temp, atom_idx)\n",
    "        df = df.rename(columns={'atom_index_farthest': f'atom_index_farthest_{atom_idx}',\n",
    "                                        'distance_farthest': f'distance_farthest_{atom_idx}',\n",
    "                                        'x_farthest': f'x_farthest_{atom_idx}',\n",
    "                                        'y_farthest': f'y_farthest_{atom_idx}',\n",
    "                                        'z_farthest': f'z_farthest_{atom_idx}'})\n",
    "    return df\n",
    "\n",
    "\n",
    "def add_cos_features(df):\n",
    "    \n",
    "    df[\"distance_center0\"] = np.sqrt((df['x_0']-df['c_x'])**2 \\\n",
    "                                   + (df['y_0']-df['c_y'])**2 \\\n",
    "                                   + (df['z_0']-df['c_z'])**2)\n",
    "    df[\"distance_center1\"] = np.sqrt((df['x_1']-df['c_x'])**2 \\\n",
    "                                   + (df['y_1']-df['c_y'])**2 \\\n",
    "                                   + (df['z_1']-df['c_z'])**2)\n",
    "    \n",
    "    df['distance_c0'] = np.sqrt((df['x_0']-df['x_closest_0'])**2 + \\\n",
    "                                (df['y_0']-df['y_closest_0'])**2 + \\\n",
    "                                (df['z_0']-df['z_closest_0'])**2)\n",
    "    df['distance_c1'] = np.sqrt((df['x_1']-df['x_closest_1'])**2 + \\\n",
    "                                (df['y_1']-df['y_closest_1'])**2 + \\\n",
    "                                (df['z_1']-df['z_closest_1'])**2)\n",
    "    \n",
    "    df[\"distance_f0\"] = np.sqrt((df['x_0']-df['x_farthest_0'])**2 + \\\n",
    "                                (df['y_0']-df['y_farthest_0'])**2 + \\\n",
    "                                (df['z_0']-df['z_farthest_0'])**2)\n",
    "    df[\"distance_f1\"] = np.sqrt((df['x_1']-df['x_farthest_1'])**2 + \\\n",
    "                                (df['y_1']-df['y_farthest_1'])**2 + \\\n",
    "                                (df['z_1']-df['z_farthest_1'])**2)\n",
    "    \n",
    "    vec_center0_x = (df['x_0']-df['c_x'])/(df[\"distance_center0\"]+1e-10)\n",
    "    vec_center0_y = (df['y_0']-df['c_y'])/(df[\"distance_center0\"]+1e-10)\n",
    "    vec_center0_z = (df['z_0']-df['c_z'])/(df[\"distance_center0\"]+1e-10)\n",
    "    \n",
    "    vec_center1_x = (df['x_1']-df['c_x'])/(df[\"distance_center1\"]+1e-10)\n",
    "    vec_center1_y = (df['y_1']-df['c_y'])/(df[\"distance_center1\"]+1e-10)\n",
    "    vec_center1_z = (df['z_1']-df['c_z'])/(df[\"distance_center1\"]+1e-10)\n",
    "    \n",
    "    vec_c0_x = (df['x_0']-df['x_closest_0'])/(df[\"distance_c0\"]+1e-10)\n",
    "    vec_c0_y = (df['y_0']-df['y_closest_0'])/(df[\"distance_c0\"]+1e-10)\n",
    "    vec_c0_z = (df['z_0']-df['z_closest_0'])/(df[\"distance_c0\"]+1e-10)\n",
    "    \n",
    "    vec_c1_x = (df['x_1']-df['x_closest_1'])/(df[\"distance_c1\"]+1e-10)\n",
    "    vec_c1_y = (df['y_1']-df['y_closest_1'])/(df[\"distance_c1\"]+1e-10)\n",
    "    vec_c1_z = (df['z_1']-df['z_closest_1'])/(df[\"distance_c1\"]+1e-10)\n",
    "    \n",
    "    vec_f0_x = (df['x_0']-df['x_farthest_0'])/(df[\"distance_f0\"]+1e-10)\n",
    "    vec_f0_y = (df['y_0']-df['y_farthest_0'])/(df[\"distance_f0\"]+1e-10)\n",
    "    vec_f0_z = (df['z_0']-df['z_farthest_0'])/(df[\"distance_f0\"]+1e-10)\n",
    "    \n",
    "    vec_f1_x = (df['x_1']-df['x_farthest_1'])/(df[\"distance_f1\"]+1e-10)\n",
    "    vec_f1_y = (df['y_1']-df['y_farthest_1'])/(df[\"distance_f1\"]+1e-10)\n",
    "    vec_f1_z = (df['z_1']-df['z_farthest_1'])/(df[\"distance_f1\"]+1e-10)\n",
    "    \n",
    "    vec_x = (df['x_1']-df['x_0'])/df['dist']\n",
    "    vec_y = (df['y_1']-df['y_0'])/df['dist']\n",
    "    vec_z = (df['z_1']-df['z_0'])/df['dist']\n",
    "    \n",
    "    df[\"cos_c0_c1\"] = vec_c0_x*vec_c1_x + vec_c0_y*vec_c1_y + vec_c0_z*vec_c1_z\n",
    "    df[\"cos_f0_f1\"] = vec_f0_x*vec_f1_x + vec_f0_y*vec_f1_y + vec_f0_z*vec_f1_z\n",
    "    \n",
    "    df[\"cos_c0_f0\"] = vec_c0_x*vec_f0_x + vec_c0_y*vec_f0_y + vec_c0_z*vec_f0_z\n",
    "    df[\"cos_c1_f1\"] = vec_c1_x*vec_f1_x + vec_c1_y*vec_f1_y + vec_c1_z*vec_f1_z\n",
    "    \n",
    "    df[\"cos_center0_center1\"] = vec_center0_x*vec_center1_x \\\n",
    "                              + vec_center0_y*vec_center1_y \\\n",
    "                              + vec_center0_z*vec_center1_z\n",
    "    \n",
    "    df[\"cos_c0\"] = vec_c0_x*vec_x + vec_c0_y*vec_y + vec_c0_z*vec_z\n",
    "    df[\"cos_c1\"] = vec_c1_x*vec_x + vec_c1_y*vec_y + vec_c1_z*vec_z\n",
    "    \n",
    "    df[\"cos_f0\"] = vec_f0_x*vec_x + vec_f0_y*vec_y + vec_f0_z*vec_z\n",
    "    df[\"cos_f1\"] = vec_f1_x*vec_x + vec_f1_y*vec_y + vec_f1_z*vec_z\n",
    "    \n",
    "    df[\"cos_center0\"] = vec_center0_x*vec_x + vec_center0_y*vec_y + vec_center0_z*vec_z\n",
    "    df[\"cos_center1\"] = vec_center1_x*vec_x + vec_center1_y*vec_y + vec_center1_z*vec_z\n",
    "\n",
    "    return df\n",
    "\n",
    "def add_dist_features(df):\n",
    "    # Andrew's features selected\n",
    "    df[f'molecule_atom_index_0_dist_mean'] = df.groupby(['molecule_name', 'atom_index_0'])['dist'].transform('mean')\n",
    "    df[f'molecule_atom_index_0_dist_mean_diff'] = df[f'molecule_atom_index_0_dist_mean'] - df['dist']\n",
    "    df[f'molecule_atom_index_0_dist_min'] = df.groupby(['molecule_name', 'atom_index_0'])['dist'].transform('min')\n",
    "    df[f'molecule_atom_index_0_dist_min_diff'] = df[f'molecule_atom_index_0_dist_min'] - df['dist']\n",
    "    df[f'molecule_atom_index_0_dist_std'] = df.groupby(['molecule_name', 'atom_index_0'])['dist'].transform('std')\n",
    "\n",
    "    df[f'molecule_atom_index_1_dist_mean'] = df.groupby(['molecule_name', 'atom_index_1'])['dist'].transform('mean')\n",
    "    df[f'molecule_atom_index_1_dist_mean_diff'] = df[f'molecule_atom_index_1_dist_mean'] - df['dist']\n",
    "    df[f'molecule_atom_index_1_dist_min'] = df.groupby(['molecule_name', 'atom_index_1'])['dist'].transform('min')\n",
    "    df[f'molecule_atom_index_1_dist_min_diff'] = df[f'molecule_atom_index_1_dist_min'] - df['dist']\n",
    "    df[f'molecule_atom_index_1_dist_std'] = df.groupby(['molecule_name', 'atom_index_1'])['dist'].transform('std')\n",
    "    \n",
    "    df[f'molecule_type_dist_mean'] = df.groupby(['molecule_name', 'type'])['dist'].transform('mean')\n",
    "    df[f'molecule_type_dist_mean_diff'] = df[f'molecule_type_dist_mean'] - df['dist']\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def dummies(df, list_cols):\n",
    "    for col in list_cols:\n",
    "        df_dummies = pd.get_dummies(df[col], drop_first=True, \n",
    "                                    prefix=(str(col)))\n",
    "        df = pd.concat([df, df_dummies], axis=1)\n",
    "    return df\n",
    "\n",
    "\n",
    "def add_qm9_features(df):\n",
    "    data_qm9 = pd.read_pickle('../data/raw/data.covs.pickle')\n",
    "    to_drop = ['type', \n",
    "               'linear', \n",
    "               'atom_index_0', \n",
    "               'atom_index_1', \n",
    "               'scalar_coupling_constant', \n",
    "               'U', 'G', 'H', \n",
    "               'mulliken_mean', 'r2', 'U0']\n",
    "    data_qm9 = data_qm9.drop(columns = to_drop, axis=1)\n",
    "    data_qm9 = reduce_mem_usage(data_qm9,verbose=False)\n",
    "    df = pd.merge(df, data_qm9, how='left', on=['molecule_name','id'])\n",
    "    del data_qm9\n",
    "    \n",
    "    df = dummies(df, ['type', 'atom_1'])\n",
    "    return df\n",
    "\n",
    "def get_features(df, struct):\n",
    "    for atom_idx in [0,1]:\n",
    "        df = map_atom_info(df, struct, atom_idx)\n",
    "        df = df.rename(columns={'atom': f'atom_{atom_idx}',\n",
    "                            'x': f'x_{atom_idx}',\n",
    "                            'y': f'y_{atom_idx}',\n",
    "                            'z': f'z_{atom_idx}'})\n",
    "        struct['c_x'] = struct.groupby('molecule_name')['x'].transform('mean')\n",
    "        struct['c_y'] = struct.groupby('molecule_name')['y'].transform('mean')\n",
    "        struct['c_z'] = struct.groupby('molecule_name')['z'].transform('mean')\n",
    "\n",
    "    df = find_dist(df)\n",
    "    df = find_closest_atom(df)\n",
    "    df = add_cos_features(df)\n",
    "    df = add_dist_features(df)\n",
    "    df = add_qm9_features(df)\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "good_columns = ['type',\n",
    " 'dist_C_0_x',\n",
    " 'dist_C_1_x',\n",
    " 'dist_C_2_x',\n",
    " 'dist_C_3_x',\n",
    " 'dist_C_4_x',\n",
    " 'dist_F_0_x',\n",
    " 'dist_F_1_x',\n",
    " 'dist_F_2_x',\n",
    " 'dist_F_3_x',\n",
    " 'dist_F_4_x',\n",
    " 'dist_H_0_x',\n",
    " 'dist_H_1_x',\n",
    " 'dist_H_2_x',\n",
    " 'dist_H_3_x',\n",
    " 'dist_H_4_x',\n",
    " 'dist_N_0_x',\n",
    " 'dist_N_1_x',\n",
    " 'dist_N_2_x',\n",
    " 'dist_N_3_x',\n",
    " 'dist_N_4_x',\n",
    " 'dist_O_0_x',\n",
    " 'dist_O_1_x',\n",
    " 'dist_O_2_x',\n",
    " 'dist_O_3_x',\n",
    " 'dist_O_4_x',\n",
    " 'dist_C_0_y',\n",
    " 'dist_C_1_y',\n",
    " 'dist_C_2_y',\n",
    " 'dist_C_3_y',\n",
    " 'dist_C_4_y',\n",
    " 'dist_F_0_y',\n",
    " 'dist_F_1_y',\n",
    " 'dist_F_2_y',\n",
    " 'dist_F_3_y',\n",
    " 'dist_F_4_y',\n",
    " 'dist_H_0_y',\n",
    " 'dist_H_1_y',\n",
    " 'dist_H_2_y',\n",
    " 'dist_H_3_y',\n",
    " 'dist_H_4_y',\n",
    " 'dist_N_0_y',\n",
    " 'dist_N_1_y',\n",
    " 'dist_N_2_y',\n",
    " 'dist_N_3_y',\n",
    " 'dist_N_4_y',\n",
    " 'dist_O_0_y',\n",
    " 'dist_O_1_y',\n",
    " 'dist_O_2_y',\n",
    " 'dist_O_3_y',\n",
    " 'dist_O_4_y',\n",
    " 'dist_inv2',\n",
    " 'distance_closest_0',\n",
    " 'distance_closest_1',\n",
    " 'distance_farthest_0',\n",
    " 'distance_farthest_1',\n",
    " 'cos_c0_c1', 'cos_f0_f1','cos_c0_f0', 'cos_c1_f1',\n",
    " 'cos_center0_center1', 'cos_c0', 'cos_c1', 'cos_f0', 'cos_f1',\n",
    " 'cos_center0', 'cos_center1',\n",
    " 'molecule_atom_index_0_dist_mean',\n",
    " 'molecule_atom_index_0_dist_mean_diff',\n",
    " 'molecule_atom_index_0_dist_min',\n",
    " 'molecule_atom_index_0_dist_min_diff',\n",
    " 'molecule_atom_index_0_dist_std',\n",
    " 'molecule_atom_index_1_dist_mean',\n",
    " 'molecule_atom_index_1_dist_mean_diff',\n",
    " 'molecule_atom_index_1_dist_min',\n",
    " 'molecule_atom_index_1_dist_min_diff',\n",
    " 'molecule_atom_index_1_dist_std',\n",
    " 'molecule_type_dist_mean',\n",
    " 'molecule_type_dist_mean_diff',\n",
    " 'rc_A',\n",
    " 'rc_B',\n",
    " 'rc_C',\n",
    " 'mu',\n",
    " 'alpha',\n",
    " 'homo',\n",
    " 'lumo',\n",
    " 'gap',\n",
    " 'zpve',\n",
    " 'Cv',\n",
    " 'freqs_min',\n",
    " 'freqs_max',\n",
    " 'freqs_mean',\n",
    " 'mulliken_min',\n",
    " 'mulliken_max',\n",
    " 'mulliken_atom_0',\n",
    " 'mulliken_atom_1']\n",
    "\n",
    "giba_columns = ['inv_dist0',\n",
    " 'inv_dist1',\n",
    " 'inv_distP',\n",
    " 'inv_dist0R',\n",
    " 'inv_dist1R',\n",
    " 'inv_distPR',\n",
    " 'inv_dist0E',\n",
    " 'inv_dist1E',\n",
    " 'inv_distPE',\n",
    " 'linkM0',\n",
    " 'linkM1',\n",
    " 'min_molecule_atom_0_dist_xyz',\n",
    " 'mean_molecule_atom_0_dist_xyz',\n",
    " 'max_molecule_atom_0_dist_xyz',\n",
    " 'sd_molecule_atom_0_dist_xyz',\n",
    " 'min_molecule_atom_1_dist_xyz',\n",
    " 'mean_molecule_atom_1_dist_xyz',\n",
    " 'max_molecule_atom_1_dist_xyz',\n",
    " 'sd_molecule_atom_1_dist_xyz',\n",
    " 'coulomb_C.x',\n",
    " 'coulomb_F.x',\n",
    " 'coulomb_H.x',\n",
    " 'coulomb_N.x',\n",
    " 'coulomb_O.x',\n",
    " 'yukawa_C.x',\n",
    " 'yukawa_F.x',\n",
    " 'yukawa_H.x',\n",
    " 'yukawa_N.x',\n",
    " 'yukawa_O.x',\n",
    " 'vander_C.x',\n",
    " 'vander_F.x',\n",
    " 'vander_H.x',\n",
    " 'vander_N.x',\n",
    " 'vander_O.x',\n",
    " 'coulomb_C.y',\n",
    " 'coulomb_F.y',\n",
    " 'coulomb_H.y',\n",
    " 'coulomb_N.y',\n",
    " 'coulomb_O.y',\n",
    " 'yukawa_C.y',\n",
    " 'yukawa_F.y',\n",
    " 'yukawa_H.y',\n",
    " 'yukawa_N.y',\n",
    " 'yukawa_O.y',\n",
    " 'vander_C.y',\n",
    " 'vander_F.y',\n",
    " 'vander_H.y',\n",
    " 'vander_N.y',\n",
    " 'vander_O.y',\n",
    " 'distC0',\n",
    " 'distH0',\n",
    " 'distN0',\n",
    " 'distC1',\n",
    " 'distH1',\n",
    " 'distN1',\n",
    " 'adH1',\n",
    " 'adH2',\n",
    " 'adH3',\n",
    " 'adH4',\n",
    " 'adC1',\n",
    " 'adC2',\n",
    " 'adC3',\n",
    " 'adC4',\n",
    " 'adN1',\n",
    " 'adN2',\n",
    " 'adN3',\n",
    " 'adN4',\n",
    " 'NC',\n",
    " 'NH',\n",
    " 'NN',\n",
    " 'NF',\n",
    " 'NO']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "lbl = LabelEncoder()\n",
    "lbl.fit(list(train['type'].values) + list(test['type'].values))\n",
    "train['type'] = lbl.transform(list(train['type'].values))\n",
    "test['type'] = lbl.transform(list(test['type'].values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training by type with time seed\n",
    "We use different numbers of iterations for different type, after running the label encoder\n",
    "```\n",
    "train['type'].unique() = [0, 3, 1, 4, 2, 6, 5, 7]\n",
    "```\n",
    "Hence the current the number of iteration `N` config (in order) is:\n",
    "\n",
    "> Type 0 = `1JHC`. <br>\n",
    "> Type 1 = `1JHN`. <br>\n",
    "> Type 2 = `2JHC`. <br>\n",
    "> Type 3 = `2JHH`. <br>\n",
    "> Type 4 = `2JHN`. <br>\n",
    "> Type 5 = `3JHC`. <br>\n",
    "> Type 6 = `3JHH`. <br>\n",
    "> Type 7 = `3JHN`. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_fold = 3\n",
    "seed = 0\n",
    "folds = KFold(n_splits=n_fold, shuffle=True, random_state=seed)\n",
    "\n",
    "catboost_params = {\n",
    "    'max_depth': 9,\n",
    "    'max_leaves': 64,\n",
    "    #\"subsample\": 0.8,\n",
    "    'reg_lambda': 0.2\n",
    "}\n",
    "\n",
    "lgb_params = {\n",
    "    'num_leaves': 400,\n",
    "          'objective': 'huber',\n",
    "          'max_depth': 9,\n",
    "          'learning_rate': 0.12,\n",
    "          \"boosting_type\": \"gbdt\",\n",
    "          \"subsample_freq\": 1,\n",
    "          \"subsample\": 0.8,\n",
    "          \"metric\": 'mae',\n",
    "          \"verbosity\": -1,\n",
    "          'lambda_l1': 0.8,\n",
    "          'lambda_l2': 0.2,\n",
    "          'feature_fraction': 0.6,\n",
    "}\n",
    "\n",
    "linreg_params = {\n",
    "    'n_jobs': -1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_short = pd.DataFrame({'ind': list(train.index), \n",
    "                        'type': train['type'].values,\n",
    "                        'oof': [0] * len(train), \n",
    "                        'target': y.values})\n",
    "X_short_test = pd.DataFrame({'ind': list(test.index), \n",
    "                             'type': test['type'].values, \n",
    "                             'prediction': [0] * len(test)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training of type 0: 1JHC.\n",
      "Generating features...\n",
      "Done in 101.90 seconds for 168 features.\n",
      "Fold 1 started at Sat Jul 20 18:28:19 2019\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input contains NaN, infinity or a value too large for dtype('float64').",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-31cc135ed760>\u001b[0m in \u001b[0;36mtrain_model_regression\u001b[0;34m(X, X_test, y, params, folds, model_type, eval_metric, columns, plot_feature_importance, model, verbose, early_stopping_rounds, n_estimators)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m             \u001b[0mimputer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSimpleImputer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmissing_values\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'NaN'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrategy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'median'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m             \u001b[0mX_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimputer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m             \u001b[0mX_valid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimputer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_valid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m             \u001b[0mX_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimputer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    551\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0;31m# fit method of arity 1 (unsupervised transformation)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 553\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    554\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    555\u001b[0m             \u001b[0;31m# fit method of arity 2 (supervised transformation)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/impute/_base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    230\u001b[0m         \u001b[0mself\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mSimpleImputer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m         \"\"\"\n\u001b[0;32m--> 232\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    233\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m         \u001b[0;31m# default fill_value is 0 for numerical input and \"missing_value\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/impute/_base.py\u001b[0m in \u001b[0;36m_validate_input\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    204\u001b[0m                                  \"\".format(self.strategy, X.dtype.kind))\n\u001b[1;32m    205\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mve\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m         \u001b[0m_check_inputs_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissing_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/impute/_base.py\u001b[0m in \u001b[0;36m_validate_input\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    197\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m             X = check_array(X, accept_sparse='csc', dtype=dtype,\n\u001b[0;32m--> 199\u001b[0;31m                             force_all_finite=force_all_finite, copy=self.copy)\n\u001b[0m\u001b[1;32m    200\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mve\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;34m\"could not convert\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mve\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    541\u001b[0m             _assert_all_finite(array,\n\u001b[0;32m--> 542\u001b[0;31m                                allow_nan=force_all_finite == 'allow-nan')\n\u001b[0m\u001b[1;32m    543\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mensure_min_samples\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan)\u001b[0m\n\u001b[1;32m     54\u001b[0m                 not allow_nan and not np.isfinite(X).all()):\n\u001b[1;32m     55\u001b[0m             \u001b[0mtype_err\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'infinity'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mallow_nan\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m'NaN, infinity'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg_err\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype_err\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m     \u001b[0;31m# for object dtype data, we only check for NaNs (GH-13254)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'object'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mallow_nan\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Input contains NaN, infinity or a value too large for dtype('float64')."
     ]
    }
   ],
   "source": [
    "%%time\n",
    "CV_score = 0\n",
    "###Iters###    [1JHC, 1JHN, 2JHC, 2JHH, 2JHN, 3JHC, 3JHH, 3JHN]\n",
    "n_estimators = [5000, 2500, 3000, 2500, 2500, 3000, 2500, 2500]\n",
    "\n",
    "\n",
    "for t in train['type'].unique():\n",
    "    type_ = lbl.inverse_transform([t])[0]\n",
    "    print(f'\\nTraining of type {t}: {type_}.')\n",
    "    index_type = (train['type'] == t)\n",
    "    index_type_test = (test['type'] == t)\n",
    "    \n",
    "    X_t = train.loc[index_type].copy()\n",
    "    X_test_t = test.loc[index_type_test].copy()\n",
    "    y_t = y[index_type]\n",
    "    \n",
    "    print(f'Generating features...')\n",
    "    start_time = time.time()\n",
    "    \n",
    "    ## Generating features from the public kernels, just by type\n",
    "    ## no memory reduction is needed\n",
    "    X_t = get_features(X_t, structures.copy())\n",
    "    X_t = X_t[good_columns].fillna(0.0)\n",
    "    \n",
    "    X_test_t = get_features(X_test_t, structures.copy())\n",
    "    X_test_t = X_test_t[good_columns].fillna(0.0)\n",
    "    \n",
    "    ## load Giba's features just for type t by getting rows to be excluded when initiating read_csv\n",
    "    rows_to_exclude = np.where(index_type==False)[0]+1 # retain the header row\n",
    "    rows_to_exclude_test = np.where(index_type_test==False)[0]+1\n",
    "    train_giba_t = pd.read_csv('../data/raw/train_giba.csv.gz',\n",
    "                        header=0, skiprows=rows_to_exclude, usecols=giba_columns)\n",
    "    test_giba_t = pd.read_csv('../data/raw/test_giba.csv.gz',\n",
    "                       header=0, skiprows=rows_to_exclude_test, usecols=giba_columns)\n",
    "    \n",
    "    X_t = pd.concat((X_t, train_giba_t), axis=1)\n",
    "\n",
    "    X_test_t = pd.concat((X_test_t,test_giba_t), axis=1)\n",
    "    \n",
    "    X_t = X_t.replace([np.inf, -np.inf], np.nan)\n",
    "    X_test_t = X_test_t.replace([np.inf, -np.inf], np.nan)\n",
    "    \n",
    "    #X_t = reduce_mem_usage(X_t)\n",
    "    #X_test_t = reduce_mem_usage(X_test_t)\n",
    "    \n",
    "    del train_giba_t, test_giba_t\n",
    "    gc.collect()\n",
    "    \n",
    "    print(f'Done in {(time.time() - start_time):.2f} seconds for {X_t.shape[1]} features.')\n",
    "    ## feature generation done\n",
    "    \n",
    "    \n",
    "    result_dict_lgb = train_model_regression(X=X_t, X_test=X_test_t, \n",
    "                                              y=y_t, params=linreg_params, \n",
    "                                              folds=folds, \n",
    "                                              model_type='sklearn-linreg', \n",
    "                                              eval_metric='mae', \n",
    "                                              plot_feature_importance=False,\n",
    "                                              verbose=2000, early_stopping_rounds=200, \n",
    "                                              n_estimators=n_estimators[t]*2)\n",
    "    del X_t, X_test_t\n",
    "    gc.collect()\n",
    "    \n",
    "    X_short.loc[X_short['type'] == t, 'oof'] = result_dict_lgb['oof']\n",
    "    X_short_test.loc[X_short_test['type'] == t, 'prediction'] = result_dict_lgb['prediction']\n",
    "    \n",
    "    ## manually computing the cv score\n",
    "    CV_score += np.log(np.array(result_dict_lgb['scores']).mean())/8 # total 8 types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub['scalar_coupling_constant'] = X_short_test['prediction']\n",
    "today = str(datetime.date.today())\n",
    "sub.to_csv(f'../submits/Catboost40k{today}_{CV_score:.4f}.csv.gz', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
