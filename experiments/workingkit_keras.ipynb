{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "w# Keras Multiple Output Solution\n",
    "Thanks to https://www.pyimagesearch.com/2018/06/04/keras-multiple-outputs-and-multiple-losses/ and https://www.kaggle.com/kmat2019/neural-network-modeling-with-multiple-outputs for the idea on approaching this problem as a multiple output problem.  Though it doesn't seem to be the favored approach for this competition, I feel that there ought to be a good neural network approach.  This kernel tries a multi-layer, dense neural network implemented in Keras.  The advantage of this approach is that it does not seem to be overfitting, which may pay off against the full dataset.\n",
    "\n",
    "Ways to improve:\n",
    "*  I'm not a domain expert in the molecular chem field... I strongly suspect that stronger feature engineering would cause this approach to score higher.  \n",
    "*  Network architecture:  I'm putting a simpler variant forward here with some options commented out.  There are tweaks that could be made to this architecture that will improve the score.  Forcing the model to overfit to gain a better score on the leaderboard does not usually pay off in the end...\n",
    "*  More epochs.  The more epochs that can be run without overfitting, the better score could be achieved.  My observation is that even after long training epochs, the model seems to still be learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.layers import Dense, Input, Activation\n",
    "from keras.layers import BatchNormalization, Add, Dropout\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Model, load_model\n",
    "from keras import callbacks\n",
    "from keras import backend as K\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "warnings.filterwarnings(action=\"ignore\", category=DeprecationWarning)\n",
    "warnings.filterwarnings(action=\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_DIR = \"../models/rich_keras\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.mkdir(MODEL_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network definition\n",
    "\n",
    "This neural network is many layers.  In the middle we define our outputs for our two Mullikan charges as well as our Dipole Moment.  The final output is the one we care the most about, the Scalar Coupling Constant.\n",
    "\n",
    "I think that BatchNormalization at each layer seems superior than small amounts of dropouts.  The network seems to not overfit, even in large numbers of training epochs.  If you do wind up seeing some overfitting, then adding the dropout to a couple of layers ought to help a lot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_nn_model(input_shape):\n",
    "    inp = Input(shape=(input_shape,))\n",
    "    x = Dense(512)(inp)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU(alpha=0.05)(x)\n",
    "    x = Dropout(0.4)(x)\n",
    "    \n",
    "    x = Dense(1024)(inp)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU(alpha=0.05)(x)\n",
    "    x = Dropout(0.4)(x)\n",
    "    \n",
    "    x = Dense(2048)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU(alpha=0.05)(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    \n",
    "    x = Dense(2048)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU(alpha=0.05)(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    \n",
    "    x = Dense(1024)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU(alpha=0.05)(x)\n",
    "    x = Dropout(0.4)(x)\n",
    "    \n",
    "    x = Dense(1024)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU(alpha=0.05)(x)\n",
    "    #x = Dropout(0.4)(x)\n",
    "    \n",
    "    x = Dense(512)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU(alpha=0.05)(x)\n",
    "    x = Dropout(0.4)(x)\n",
    "    \n",
    "    out1 = Dense(2, activation=\"linear\")(x)  # mulliken charge 2\n",
    "    out2 = Dense(6, activation=\"linear\")(x)  # tensor 6(xx,yy,zz)\n",
    "    out3 = Dense(12, activation=\"linear\")(x)  # tensor 12(others)\n",
    "    \n",
    "    x = Dense(256)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU(alpha=0.05)(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    \n",
    "    x = Dense(256)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU(alpha=0.05)(x)\n",
    "    \n",
    "    x = Dense(128)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU(alpha=0.05)(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    \n",
    "    x = Dense(64)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU(alpha=0.05)(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    \n",
    "    out = Dense(1, activation=\"linear\")(x)  # scalar_coupling_constant\n",
    "    model = Model(inputs=inp, outputs=[out, out1, out2, out3])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Function\n",
    "I rely a lot on loss plots to detect when learning has stopped as well as when overfitting begins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(history, label):\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('Loss for %s' % label)\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    _ = plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_hdf(\"../data/champs+giba/train_features.hdf\")\n",
    "df_test = pd.read_hdf(\"../data/champs+giba/test_features.hdf\")\n",
    "\n",
    "df_label = pd.read_hdf(\"../data/champs+giba/train_label.hdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['train_label.hdf',\n",
       " 'train_target_2.hdf',\n",
       " '.ipynb_checkpoints',\n",
       " 'train_target_3.hdf',\n",
       " 'test_features.hdf',\n",
       " 'train_target_1.hdf',\n",
       " 'train_features.hdf']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(\"../data/champs+giba/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_target_1 = pd.read_hdf(\"../data/champs+giba/train_target_1.hdf\")\n",
    "df_target_2 = pd.read_hdf(\"../data/champs+giba/train_target_2.hdf\")\n",
    "df_target_3 = pd.read_hdf(\"../data/champs+giba/train_target_3.hdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.concat([df_train, df_label, df_target_1, df_target_2, df_target_3], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_prediction = np.zeros(len(df_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Routine\n",
    "\n",
    "A bunch of stuff happens here.  Pay attention to the callbacks.  I train a different model for each molecule type, which allows for future retraining.  If you have kept your network the same (except for dropout, etc.), and want to retrain for a few more epochs without having to go back to the beginning, then set the retrain flag to False and it will grab the trained models as starting points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training 1JHC out of ['1JHC' '2JHH' '1JHN' '2JHN' '2JHC' '3JHH' '3JHC' '3JHN'] \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0718 23:26:33.991197 139654122051392 deprecation_wrapper.py:119] From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0718 23:26:33.991913 139654122051392 deprecation_wrapper.py:119] From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0718 23:26:33.994453 139654122051392 deprecation_wrapper.py:119] From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W0718 23:26:34.139677 139654122051392 deprecation_wrapper.py:119] From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "W0718 23:26:34.153573 139654122051392 deprecation.py:506] From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "W0718 23:26:34.937911 139654122051392 deprecation_wrapper.py:119] From /usr/local/lib/python3.7/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W0718 23:26:36.138712 139654122051392 deprecation_wrapper.py:119] From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "\n",
    "mol_types = df_train[\"type\"].unique()\n",
    "cv_score = []\n",
    "cv_score_total = 0\n",
    "epoch_n = 300\n",
    "verbose = 0\n",
    "batch_size = 2048\n",
    "\n",
    "# Set to True if we want to train from scratch.  False will reuse saved models as a starting point.\n",
    "retrain = True\n",
    "\n",
    "\n",
    "# Set up GPU preferences\n",
    "config = tf.ConfigProto(device_count={'GPU': 1, 'CPU': 2})\n",
    "config.gpu_options.allow_growth = True\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.6\n",
    "sess = tf.Session(config=config)\n",
    "K.set_session(sess)\n",
    "\n",
    "start_time = datetime.now()\n",
    "\n",
    "# Loop through each molecule type\n",
    "for mol_type in mol_types:\n",
    "    model_name_rd = (\n",
    "        f'{MODEL_DIR}/molecule_model_{mol_type}.hdf5')\n",
    "    model_name_wrt = (f'{MODEL_DIR}/molecule_model_{mol_type}.hdf5')\n",
    "    print('Training %s' % mol_type, 'out of', mol_types, '\\n')\n",
    "\n",
    "    df_train_ = df_train[df_train[\"type\"] == mol_type]\n",
    "    df_test_ = df_test[df_test[\"type\"] == mol_type]\n",
    "\n",
    "    # Here's our best features.  We think.\n",
    "    input_features = [\n",
    "        \"x_0\", \"y_0\", \"z_0\",\n",
    "        \"x_1\", \"y_1\", \"z_1\",\n",
    "        \"c_x\", \"c_y\", \"c_z\",\n",
    "        'x_closest_0', 'y_closest_0', 'z_closest_0',\n",
    "        'x_closest_1', 'y_closest_1', 'z_closest_1', \n",
    "        \"distance\", \"distance_center0\",\"distance_center1\", \"distance_c0\",\n",
    "        \"distance_c1\", \"distance_f0\", \"distance_f1\", \n",
    "        \"cos_c0_c1\", \"cos_f0_f1\", \"cos_center0_center1\", \"cos_c0\", \"cos_c1\",\n",
    "        \"cos_f0\", \"cos_f1\", \"cos_center0\", \"cos_center1\",\n",
    "        \"atom_n\"\n",
    "    ]\n",
    "\n",
    "    # Standard Scaler from sklearn does seem to work better here than other Scalers\n",
    "    input_data = StandardScaler().fit_transform(\n",
    "        pd.concat([df_train_.loc[:, input_features], df_test_.loc[:, input_features]]))\n",
    "\n",
    "    target_data = df_train_.loc[:, \"scalar_coupling_constant\"].values\n",
    "    target_data_1 = df_train_.loc[:, [\"charge_0\", \"charge_1\"]]\n",
    "    target_data_2 = df_train_.loc[:, [\n",
    "        \"XX_0\", \"YY_0\", \"ZZ_0\", \"XX_1\", \"YY_1\", \"ZZ_1\"]]\n",
    "    target_data_3 = df_train_.loc[:, [\"YX_0\", \"ZX_0\", \"XY_0\", \"ZY_0\",\n",
    "                                      \"XZ_0\", \"YZ_0\", \"YX_1\", \"ZX_1\", \"XY_1\", \"ZY_1\", \"XZ_1\", \"YZ_1\"]]\n",
    "\n",
    "    # following parameters should be adjusted to control the loss function\n",
    "    # if all parameters are zero, attractors do not work. (-> simple neural network)\n",
    "    m1 = 1\n",
    "    m2 = 4\n",
    "    m3 = 1\n",
    "    target_data_1 = m1*(StandardScaler().fit_transform(target_data_1))\n",
    "    target_data_2 = m2*(StandardScaler().fit_transform(target_data_2))\n",
    "    target_data_3 = m3*(StandardScaler().fit_transform(target_data_3))\n",
    "\n",
    "    # Simple split to provide us a validation set to do our CV checks with\n",
    "    train_index, cv_index = train_test_split(\n",
    "        np.arange(len(df_train_)), random_state=111, test_size=0.1)\n",
    "\n",
    "    # Split all our input and targets by train and cv indexes\n",
    "    train_input = input_data[train_index]\n",
    "    cv_input = input_data[cv_index]\n",
    "    train_target = target_data[train_index]\n",
    "    cv_target = target_data[cv_index]\n",
    "    train_target_1 = target_data_1[train_index]\n",
    "    cv_target_1 = target_data_1[cv_index]\n",
    "    train_target_2 = target_data_2[train_index]\n",
    "    cv_target_2 = target_data_2[cv_index]\n",
    "    train_target_3 = target_data_3[train_index]\n",
    "    cv_target_3 = target_data_3[cv_index]\n",
    "    test_input = input_data[len(df_train_):, :]\n",
    "\n",
    "    # Build the Neural Net\n",
    "    nn_model = create_nn_model(train_input.shape[1])\n",
    "\n",
    "    # If retrain==False, then we load a previous saved model as a starting point.\n",
    "    if not retrain:\n",
    "        nn_model = load_model(model_name_rd)\n",
    "\n",
    "    nn_model.compile(loss='mae', optimizer=Adam(lr=0.001))  # , metrics=[auc])\n",
    "\n",
    "    # Callback for Early Stopping... May want to raise the min_delta for small numbers of epochs\n",
    "    es = callbacks.EarlyStopping(monitor='val_loss', min_delta=0.0001,\n",
    "                                 patience=8, verbose=1, mode='auto', restore_best_weights=True)\n",
    "    # Callback for Reducing the Learning Rate... when the monitor levels out for 'patience' epochs, then the LR is reduced\n",
    "    rlr = callbacks.ReduceLROnPlateau(\n",
    "        monitor='val_loss', factor=0.1, patience=7, min_lr=1e-6, mode='auto', verbose=1)\n",
    "    # Save the best value of the model for future use\n",
    "    sv_mod = callbacks.ModelCheckpoint(\n",
    "        model_name_wrt, monitor='val_loss', save_best_only=True, period=1)\n",
    "\n",
    "    history = nn_model.fit(train_input, [train_target, train_target_1, train_target_2, train_target_3],\n",
    "                           validation_data=(\n",
    "                               cv_input, [cv_target, cv_target_1, cv_target_2, cv_target_3]),\n",
    "                           callbacks=[es, rlr, sv_mod], epochs=epoch_n, batch_size=batch_size, verbose=verbose)\n",
    "\n",
    "    cv_predict = nn_model.predict(cv_input)\n",
    "    plot_history(history, mol_type)\n",
    "\n",
    "    accuracy = np.mean(np.abs(cv_target-cv_predict[0][:, 0]))\n",
    "    cv_score.append(np.log(accuracy))\n",
    "    cv_score_total += np.log(accuracy)\n",
    "\n",
    "    # Predict on the test data set using our trained model\n",
    "    test_predict = nn_model.predict(test_input)\n",
    "\n",
    "    # for each molecule type we'll grab the predicted values\n",
    "    test_prediction[df_test[\"type\"] == mol_type] = test_predict[0][:, 0]\n",
    "    K.clear_session()\n",
    "\n",
    "cv_score_total /= len(mol_types)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare results for Submission\n",
    "\n",
    "The total CV score matches Kaggle's score pretty closely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2505542 2505542\n",
      "Total training time:  0:41:49.760125\n",
      "1JHC : cv score is  1.049739156936274\n",
      "2JHH : cv score is  -0.9127646321658073\n",
      "1JHN : cv score is  0.39138541081891826\n",
      "2JHN : cv score is  -1.1895794203918477\n",
      "2JHC : cv score is  -0.4723221583690963\n",
      "3JHH : cv score is  -1.0482714921450385\n",
      "3JHC : cv score is  -0.4987939800680683\n",
      "3JHN : cv score is  -1.56905495863202\n",
      "total cv score is -0.5312077592520859\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "datetime.now()\n",
    "\n",
    "def submit(predictions):\n",
    "    submit = pd.read_csv('../data/raw/sample_submission.csv')\n",
    "    print(len(submit), len(predictions))\n",
    "    submit[\"scalar_coupling_constant\"] = predictions\n",
    "    submit.to_csv(f\"../submits/workingkit-{datetime.now()}-{cv_score_total}.csv.zip\", index=False)\n",
    "\n",
    "\n",
    "submit(test_prediction)\n",
    "\n",
    "print('Total training time: ', datetime.now() - start_time)\n",
    "\n",
    "i = 0\n",
    "for mol_type in mol_types:\n",
    "    print(mol_type, \": cv score is \", cv_score[i])\n",
    "    i += 1\n",
    "print(\"total cv score is\", cv_score_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.4\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "print(keras.__version__)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
